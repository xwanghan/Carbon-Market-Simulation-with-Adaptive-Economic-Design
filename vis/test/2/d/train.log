seed (final): 25802000
Not restoring trainer...
Restoring agents weights...
Starting with fresh planner weights.
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.10826587677001953
    StateBufferConnector_ms: 0.009912252426147461
    ViewRequirementAgentConnector_ms: 0.2086639404296875
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 3066.7940959843436
  episode_reward_mean: 2677.3805409079996
  episode_reward_min: 2287.9669858316556
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 3066.7940959843436
    - 2287.9669858316556
    policy_a_reward: [303.89107986803396, 412.0687858908959, 396.81216724317085, 384.6088632972875,
      470.5210095042071, 290.6748762067047, 445.5373990208065, 352.08831303653386,
      421.89221643261004, 287.08040398229906]
    policy_p_reward:
    - 1098.892190180744
    - 490.6937771527008
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 470.5210095042071
    p: 1098.892190180744
  policy_reward_mean:
    a: 376.51751144825494
    p: 794.7929836667224
  policy_reward_min:
    a: 287.08040398229906
    p: 490.6937771527008
  sampler_perf:
    mean_action_processing_ms: 0.5808861669666039
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.622528890887658
    mean_inference_ms: 8.114765266220488
    mean_raw_obs_processing_ms: 2.429361114958803
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.10826587677001953
      StateBufferConnector_ms: 0.009912252426147461
      ViewRequirementAgentConnector_ms: 0.2086639404296875
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 3066.7940959843436
    episode_reward_mean: 2677.3805409079996
    episode_reward_min: 2287.9669858316556
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 3066.7940959843436
      - 2287.9669858316556
      policy_a_reward: [303.89107986803396, 412.0687858908959, 396.81216724317085,
        384.6088632972875, 470.5210095042071, 290.6748762067047, 445.5373990208065,
        352.08831303653386, 421.89221643261004, 287.08040398229906]
      policy_p_reward:
      - 1098.892190180744
      - 490.6937771527008
    num_faulty_episodes: 0
    policy_reward_max:
      a: 470.5210095042071
      p: 1098.892190180744
    policy_reward_mean:
      a: 376.51751144825494
      p: 794.7929836667224
    policy_reward_min:
      a: 287.08040398229906
      p: 490.6937771527008
    sampler_perf:
      mean_action_processing_ms: 0.5808861669666039
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.622528890887658
      mean_inference_ms: 8.114765266220488
      mean_raw_obs_processing_ms: 2.429361114958803
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/2/d/dense_logs/logs_00
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.11501312255859375
    StateBufferConnector_ms: 0.010114908218383789
    ViewRequirementAgentConnector_ms: 0.23306012153625488
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 2602.159033726348
  episode_reward_mean: 2423.0278993673464
  episode_reward_min: 2243.8967650083455
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 2602.159033726348
    - 2243.8967650083455
    policy_a_reward: [346.1861857545655, 454.8958810997613, 387.3955481920038, 373.520057640851,
      414.3578957970239, 409.61411984258564, 436.93729791173536, 460.3171086013029,
      400.6354110409512, 411.65769060386117]
    policy_p_reward:
    - 625.8034652421483
    - 124.73513700791335
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 460.3171086013029
    p: 625.8034652421483
  policy_reward_mean:
    a: 409.5517196484642
    p: 375.2693011250308
  policy_reward_min:
    a: 346.1861857545655
    p: 124.73513700791335
  sampler_perf:
    mean_action_processing_ms: 0.5656503416322447
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.515881066793924
    mean_inference_ms: 8.384210127336043
    mean_raw_obs_processing_ms: 2.2762205217268083
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.11501312255859375
      StateBufferConnector_ms: 0.010114908218383789
      ViewRequirementAgentConnector_ms: 0.23306012153625488
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 2602.159033726348
    episode_reward_mean: 2423.0278993673464
    episode_reward_min: 2243.8967650083455
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 2602.159033726348
      - 2243.8967650083455
      policy_a_reward: [346.1861857545655, 454.8958810997613, 387.3955481920038, 373.520057640851,
        414.3578957970239, 409.61411984258564, 436.93729791173536, 460.3171086013029,
        400.6354110409512, 411.65769060386117]
      policy_p_reward:
      - 625.8034652421483
      - 124.73513700791335
    num_faulty_episodes: 0
    policy_reward_max:
      a: 460.3171086013029
      p: 625.8034652421483
    policy_reward_mean:
      a: 409.5517196484642
      p: 375.2693011250308
    policy_reward_min:
      a: 346.1861857545655
      p: 124.73513700791335
    sampler_perf:
      mean_action_processing_ms: 0.5656503416322447
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.515881066793924
      mean_inference_ms: 8.384210127336043
      mean_raw_obs_processing_ms: 2.2762205217268083
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/2/d/dense_logs/logs_01
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.1029670238494873
    StateBufferConnector_ms: 0.009173154830932617
    ViewRequirementAgentConnector_ms: 0.20624399185180664
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 2620.4139504225273
  episode_reward_mean: 2446.9386167724588
  episode_reward_min: 2273.46328312239
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 2273.46328312239
    - 2620.4139504225273
    policy_a_reward: [328.8496025829804, 308.64294335696184, 447.07666691553896, 271.35601189788207,
      445.87008990267424, 257.5726791815185, 498.62039328317474, 413.3666447540594,
      280.0412789785903, 379.8212534866463]
    policy_p_reward:
    - 471.6679684663554
    - 790.9917007385357
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 498.62039328317474
    p: 790.9917007385357
  policy_reward_mean:
    a: 363.1217564340027
    p: 631.3298346024455
  policy_reward_min:
    a: 257.5726791815185
    p: 471.6679684663554
  sampler_perf:
    mean_action_processing_ms: 0.5589475002708155
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.514712162767546
    mean_inference_ms: 8.469194987866976
    mean_raw_obs_processing_ms: 2.243761854279764
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.1029670238494873
      StateBufferConnector_ms: 0.009173154830932617
      ViewRequirementAgentConnector_ms: 0.20624399185180664
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 2620.4139504225273
    episode_reward_mean: 2446.9386167724588
    episode_reward_min: 2273.46328312239
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 2273.46328312239
      - 2620.4139504225273
      policy_a_reward: [328.8496025829804, 308.64294335696184, 447.07666691553896,
        271.35601189788207, 445.87008990267424, 257.5726791815185, 498.62039328317474,
        413.3666447540594, 280.0412789785903, 379.8212534866463]
      policy_p_reward:
      - 471.6679684663554
      - 790.9917007385357
    num_faulty_episodes: 0
    policy_reward_max:
      a: 498.62039328317474
      p: 790.9917007385357
    policy_reward_mean:
      a: 363.1217564340027
      p: 631.3298346024455
    policy_reward_min:
      a: 257.5726791815185
      p: 471.6679684663554
    sampler_perf:
      mean_action_processing_ms: 0.5589475002708155
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.514712162767546
      mean_inference_ms: 8.469194987866976
      mean_raw_obs_processing_ms: 2.243761854279764
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/2/d/dense_logs/logs_02
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.10110139846801758
    StateBufferConnector_ms: 0.009018182754516602
    ViewRequirementAgentConnector_ms: 0.20783543586730957
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 2582.9406921251475
  episode_reward_mean: 2401.1739104121507
  episode_reward_min: 2219.407128699154
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 2219.407128699154
    - 2582.9406921251475
    policy_a_reward: [243.50182516878814, 283.61196195315426, 278.9017457920718, 405.4208081530173,
      256.4694210764569, 292.72477118893215, 410.6273040676551, 225.34013490003827,
      286.6292273370112, 352.16681925073857]
    policy_p_reward:
    - 751.5013665556656
    - 1015.4524353807665
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 410.6273040676551
    p: 1015.4524353807665
  policy_reward_mean:
    a: 303.5394018887864
    p: 883.4769009682161
  policy_reward_min:
    a: 225.34013490003827
    p: 751.5013665556656
  sampler_perf:
    mean_action_processing_ms: 0.5574107229679838
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.392891630776104
    mean_inference_ms: 8.515608185592262
    mean_raw_obs_processing_ms: 2.2077267316506544
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.10110139846801758
      StateBufferConnector_ms: 0.009018182754516602
      ViewRequirementAgentConnector_ms: 0.20783543586730957
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 2582.9406921251475
    episode_reward_mean: 2401.1739104121507
    episode_reward_min: 2219.407128699154
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 2219.407128699154
      - 2582.9406921251475
      policy_a_reward: [243.50182516878814, 283.61196195315426, 278.9017457920718,
        405.4208081530173, 256.4694210764569, 292.72477118893215, 410.6273040676551,
        225.34013490003827, 286.6292273370112, 352.16681925073857]
      policy_p_reward:
      - 751.5013665556656
      - 1015.4524353807665
    num_faulty_episodes: 0
    policy_reward_max:
      a: 410.6273040676551
      p: 1015.4524353807665
    policy_reward_mean:
      a: 303.5394018887864
      p: 883.4769009682161
    policy_reward_min:
      a: 225.34013490003827
      p: 751.5013665556656
    sampler_perf:
      mean_action_processing_ms: 0.5574107229679838
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.392891630776104
      mean_inference_ms: 8.515608185592262
      mean_raw_obs_processing_ms: 2.2077267316506544
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/2/d/dense_logs/logs_03
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.09831786155700684
    StateBufferConnector_ms: 0.009357929229736328
    ViewRequirementAgentConnector_ms: 0.24189352989196777
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 2772.7720360471494
  episode_reward_mean: 2431.812912598063
  episode_reward_min: 2090.8537891489773
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 2090.8537891489773
    - 2772.7720360471494
    policy_a_reward: [423.80509272297144, 406.62824527034064, 301.4957122107831, 303.4021021285048,
      389.5573657385482, 354.5904674757167, 482.0828226708491, 277.63263684265985,
      402.60981254249407, 295.90943705885974]
    policy_p_reward:
    - 265.96527107783106
    - 959.9468594565661
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 482.0828226708491
    p: 959.9468594565661
  policy_reward_mean:
    a: 363.77136946617276
    p: 612.9560652671986
  policy_reward_min:
    a: 277.63263684265985
    p: 265.96527107783106
  sampler_perf:
    mean_action_processing_ms: 0.5577961381365423
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.384813857812588
    mean_inference_ms: 8.561376093483505
    mean_raw_obs_processing_ms: 2.1842697628590164
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.09831786155700684
      StateBufferConnector_ms: 0.009357929229736328
      ViewRequirementAgentConnector_ms: 0.24189352989196777
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 2772.7720360471494
    episode_reward_mean: 2431.812912598063
    episode_reward_min: 2090.8537891489773
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 2090.8537891489773
      - 2772.7720360471494
      policy_a_reward: [423.80509272297144, 406.62824527034064, 301.4957122107831,
        303.4021021285048, 389.5573657385482, 354.5904674757167, 482.0828226708491,
        277.63263684265985, 402.60981254249407, 295.90943705885974]
      policy_p_reward:
      - 265.96527107783106
      - 959.9468594565661
    num_faulty_episodes: 0
    policy_reward_max:
      a: 482.0828226708491
      p: 959.9468594565661
    policy_reward_mean:
      a: 363.77136946617276
      p: 612.9560652671986
    policy_reward_min:
      a: 277.63263684265985
      p: 265.96527107783106
    sampler_perf:
      mean_action_processing_ms: 0.5577961381365423
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.384813857812588
      mean_inference_ms: 8.561376093483505
      mean_raw_obs_processing_ms: 2.1842697628590164
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/2/d/dense_logs/logs_04
Completing! Saving final snapshot...


Final snapshot saved! All done.
