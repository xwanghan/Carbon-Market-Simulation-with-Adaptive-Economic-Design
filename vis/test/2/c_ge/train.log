seed (final): 39129000
Not restoring trainer...
Restoring agents weights...
Starting with fresh planner weights.
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.08870959281921387
    StateBufferConnector_ms: 0.009649991989135742
    ViewRequirementAgentConnector_ms: 0.28504133224487305
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1980.2396828243113
  episode_reward_mean: 1950.0306715413485
  episode_reward_min: 1919.8216602583857
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1919.8216602583857
    - 1980.2396828243113
    policy_a_reward: [525.4573149541324, 426.0477422899372, 335.9897596344608, 366.50010191858934,
      80.26898484710155, 143.99057809867085, 403.0459196735805, 238.00792931825205,
      483.4296857089508, 513.2318699768355]
    policy_p_reward:
    - 185.55775661416425
    - 198.53370004802744
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 525.4573149541324
    p: 198.53370004802744
  policy_reward_mean:
    a: 351.5969886420511
    p: 192.04572833109586
  policy_reward_min:
    a: 80.26898484710155
    p: 185.55775661416425
  sampler_perf:
    mean_action_processing_ms: 0.5253208373597044
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 11.158736642011387
    mean_inference_ms: 8.699514194876848
    mean_raw_obs_processing_ms: 5.5690468428377615
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.08870959281921387
      StateBufferConnector_ms: 0.009649991989135742
      ViewRequirementAgentConnector_ms: 0.28504133224487305
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1980.2396828243113
    episode_reward_mean: 1950.0306715413485
    episode_reward_min: 1919.8216602583857
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1919.8216602583857
      - 1980.2396828243113
      policy_a_reward: [525.4573149541324, 426.0477422899372, 335.9897596344608, 366.50010191858934,
        80.26898484710155, 143.99057809867085, 403.0459196735805, 238.00792931825205,
        483.4296857089508, 513.2318699768355]
      policy_p_reward:
      - 185.55775661416425
      - 198.53370004802744
    num_faulty_episodes: 0
    policy_reward_max:
      a: 525.4573149541324
      p: 198.53370004802744
    policy_reward_mean:
      a: 351.5969886420511
      p: 192.04572833109586
    policy_reward_min:
      a: 80.26898484710155
      p: 185.55775661416425
    sampler_perf:
      mean_action_processing_ms: 0.5253208373597044
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 11.158736642011387
      mean_inference_ms: 8.699514194876848
      mean_raw_obs_processing_ms: 5.5690468428377615
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/2/c_ge/dense_logs/logs_00
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.10682940483093262
    StateBufferConnector_ms: 0.00903010368347168
    ViewRequirementAgentConnector_ms: 0.19591450691223145
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1902.2261315345022
  episode_reward_mean: 1763.467915251903
  episode_reward_min: 1624.7096989693039
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1624.7096989693039
    - 1902.2261315345022
    policy_a_reward: [312.77761450830855, 437.2237831313859, 216.45934582097252, 159.23325657991097,
      397.04688017004577, 431.88401283830865, 450.70672354554665, 389.6628181969528,
      160.57227637697684, 383.4206455888204]
    policy_p_reward:
    - 101.96881875867841
    - 85.97965498789505
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 450.70672354554665
    p: 101.96881875867841
  policy_reward_mean:
    a: 333.89873567572283
    p: 93.97423687328673
  policy_reward_min:
    a: 159.23325657991097
    p: 85.97965498789505
  sampler_perf:
    mean_action_processing_ms: 0.533761082591115
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 7.857475128326264
    mean_inference_ms: 8.82660568534554
    mean_raw_obs_processing_ms: 3.7900105818406447
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.10682940483093262
      StateBufferConnector_ms: 0.00903010368347168
      ViewRequirementAgentConnector_ms: 0.19591450691223145
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1902.2261315345022
    episode_reward_mean: 1763.467915251903
    episode_reward_min: 1624.7096989693039
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1624.7096989693039
      - 1902.2261315345022
      policy_a_reward: [312.77761450830855, 437.2237831313859, 216.45934582097252,
        159.23325657991097, 397.04688017004577, 431.88401283830865, 450.70672354554665,
        389.6628181969528, 160.57227637697684, 383.4206455888204]
      policy_p_reward:
      - 101.96881875867841
      - 85.97965498789505
    num_faulty_episodes: 0
    policy_reward_max:
      a: 450.70672354554665
      p: 101.96881875867841
    policy_reward_mean:
      a: 333.89873567572283
      p: 93.97423687328673
    policy_reward_min:
      a: 159.23325657991097
      p: 85.97965498789505
    sampler_perf:
      mean_action_processing_ms: 0.533761082591115
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 7.857475128326264
      mean_inference_ms: 8.82660568534554
      mean_raw_obs_processing_ms: 3.7900105818406447
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/2/c_ge/dense_logs/logs_01
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.09410381317138672
    StateBufferConnector_ms: 0.00947713851928711
    ViewRequirementAgentConnector_ms: 0.196075439453125
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1644.922217322149
  episode_reward_mean: 1575.3583360261125
  episode_reward_min: 1505.794454730076
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1644.922217322149
    - 1505.794454730076
    policy_a_reward: [206.12865362679, 434.98288659237085, 428.6541711575623, 115.2176487557177,
      343.3050711171867, 268.7239823568033, 440.7750330298936, 184.00892576394392,
      327.3541242419768, 169.03878875465182]
    policy_p_reward:
    - 116.63378607252231
    - 115.89360058280899
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 440.7750330298936
    p: 116.63378607252231
  policy_reward_mean:
    a: 291.8189285396897
    p: 116.26369332766565
  policy_reward_min:
    a: 115.2176487557177
    p: 115.89360058280899
  sampler_perf:
    mean_action_processing_ms: 0.5288795976937413
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 6.768370373577852
    mean_inference_ms: 8.741568280092324
    mean_raw_obs_processing_ms: 3.1821165459700858
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.09410381317138672
      StateBufferConnector_ms: 0.00947713851928711
      ViewRequirementAgentConnector_ms: 0.196075439453125
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1644.922217322149
    episode_reward_mean: 1575.3583360261125
    episode_reward_min: 1505.794454730076
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1644.922217322149
      - 1505.794454730076
      policy_a_reward: [206.12865362679, 434.98288659237085, 428.6541711575623, 115.2176487557177,
        343.3050711171867, 268.7239823568033, 440.7750330298936, 184.00892576394392,
        327.3541242419768, 169.03878875465182]
      policy_p_reward:
      - 116.63378607252231
      - 115.89360058280899
    num_faulty_episodes: 0
    policy_reward_max:
      a: 440.7750330298936
      p: 116.63378607252231
    policy_reward_mean:
      a: 291.8189285396897
      p: 116.26369332766565
    policy_reward_min:
      a: 115.2176487557177
      p: 115.89360058280899
    sampler_perf:
      mean_action_processing_ms: 0.5288795976937413
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 6.768370373577852
      mean_inference_ms: 8.741568280092324
      mean_raw_obs_processing_ms: 3.1821165459700858
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/2/c_ge/dense_logs/logs_02
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0993192195892334
    StateBufferConnector_ms: 0.009149312973022461
    ViewRequirementAgentConnector_ms: 0.21270513534545898
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1964.2640408953698
  episode_reward_mean: 1759.465065993547
  episode_reward_min: 1554.6660910917242
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1554.6660910917242
    - 1964.2640408953698
    policy_a_reward: [160.46547231865557, 333.7015103007428, 407.39801196803694, 337.4727536081539,
      149.61685601298066, 377.6481525007571, 366.59319582136396, 469.5137018881244,
      199.03290332488982, 398.2027717969959]
    policy_p_reward:
    - 166.01148688315305
    - 153.27331556323657
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 469.5137018881244
    p: 166.01148688315305
  policy_reward_mean:
    a: 319.9645329540701
    p: 159.6424012231948
  policy_reward_min:
    a: 149.61685601298066
    p: 153.27331556323657
  sampler_perf:
    mean_action_processing_ms: 0.5263075239952656
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 6.022409222711032
    mean_inference_ms: 8.682708154017778
    mean_raw_obs_processing_ms: 2.8826558905682047
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.0993192195892334
      StateBufferConnector_ms: 0.009149312973022461
      ViewRequirementAgentConnector_ms: 0.21270513534545898
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1964.2640408953698
    episode_reward_mean: 1759.465065993547
    episode_reward_min: 1554.6660910917242
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1554.6660910917242
      - 1964.2640408953698
      policy_a_reward: [160.46547231865557, 333.7015103007428, 407.39801196803694,
        337.4727536081539, 149.61685601298066, 377.6481525007571, 366.59319582136396,
        469.5137018881244, 199.03290332488982, 398.2027717969959]
      policy_p_reward:
      - 166.01148688315305
      - 153.27331556323657
    num_faulty_episodes: 0
    policy_reward_max:
      a: 469.5137018881244
      p: 166.01148688315305
    policy_reward_mean:
      a: 319.9645329540701
      p: 159.6424012231948
    policy_reward_min:
      a: 149.61685601298066
      p: 153.27331556323657
    sampler_perf:
      mean_action_processing_ms: 0.5263075239952656
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 6.022409222711032
      mean_inference_ms: 8.682708154017778
      mean_raw_obs_processing_ms: 2.8826558905682047
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/2/c_ge/dense_logs/logs_03
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.09109973907470703
    StateBufferConnector_ms: 0.009888410568237305
    ViewRequirementAgentConnector_ms: 0.2026677131652832
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1789.9227928683265
  episode_reward_mean: 1662.649832790512
  episode_reward_min: 1535.3768727126971
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1789.9227928683265
    - 1535.3768727126971
    policy_a_reward: [337.3998958597004, 397.2376381115955, 350.01474621186406, 398.54102982337486,
      263.3149619632578, 300.01852969599014, 182.20704997671174, 358.2188224098915,
      395.35510758593733, 189.71241037943187]
    policy_p_reward:
    - 43.4145208985331
    - 109.86495266473506
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 398.54102982337486
    p: 109.86495266473506
  policy_reward_mean:
    a: 317.2020192017755
    p: 76.63973678163407
  policy_reward_min:
    a: 182.20704997671174
    p: 43.4145208985331
  sampler_perf:
    mean_action_processing_ms: 0.5248157275480921
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 5.6090174747056745
    mean_inference_ms: 8.683586158737189
    mean_raw_obs_processing_ms: 2.701745706289018
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.09109973907470703
      StateBufferConnector_ms: 0.009888410568237305
      ViewRequirementAgentConnector_ms: 0.2026677131652832
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1789.9227928683265
    episode_reward_mean: 1662.649832790512
    episode_reward_min: 1535.3768727126971
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1789.9227928683265
      - 1535.3768727126971
      policy_a_reward: [337.3998958597004, 397.2376381115955, 350.01474621186406,
        398.54102982337486, 263.3149619632578, 300.01852969599014, 182.20704997671174,
        358.2188224098915, 395.35510758593733, 189.71241037943187]
      policy_p_reward:
      - 43.4145208985331
      - 109.86495266473506
    num_faulty_episodes: 0
    policy_reward_max:
      a: 398.54102982337486
      p: 109.86495266473506
    policy_reward_mean:
      a: 317.2020192017755
      p: 76.63973678163407
    policy_reward_min:
      a: 182.20704997671174
      p: 43.4145208985331
    sampler_perf:
      mean_action_processing_ms: 0.5248157275480921
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 5.6090174747056745
      mean_inference_ms: 8.683586158737189
      mean_raw_obs_processing_ms: 2.701745706289018
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/2/c_ge/dense_logs/logs_04
Completing! Saving final snapshot...


Final snapshot saved! All done.
