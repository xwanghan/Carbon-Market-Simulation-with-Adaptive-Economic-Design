seed (final): 39383000
Not restoring trainer...
Restoring agents weights...
Starting with fresh planner weights.
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.1080632209777832
    StateBufferConnector_ms: 0.009703636169433594
    ViewRequirementAgentConnector_ms: 0.2397000789642334
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 2416.040532365222
  episode_reward_mean: 2297.3798304974507
  episode_reward_min: 2178.719128629679
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 2416.040532365222
    - 2178.719128629679
    policy_a_reward: [156.3726989155077, 468.2571080010171, 451.8504597068246, 548.2815401523324,
      359.4747766031319, 219.78671549834942, 210.30938824453204, 290.37178878882287,
      529.1820728024388, 342.00018884906626]
    policy_p_reward:
    - 431.8039489864022
    - 587.0689744464687
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 548.2815401523324
    p: 587.0689744464687
  policy_reward_mean:
    a: 357.5886737562023
    p: 509.4364617164355
  policy_reward_min:
    a: 156.3726989155077
    p: 431.8039489864022
  sampler_perf:
    mean_action_processing_ms: 0.5759331518542504
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.964786613296844
    mean_inference_ms: 7.606758566911586
    mean_raw_obs_processing_ms: 2.5584188526024123
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.1080632209777832
      StateBufferConnector_ms: 0.009703636169433594
      ViewRequirementAgentConnector_ms: 0.2397000789642334
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 2416.040532365222
    episode_reward_mean: 2297.3798304974507
    episode_reward_min: 2178.719128629679
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 2416.040532365222
      - 2178.719128629679
      policy_a_reward: [156.3726989155077, 468.2571080010171, 451.8504597068246, 548.2815401523324,
        359.4747766031319, 219.78671549834942, 210.30938824453204, 290.37178878882287,
        529.1820728024388, 342.00018884906626]
      policy_p_reward:
      - 431.8039489864022
      - 587.0689744464687
    num_faulty_episodes: 0
    policy_reward_max:
      a: 548.2815401523324
      p: 587.0689744464687
    policy_reward_mean:
      a: 357.5886737562023
      p: 509.4364617164355
    policy_reward_min:
      a: 156.3726989155077
      p: 431.8039489864022
    sampler_perf:
      mean_action_processing_ms: 0.5759331518542504
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.964786613296844
      mean_inference_ms: 7.606758566911586
      mean_raw_obs_processing_ms: 2.5584188526024123
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/2/d_gml/dense_logs/logs_00
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.1535177230834961
    StateBufferConnector_ms: 0.015848875045776367
    ViewRequirementAgentConnector_ms: 0.3287017345428467
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 2453.690618730632
  episode_reward_mean: 2343.516730344674
  episode_reward_min: 2233.3428419587162
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 2233.3428419587162
    - 2453.690618730632
    policy_a_reward: [420.3163204576042, 252.57685621264238, 267.66280875690455, 334.4147860638885,
      483.85319467837826, 242.05202803693334, 357.4961626521708, 382.621966907793,
      469.7150181245305, 260.56605077254403]
    policy_p_reward:
    - 474.5188757893065
    - 741.2393922366618
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 483.85319467837826
    p: 741.2393922366618
  policy_reward_mean:
    a: 347.127519266339
    p: 607.8791340129842
  policy_reward_min:
    a: 242.05202803693334
    p: 474.5188757893065
  sampler_perf:
    mean_action_processing_ms: 0.6000859873159068
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 5.252430131742647
    mean_inference_ms: 8.630674916666585
    mean_raw_obs_processing_ms: 2.4905874059869575
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.1535177230834961
      StateBufferConnector_ms: 0.015848875045776367
      ViewRequirementAgentConnector_ms: 0.3287017345428467
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 2453.690618730632
    episode_reward_mean: 2343.516730344674
    episode_reward_min: 2233.3428419587162
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 2233.3428419587162
      - 2453.690618730632
      policy_a_reward: [420.3163204576042, 252.57685621264238, 267.66280875690455,
        334.4147860638885, 483.85319467837826, 242.05202803693334, 357.4961626521708,
        382.621966907793, 469.7150181245305, 260.56605077254403]
      policy_p_reward:
      - 474.5188757893065
      - 741.2393922366618
    num_faulty_episodes: 0
    policy_reward_max:
      a: 483.85319467837826
      p: 741.2393922366618
    policy_reward_mean:
      a: 347.127519266339
      p: 607.8791340129842
    policy_reward_min:
      a: 242.05202803693334
      p: 474.5188757893065
    sampler_perf:
      mean_action_processing_ms: 0.6000859873159068
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 5.252430131742647
      mean_inference_ms: 8.630674916666585
      mean_raw_obs_processing_ms: 2.4905874059869575
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/2/d_gml/dense_logs/logs_01
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.13198256492614746
    StateBufferConnector_ms: 0.010448694229125977
    ViewRequirementAgentConnector_ms: 0.23472905158996582
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 2234.281169819063
  episode_reward_mean: 2174.298807624371
  episode_reward_min: 2114.3164454296793
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 2234.281169819063
    - 2114.3164454296793
    policy_a_reward: [262.68934029014963, 369.35067548040934, 170.27973977083968,
      427.9420531297345, 513.0312946461297, 352.1514125284122, 326.47660435290817,
      223.66450317145046, 361.6731113699171, 329.8543889945327]
    policy_p_reward:
    - 490.9880665018
    - 520.4964250124623
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 513.0312946461297
    p: 520.4964250124623
  policy_reward_mean:
    a: 333.7113123734483
    p: 505.7422457571312
  policy_reward_min:
    a: 170.27973977083968
    p: 490.9880665018
  sampler_perf:
    mean_action_processing_ms: 0.6208189482056721
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 5.3763866106880895
    mean_inference_ms: 9.489030539393822
    mean_raw_obs_processing_ms: 2.534902548488182
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.13198256492614746
      StateBufferConnector_ms: 0.010448694229125977
      ViewRequirementAgentConnector_ms: 0.23472905158996582
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 2234.281169819063
    episode_reward_mean: 2174.298807624371
    episode_reward_min: 2114.3164454296793
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 2234.281169819063
      - 2114.3164454296793
      policy_a_reward: [262.68934029014963, 369.35067548040934, 170.27973977083968,
        427.9420531297345, 513.0312946461297, 352.1514125284122, 326.47660435290817,
        223.66450317145046, 361.6731113699171, 329.8543889945327]
      policy_p_reward:
      - 490.9880665018
      - 520.4964250124623
    num_faulty_episodes: 0
    policy_reward_max:
      a: 513.0312946461297
      p: 520.4964250124623
    policy_reward_mean:
      a: 333.7113123734483
      p: 505.7422457571312
    policy_reward_min:
      a: 170.27973977083968
      p: 490.9880665018
    sampler_perf:
      mean_action_processing_ms: 0.6208189482056721
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 5.3763866106880895
      mean_inference_ms: 9.489030539393822
      mean_raw_obs_processing_ms: 2.534902548488182
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/2/d_gml/dense_logs/logs_02
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.13113617897033691
    StateBufferConnector_ms: 0.011354684829711914
    ViewRequirementAgentConnector_ms: 0.27460455894470215
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 2878.043312455299
  episode_reward_mean: 2681.718784153147
  episode_reward_min: 2485.3942558509953
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 2485.3942558509953
    - 2878.043312455299
    policy_a_reward: [510.1963515159358, 494.76887129172275, 247.12376440744185, 352.6729453004284,
      278.22931956063917, 327.01226232422545, 428.03538396086844, 432.8202199809386,
      505.6116994824999, 503.3231086750882]
    policy_p_reward:
    - 602.4030037748298
    - 681.2406380316766
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 510.1963515159358
    p: 681.2406380316766
  policy_reward_mean:
    a: 407.9793926499789
    p: 641.8218209032532
  policy_reward_min:
    a: 247.12376440744185
    p: 602.4030037748298
  sampler_perf:
    mean_action_processing_ms: 0.6302823310253443
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 5.4568158930864765
    mean_inference_ms: 9.854126191031986
    mean_raw_obs_processing_ms: 2.5695156657892366
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.13113617897033691
      StateBufferConnector_ms: 0.011354684829711914
      ViewRequirementAgentConnector_ms: 0.27460455894470215
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 2878.043312455299
    episode_reward_mean: 2681.718784153147
    episode_reward_min: 2485.3942558509953
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 2485.3942558509953
      - 2878.043312455299
      policy_a_reward: [510.1963515159358, 494.76887129172275, 247.12376440744185,
        352.6729453004284, 278.22931956063917, 327.01226232422545, 428.03538396086844,
        432.8202199809386, 505.6116994824999, 503.3231086750882]
      policy_p_reward:
      - 602.4030037748298
      - 681.2406380316766
    num_faulty_episodes: 0
    policy_reward_max:
      a: 510.1963515159358
      p: 681.2406380316766
    policy_reward_mean:
      a: 407.9793926499789
      p: 641.8218209032532
    policy_reward_min:
      a: 247.12376440744185
      p: 602.4030037748298
    sampler_perf:
      mean_action_processing_ms: 0.6302823310253443
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 5.4568158930864765
      mean_inference_ms: 9.854126191031986
      mean_raw_obs_processing_ms: 2.5695156657892366
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/2/d_gml/dense_logs/logs_03
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.13924241065979004
    StateBufferConnector_ms: 0.011754035949707031
    ViewRequirementAgentConnector_ms: 0.25190114974975586
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 2534.134080223612
  episode_reward_mean: 2403.0308330483304
  episode_reward_min: 2271.9275858730484
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 2534.134080223612
    - 2271.9275858730484
    policy_a_reward: [310.23574980016434, 254.5598204825647, 331.1289634221761, 474.476059811304,
      255.27689679676143, 260.1617338556809, 418.61567632456604, 338.78485627870646,
      324.2366866986728, 459.8258230179223]
    policy_p_reward:
    - 908.4565899106518
    - 470.302809697506
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 474.476059811304
    p: 908.4565899106518
  policy_reward_mean:
    a: 342.73022664885184
    p: 689.3796998040789
  policy_reward_min:
    a: 254.5598204825647
    p: 470.302809697506
  sampler_perf:
    mean_action_processing_ms: 0.6380313780249619
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 5.459494421073123
    mean_inference_ms: 10.119464005626998
    mean_raw_obs_processing_ms: 2.5968182711351493
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.13924241065979004
      StateBufferConnector_ms: 0.011754035949707031
      ViewRequirementAgentConnector_ms: 0.25190114974975586
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 2534.134080223612
    episode_reward_mean: 2403.0308330483304
    episode_reward_min: 2271.9275858730484
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 2534.134080223612
      - 2271.9275858730484
      policy_a_reward: [310.23574980016434, 254.5598204825647, 331.1289634221761,
        474.476059811304, 255.27689679676143, 260.1617338556809, 418.61567632456604,
        338.78485627870646, 324.2366866986728, 459.8258230179223]
      policy_p_reward:
      - 908.4565899106518
      - 470.302809697506
    num_faulty_episodes: 0
    policy_reward_max:
      a: 474.476059811304
      p: 908.4565899106518
    policy_reward_mean:
      a: 342.73022664885184
      p: 689.3796998040789
    policy_reward_min:
      a: 254.5598204825647
      p: 470.302809697506
    sampler_perf:
      mean_action_processing_ms: 0.6380313780249619
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 5.459494421073123
      mean_inference_ms: 10.119464005626998
      mean_raw_obs_processing_ms: 2.5968182711351493
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/2/d_gml/dense_logs/logs_04
Completing! Saving final snapshot...


Final snapshot saved! All done.
