seed (final): 39735000
Not restoring trainer...
Restoring agents weights...
Starting with fresh planner weights.
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.12481808662414551
    StateBufferConnector_ms: 0.011271238327026367
    ViewRequirementAgentConnector_ms: 0.2619326114654541
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 2289.1803687804113
  episode_reward_mean: 2127.4642002473392
  episode_reward_min: 1965.7480317142672
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1965.7480317142672
    - 2289.1803687804113
    policy_a_reward: [381.0458011003663, 518.297723508861, 198.32599409961307, 346.3026098196962,
      443.65333367259916, 448.0607168801026, 548.0059415314798, 481.60021941515794,
      397.3241218463607, 327.37009455280526]
    policy_p_reward:
    - 78.1225695131329
    - 86.819274554501
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 548.0059415314798
    p: 86.819274554501
  policy_reward_mean:
    a: 408.99865564270425
    p: 82.47092203381695
  policy_reward_min:
    a: 198.32599409961307
    p: 78.1225695131329
  sampler_perf:
    mean_action_processing_ms: 0.6422948932457351
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 6.684620223359433
    mean_inference_ms: 9.990272407760163
    mean_raw_obs_processing_ms: 2.8623278269510783
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.12481808662414551
      StateBufferConnector_ms: 0.011271238327026367
      ViewRequirementAgentConnector_ms: 0.2619326114654541
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 2289.1803687804113
    episode_reward_mean: 2127.4642002473392
    episode_reward_min: 1965.7480317142672
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1965.7480317142672
      - 2289.1803687804113
      policy_a_reward: [381.0458011003663, 518.297723508861, 198.32599409961307, 346.3026098196962,
        443.65333367259916, 448.0607168801026, 548.0059415314798, 481.60021941515794,
        397.3241218463607, 327.37009455280526]
      policy_p_reward:
      - 78.1225695131329
      - 86.819274554501
    num_faulty_episodes: 0
    policy_reward_max:
      a: 548.0059415314798
      p: 86.819274554501
    policy_reward_mean:
      a: 408.99865564270425
      p: 82.47092203381695
    policy_reward_min:
      a: 198.32599409961307
      p: 78.1225695131329
    sampler_perf:
      mean_action_processing_ms: 0.6422948932457351
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 6.684620223359433
      mean_inference_ms: 9.990272407760163
      mean_raw_obs_processing_ms: 2.8623278269510783
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/2/c_gml/dense_logs/logs_00
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.14705657958984375
    StateBufferConnector_ms: 0.011628866195678711
    ViewRequirementAgentConnector_ms: 0.2597510814666748
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 2054.664499184378
  episode_reward_mean: 1731.150576828201
  episode_reward_min: 1407.636654472024
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1407.636654472024
    - 2054.664499184378
    policy_a_reward: [58.962614725616554, 265.94081207220705, 482.30723355741696,
      359.90015785060996, 155.99034982699493, 473.6620453291312, 332.0729183701063,
      378.26890544016794, 497.5412034420287, 328.2935866594545]
    policy_p_reward:
    - 84.53548643918174
    - 44.82583994349307
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 497.5412034420287
    p: 84.53548643918174
  policy_reward_mean:
    a: 333.2939827273734
    p: 64.6806631913374
  policy_reward_min:
    a: 58.962614725616554
    p: 44.82583994349307
  sampler_perf:
    mean_action_processing_ms: 0.6752752519392229
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 6.087457026158656
    mean_inference_ms: 10.78989741566417
    mean_raw_obs_processing_ms: 2.6933787228701473
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.14705657958984375
      StateBufferConnector_ms: 0.011628866195678711
      ViewRequirementAgentConnector_ms: 0.2597510814666748
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 2054.664499184378
    episode_reward_mean: 1731.150576828201
    episode_reward_min: 1407.636654472024
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1407.636654472024
      - 2054.664499184378
      policy_a_reward: [58.962614725616554, 265.94081207220705, 482.30723355741696,
        359.90015785060996, 155.99034982699493, 473.6620453291312, 332.0729183701063,
        378.26890544016794, 497.5412034420287, 328.2935866594545]
      policy_p_reward:
      - 84.53548643918174
      - 44.82583994349307
    num_faulty_episodes: 0
    policy_reward_max:
      a: 497.5412034420287
      p: 84.53548643918174
    policy_reward_mean:
      a: 333.2939827273734
      p: 64.6806631913374
    policy_reward_min:
      a: 58.962614725616554
      p: 44.82583994349307
    sampler_perf:
      mean_action_processing_ms: 0.6752752519392229
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 6.087457026158656
      mean_inference_ms: 10.78989741566417
      mean_raw_obs_processing_ms: 2.6933787228701473
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/2/c_gml/dense_logs/logs_01
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.11133551597595215
    StateBufferConnector_ms: 0.010609626770019531
    ViewRequirementAgentConnector_ms: 0.2544224262237549
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 2187.423914961844
  episode_reward_mean: 1822.980856849836
  episode_reward_min: 1458.537798737828
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1458.537798737828
    - 2187.423914961844
    policy_a_reward: [209.1234779146214, 374.6079030118657, 372.03591984029475, 265.4472257624807,
      214.59883063620563, 179.53589796331138, 550.3092781509649, 485.44605031603004,
      463.1878906790048, 496.38420070848883]
    policy_p_reward:
    - 22.724441572363222
    - 12.560597144040992
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 550.3092781509649
    p: 22.724441572363222
  policy_reward_mean:
    a: 361.06766749832684
    p: 17.642519358202108
  policy_reward_min:
    a: 179.53589796331138
    p: 12.560597144040992
  sampler_perf:
    mean_action_processing_ms: 0.6634072412418414
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 5.727643890431688
    mean_inference_ms: 10.724917322218538
    mean_raw_obs_processing_ms: 2.607058875168426
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.11133551597595215
      StateBufferConnector_ms: 0.010609626770019531
      ViewRequirementAgentConnector_ms: 0.2544224262237549
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 2187.423914961844
    episode_reward_mean: 1822.980856849836
    episode_reward_min: 1458.537798737828
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1458.537798737828
      - 2187.423914961844
      policy_a_reward: [209.1234779146214, 374.6079030118657, 372.03591984029475,
        265.4472257624807, 214.59883063620563, 179.53589796331138, 550.3092781509649,
        485.44605031603004, 463.1878906790048, 496.38420070848883]
      policy_p_reward:
      - 22.724441572363222
      - 12.560597144040992
    num_faulty_episodes: 0
    policy_reward_max:
      a: 550.3092781509649
      p: 22.724441572363222
    policy_reward_mean:
      a: 361.06766749832684
      p: 17.642519358202108
    policy_reward_min:
      a: 179.53589796331138
      p: 12.560597144040992
    sampler_perf:
      mean_action_processing_ms: 0.6634072412418414
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 5.727643890431688
      mean_inference_ms: 10.724917322218538
      mean_raw_obs_processing_ms: 2.607058875168426
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/2/c_gml/dense_logs/logs_02
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.12769103050231934
    StateBufferConnector_ms: 0.013709068298339844
    ViewRequirementAgentConnector_ms: 0.24654269218444824
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 2373.9072882699456
  episode_reward_mean: 2210.69392384943
  episode_reward_min: 2047.4805594289144
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 2047.4805594289144
    - 2373.9072882699456
    policy_a_reward: [422.0526957029622, 287.5822510029658, 215.24568279325246, 426.3146185716179,
      531.3353660936558, 334.57618304851076, 459.54660529612244, 554.518926994775,
      446.22694700617194, 210.5823917305512]
    policy_p_reward:
    - 164.94994526446098
    - 368.4562341938223
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 554.518926994775
    p: 368.4562341938223
  policy_reward_mean:
    a: 388.7981668240585
    p: 266.70308972914165
  policy_reward_min:
    a: 210.5823917305512
    p: 164.94994526446098
  sampler_perf:
    mean_action_processing_ms: 0.6677698100107661
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 5.498559042431604
    mean_inference_ms: 10.866697164608919
    mean_raw_obs_processing_ms: 2.6103292090603256
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.12769103050231934
      StateBufferConnector_ms: 0.013709068298339844
      ViewRequirementAgentConnector_ms: 0.24654269218444824
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 2373.9072882699456
    episode_reward_mean: 2210.69392384943
    episode_reward_min: 2047.4805594289144
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 2047.4805594289144
      - 2373.9072882699456
      policy_a_reward: [422.0526957029622, 287.5822510029658, 215.24568279325246,
        426.3146185716179, 531.3353660936558, 334.57618304851076, 459.54660529612244,
        554.518926994775, 446.22694700617194, 210.5823917305512]
      policy_p_reward:
      - 164.94994526446098
      - 368.4562341938223
    num_faulty_episodes: 0
    policy_reward_max:
      a: 554.518926994775
      p: 368.4562341938223
    policy_reward_mean:
      a: 388.7981668240585
      p: 266.70308972914165
    policy_reward_min:
      a: 210.5823917305512
      p: 164.94994526446098
    sampler_perf:
      mean_action_processing_ms: 0.6677698100107661
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 5.498559042431604
      mean_inference_ms: 10.866697164608919
      mean_raw_obs_processing_ms: 2.6103292090603256
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/2/c_gml/dense_logs/logs_03
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.1320958137512207
    StateBufferConnector_ms: 0.012892484664916992
    ViewRequirementAgentConnector_ms: 0.27411580085754395
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 2326.933203037614
  episode_reward_mean: 2223.468831512896
  episode_reward_min: 2120.0044599881776
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 2120.0044599881776
    - 2326.933203037614
    policy_a_reward: [363.8386255218042, 183.47226980334926, 204.2460452552433, 475.50664674529827,
      443.0454996457338, 318.55656811339287, 193.97182787011616, 553.7067707068676,
      435.8857496526967, 523.7501066750416]
    policy_p_reward:
    - 449.89537301675495
    - 301.06218001949304
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 553.7067707068676
    p: 449.89537301675495
  policy_reward_mean:
    a: 369.59801099895435
    p: 375.478776518124
  policy_reward_min:
    a: 183.47226980334926
    p: 301.06218001949304
  sampler_perf:
    mean_action_processing_ms: 0.6682084398906453
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 5.416140657384507
    mean_inference_ms: 11.017468203453483
    mean_raw_obs_processing_ms: 2.622241928118889
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.1320958137512207
      StateBufferConnector_ms: 0.012892484664916992
      ViewRequirementAgentConnector_ms: 0.27411580085754395
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 2326.933203037614
    episode_reward_mean: 2223.468831512896
    episode_reward_min: 2120.0044599881776
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 2120.0044599881776
      - 2326.933203037614
      policy_a_reward: [363.8386255218042, 183.47226980334926, 204.2460452552433,
        475.50664674529827, 443.0454996457338, 318.55656811339287, 193.97182787011616,
        553.7067707068676, 435.8857496526967, 523.7501066750416]
      policy_p_reward:
      - 449.89537301675495
      - 301.06218001949304
    num_faulty_episodes: 0
    policy_reward_max:
      a: 553.7067707068676
      p: 449.89537301675495
    policy_reward_mean:
      a: 369.59801099895435
      p: 375.478776518124
    policy_reward_min:
      a: 183.47226980334926
      p: 301.06218001949304
    sampler_perf:
      mean_action_processing_ms: 0.6682084398906453
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 5.416140657384507
      mean_inference_ms: 11.017468203453483
      mean_raw_obs_processing_ms: 2.622241928118889
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/2/c_gml/dense_logs/logs_04
Completing! Saving final snapshot...


Final snapshot saved! All done.
