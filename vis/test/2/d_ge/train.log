seed (final): 39203000
Not restoring trainer...
Restoring agents weights...
Starting with fresh planner weights.
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.12208223342895508
    StateBufferConnector_ms: 0.013041496276855469
    ViewRequirementAgentConnector_ms: 0.2781093120574951
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 2767.1673688547235
  episode_reward_mean: 2623.3122053100624
  episode_reward_min: 2479.4570417654013
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 2479.4570417654013
    - 2767.1673688547235
    policy_a_reward: [456.0587433614169, 415.3708112690717, 322.5038679632924, 447.4950132023542,
      484.32036898447086, 369.3365992826318, 349.36166118832296, 456.02519490862227,
      354.35568305028164, 282.11139567667396]
    policy_p_reward:
    - 353.7082369848065
    - 955.9768347481858
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 484.32036898447086
    p: 955.9768347481858
  policy_reward_mean:
    a: 393.69393388871384
    p: 654.8425358664961
  policy_reward_min:
    a: 282.11139567667396
    p: 353.7082369848065
  sampler_perf:
    mean_action_processing_ms: 0.6432590370406648
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 5.610537386226083
    mean_inference_ms: 10.107077524333656
    mean_raw_obs_processing_ms: 2.69757107108415
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.12208223342895508
      StateBufferConnector_ms: 0.013041496276855469
      ViewRequirementAgentConnector_ms: 0.2781093120574951
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 2767.1673688547235
    episode_reward_mean: 2623.3122053100624
    episode_reward_min: 2479.4570417654013
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 2479.4570417654013
      - 2767.1673688547235
      policy_a_reward: [456.0587433614169, 415.3708112690717, 322.5038679632924, 447.4950132023542,
        484.32036898447086, 369.3365992826318, 349.36166118832296, 456.02519490862227,
        354.35568305028164, 282.11139567667396]
      policy_p_reward:
      - 353.7082369848065
      - 955.9768347481858
    num_faulty_episodes: 0
    policy_reward_max:
      a: 484.32036898447086
      p: 955.9768347481858
    policy_reward_mean:
      a: 393.69393388871384
      p: 654.8425358664961
    policy_reward_min:
      a: 282.11139567667396
      p: 353.7082369848065
    sampler_perf:
      mean_action_processing_ms: 0.6432590370406648
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 5.610537386226083
      mean_inference_ms: 10.107077524333656
      mean_raw_obs_processing_ms: 2.69757107108415
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/2/d_ge/dense_logs/logs_00
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.10988116264343262
    StateBufferConnector_ms: 0.010448694229125977
    ViewRequirementAgentConnector_ms: 0.23205280303955078
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 2477.333811925235
  episode_reward_mean: 2420.987906334798
  episode_reward_min: 2364.642000744361
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 2477.333811925235
    - 2364.642000744361
    policy_a_reward: [431.88010805789014, 358.114580392641, 333.88423033077623, 289.51111991516746,
      464.76223665053743, 472.9469576694107, 412.75279855440596, 428.51293289085737,
      362.60998684529073, 427.20738782516423]
    policy_p_reward:
    - 599.1815365782312
    - 260.611936959243
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 472.9469576694107
    p: 599.1815365782312
  policy_reward_mean:
    a: 398.21823391321414
    p: 429.8967367687371
  policy_reward_min:
    a: 289.51111991516746
    p: 260.611936959243
  sampler_perf:
    mean_action_processing_ms: 0.6330949324113386
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 5.2339766289923455
    mean_inference_ms: 10.084669549505671
    mean_raw_obs_processing_ms: 2.560596008758088
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.10988116264343262
      StateBufferConnector_ms: 0.010448694229125977
      ViewRequirementAgentConnector_ms: 0.23205280303955078
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 2477.333811925235
    episode_reward_mean: 2420.987906334798
    episode_reward_min: 2364.642000744361
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 2477.333811925235
      - 2364.642000744361
      policy_a_reward: [431.88010805789014, 358.114580392641, 333.88423033077623,
        289.51111991516746, 464.76223665053743, 472.9469576694107, 412.75279855440596,
        428.51293289085737, 362.60998684529073, 427.20738782516423]
      policy_p_reward:
      - 599.1815365782312
      - 260.611936959243
    num_faulty_episodes: 0
    policy_reward_max:
      a: 472.9469576694107
      p: 599.1815365782312
    policy_reward_mean:
      a: 398.21823391321414
      p: 429.8967367687371
    policy_reward_min:
      a: 289.51111991516746
      p: 260.611936959243
    sampler_perf:
      mean_action_processing_ms: 0.6330949324113386
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 5.2339766289923455
      mean_inference_ms: 10.084669549505671
      mean_raw_obs_processing_ms: 2.560596008758088
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/2/d_ge/dense_logs/logs_01
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.13809800148010254
    StateBufferConnector_ms: 0.014615058898925781
    ViewRequirementAgentConnector_ms: 0.3187596797943115
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 2614.3704605915127
  episode_reward_mean: 2284.226598989938
  episode_reward_min: 1954.0827373883635
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1954.0827373883635
    - 2614.3704605915127
    policy_a_reward: [354.146720109401, 402.4442444180241, 493.0430609384013, 312.0565117335386,
      204.9147418304484, 456.2469187051961, 298.56827415321396, 442.0964029664377,
      444.16396716475305, 454.24346597085474]
    policy_p_reward:
    - 187.47745835855125
    - 519.0514316310565
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 493.0430609384013
    p: 519.0514316310565
  policy_reward_mean:
    a: 386.1924307990269
    p: 353.26444499480385
  policy_reward_min:
    a: 204.9147418304484
    p: 187.47745835855125
  sampler_perf:
    mean_action_processing_ms: 0.6280477486952871
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 5.552386220973941
    mean_inference_ms: 10.221154748559554
    mean_raw_obs_processing_ms: 2.504902788196541
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.13809800148010254
      StateBufferConnector_ms: 0.014615058898925781
      ViewRequirementAgentConnector_ms: 0.3187596797943115
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 2614.3704605915127
    episode_reward_mean: 2284.226598989938
    episode_reward_min: 1954.0827373883635
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1954.0827373883635
      - 2614.3704605915127
      policy_a_reward: [354.146720109401, 402.4442444180241, 493.0430609384013, 312.0565117335386,
        204.9147418304484, 456.2469187051961, 298.56827415321396, 442.0964029664377,
        444.16396716475305, 454.24346597085474]
      policy_p_reward:
      - 187.47745835855125
      - 519.0514316310565
    num_faulty_episodes: 0
    policy_reward_max:
      a: 493.0430609384013
      p: 519.0514316310565
    policy_reward_mean:
      a: 386.1924307990269
      p: 353.26444499480385
    policy_reward_min:
      a: 204.9147418304484
      p: 187.47745835855125
    sampler_perf:
      mean_action_processing_ms: 0.6280477486952871
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 5.552386220973941
      mean_inference_ms: 10.221154748559554
      mean_raw_obs_processing_ms: 2.504902788196541
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/2/d_ge/dense_logs/logs_02
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.10666251182556152
    StateBufferConnector_ms: 0.010031461715698242
    ViewRequirementAgentConnector_ms: 0.22554397583007812
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 2387.310967657154
  episode_reward_mean: 2129.0386110629033
  episode_reward_min: 1870.7662544686527
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1870.7662544686527
    - 2387.310967657154
    policy_a_reward: [252.56463318659223, 326.26249355007127, 345.444336183753, 259.2979054423526,
      376.47869777742767, 304.8908085466575, 432.2837332207361, 354.9628520488523,
      289.9709059108633, 337.38946085775615]
    policy_p_reward:
    - 310.71818832846134
    - 667.8132070722926
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 432.2837332207361
    p: 667.8132070722926
  policy_reward_mean:
    a: 327.95458267250626
    p: 489.26569770037696
  policy_reward_min:
    a: 252.56463318659223
    p: 310.71818832846134
  sampler_perf:
    mean_action_processing_ms: 0.6181435963918065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 5.324433530228904
    mean_inference_ms: 10.106548555251184
    mean_raw_obs_processing_ms: 2.456815167703014
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.10666251182556152
      StateBufferConnector_ms: 0.010031461715698242
      ViewRequirementAgentConnector_ms: 0.22554397583007812
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 2387.310967657154
    episode_reward_mean: 2129.0386110629033
    episode_reward_min: 1870.7662544686527
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1870.7662544686527
      - 2387.310967657154
      policy_a_reward: [252.56463318659223, 326.26249355007127, 345.444336183753,
        259.2979054423526, 376.47869777742767, 304.8908085466575, 432.2837332207361,
        354.9628520488523, 289.9709059108633, 337.38946085775615]
      policy_p_reward:
      - 310.71818832846134
      - 667.8132070722926
    num_faulty_episodes: 0
    policy_reward_max:
      a: 432.2837332207361
      p: 667.8132070722926
    policy_reward_mean:
      a: 327.95458267250626
      p: 489.26569770037696
    policy_reward_min:
      a: 252.56463318659223
      p: 310.71818832846134
    sampler_perf:
      mean_action_processing_ms: 0.6181435963918065
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 5.324433530228904
      mean_inference_ms: 10.106548555251184
      mean_raw_obs_processing_ms: 2.456815167703014
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/2/d_ge/dense_logs/logs_03
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.10629892349243164
    StateBufferConnector_ms: 0.009894371032714844
    ViewRequirementAgentConnector_ms: 0.21647214889526367
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 2579.414682220112
  episode_reward_mean: 2570.0871261613597
  episode_reward_min: 2560.7595701026075
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 2579.414682220112
    - 2560.7595701026075
    policy_a_reward: [419.47076958262454, 365.92935019394287, 421.4647176171897, 436.6620680769741,
      327.2721488573345, 398.5179009514421, 401.7964401116892, 448.254119364012, 463.41465853984124,
      406.649479021535]
    policy_p_reward:
    - 608.615627892045
    - 442.1269721140817
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 463.41465853984124
    p: 608.615627892045
  policy_reward_mean:
    a: 408.94316523165855
    p: 525.3713000030634
  policy_reward_min:
    a: 327.2721488573345
    p: 442.1269721140817
  sampler_perf:
    mean_action_processing_ms: 0.6126981885468469
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 5.150599176528119
    mean_inference_ms: 10.094600789597491
    mean_raw_obs_processing_ms: 2.4054290675392442
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.10629892349243164
      StateBufferConnector_ms: 0.009894371032714844
      ViewRequirementAgentConnector_ms: 0.21647214889526367
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 2579.414682220112
    episode_reward_mean: 2570.0871261613597
    episode_reward_min: 2560.7595701026075
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 2579.414682220112
      - 2560.7595701026075
      policy_a_reward: [419.47076958262454, 365.92935019394287, 421.4647176171897,
        436.6620680769741, 327.2721488573345, 398.5179009514421, 401.7964401116892,
        448.254119364012, 463.41465853984124, 406.649479021535]
      policy_p_reward:
      - 608.615627892045
      - 442.1269721140817
    num_faulty_episodes: 0
    policy_reward_max:
      a: 463.41465853984124
      p: 608.615627892045
    policy_reward_mean:
      a: 408.94316523165855
      p: 525.3713000030634
    policy_reward_min:
      a: 327.2721488573345
      p: 442.1269721140817
    sampler_perf:
      mean_action_processing_ms: 0.6126981885468469
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 5.150599176528119
      mean_inference_ms: 10.094600789597491
      mean_raw_obs_processing_ms: 2.4054290675392442
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/2/d_ge/dense_logs/logs_04
Completing! Saving final snapshot...


Final snapshot saved! All done.
