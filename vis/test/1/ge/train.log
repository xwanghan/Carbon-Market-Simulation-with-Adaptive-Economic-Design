seed (final): 35432000
Not restoring trainer...
Restoring agents weights...
Starting with fresh planner weights.
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.08131265640258789
    StateBufferConnector_ms: 0.008404254913330078
    ViewRequirementAgentConnector_ms: 0.19035935401916504
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 857.5309306492854
  episode_reward_mean: 750.7879709565598
  episode_reward_min: 644.0450112638341
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 644.0450112638341
    - 857.5309306492854
    policy_a_reward: [101.27954839699154, 72.60356293716713, 93.87661180197487, 95.16382598989911,
      61.42146213780326, 99.25742089449638, 130.84310155628813, 130.7166926338588,
      103.0999564177506, 87.30809418426729]
    policy_p_reward:
    - 219.70000000000024
    - 306.3056649626261
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 130.84310155628813
    p: 306.3056649626261
  policy_reward_mean:
    a: 97.5570276950497
    p: 263.0028324813132
  policy_reward_min:
    a: 61.42146213780326
    p: 219.70000000000024
  sampler_perf:
    mean_action_processing_ms: 0.541558046778757
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3.87548448558815
    mean_inference_ms: 6.507122588014888
    mean_raw_obs_processing_ms: 2.0425871698680274
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.08131265640258789
      StateBufferConnector_ms: 0.008404254913330078
      ViewRequirementAgentConnector_ms: 0.19035935401916504
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 857.5309306492854
    episode_reward_mean: 750.7879709565598
    episode_reward_min: 644.0450112638341
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 644.0450112638341
      - 857.5309306492854
      policy_a_reward: [101.27954839699154, 72.60356293716713, 93.87661180197487,
        95.16382598989911, 61.42146213780326, 99.25742089449638, 130.84310155628813,
        130.7166926338588, 103.0999564177506, 87.30809418426729]
      policy_p_reward:
      - 219.70000000000024
      - 306.3056649626261
    num_faulty_episodes: 0
    policy_reward_max:
      a: 130.84310155628813
      p: 306.3056649626261
    policy_reward_mean:
      a: 97.5570276950497
      p: 263.0028324813132
    policy_reward_min:
      a: 61.42146213780326
      p: 219.70000000000024
    sampler_perf:
      mean_action_processing_ms: 0.541558046778757
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 3.87548448558815
      mean_inference_ms: 6.507122588014888
      mean_raw_obs_processing_ms: 2.0425871698680274
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/ge1/dense_logs/logs_00
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.08956193923950195
    StateBufferConnector_ms: 0.008422136306762695
    ViewRequirementAgentConnector_ms: 0.18985271453857422
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 725.0098594671972
  episode_reward_mean: 704.1921634758367
  episode_reward_min: 683.3744674844761
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 683.3744674844761
    - 725.0098594671972
    policy_a_reward: [91.4662454508026, 109.13458967303741, 70.14334043473741, 95.9380108408346,
      78.69228108506543, 99.12431821247603, 110.74934152920102, 97.323727666283, 66.85153541105463,
      94.67760331484988]
    policy_p_reward:
    - 238.00000000000023
    - 256.28333333333353
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 110.74934152920102
    p: 256.28333333333353
  policy_reward_mean:
    a: 91.4100993618342
    p: 247.14166666666688
  policy_reward_min:
    a: 66.85153541105463
    p: 238.00000000000023
  sampler_perf:
    mean_action_processing_ms: 0.5322007627992126
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3.9553937616643613
    mean_inference_ms: 6.544778397033265
    mean_raw_obs_processing_ms: 2.0067615585250933
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.08956193923950195
      StateBufferConnector_ms: 0.008422136306762695
      ViewRequirementAgentConnector_ms: 0.18985271453857422
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 725.0098594671972
    episode_reward_mean: 704.1921634758367
    episode_reward_min: 683.3744674844761
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 683.3744674844761
      - 725.0098594671972
      policy_a_reward: [91.4662454508026, 109.13458967303741, 70.14334043473741, 95.9380108408346,
        78.69228108506543, 99.12431821247603, 110.74934152920102, 97.323727666283,
        66.85153541105463, 94.67760331484988]
      policy_p_reward:
      - 238.00000000000023
      - 256.28333333333353
    num_faulty_episodes: 0
    policy_reward_max:
      a: 110.74934152920102
      p: 256.28333333333353
    policy_reward_mean:
      a: 91.4100993618342
      p: 247.14166666666688
    policy_reward_min:
      a: 66.85153541105463
      p: 238.00000000000023
    sampler_perf:
      mean_action_processing_ms: 0.5322007627992126
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 3.9553937616643613
      mean_inference_ms: 6.544778397033265
      mean_raw_obs_processing_ms: 2.0067615585250933
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/ge1/dense_logs/logs_01
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.09331703186035156
    StateBufferConnector_ms: 0.00820159912109375
    ViewRequirementAgentConnector_ms: 0.21204352378845215
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 691.4391285378302
  episode_reward_mean: 678.3524087642963
  episode_reward_min: 665.2656889907623
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 665.2656889907623
    - 691.4391285378302
    policy_a_reward: [105.64818998670877, 107.29598206188697, 69.2520824843071, 71.78457705137608,
      82.50024202186437, 95.83805670778834, 118.57371297337163, 86.85220922101172,
      72.33428365058418, 78.9408659850652]
    policy_p_reward:
    - 228.78461538461565
    - 238.90000000000026
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 118.57371297337163
    p: 238.90000000000026
  policy_reward_mean:
    a: 88.90202021439644
    p: 233.84230769230794
  policy_reward_min:
    a: 69.2520824843071
    p: 228.78461538461565
  sampler_perf:
    mean_action_processing_ms: 0.5220302655488789
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3.984390458291884
    mean_inference_ms: 6.980204884010025
    mean_raw_obs_processing_ms: 1.9766439365435249
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.09331703186035156
      StateBufferConnector_ms: 0.00820159912109375
      ViewRequirementAgentConnector_ms: 0.21204352378845215
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 691.4391285378302
    episode_reward_mean: 678.3524087642963
    episode_reward_min: 665.2656889907623
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 665.2656889907623
      - 691.4391285378302
      policy_a_reward: [105.64818998670877, 107.29598206188697, 69.2520824843071,
        71.78457705137608, 82.50024202186437, 95.83805670778834, 118.57371297337163,
        86.85220922101172, 72.33428365058418, 78.9408659850652]
      policy_p_reward:
      - 228.78461538461565
      - 238.90000000000026
    num_faulty_episodes: 0
    policy_reward_max:
      a: 118.57371297337163
      p: 238.90000000000026
    policy_reward_mean:
      a: 88.90202021439644
      p: 233.84230769230794
    policy_reward_min:
      a: 69.2520824843071
      p: 228.78461538461565
    sampler_perf:
      mean_action_processing_ms: 0.5220302655488789
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 3.984390458291884
      mean_inference_ms: 6.980204884010025
      mean_raw_obs_processing_ms: 1.9766439365435249
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/ge1/dense_logs/logs_02
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0953972339630127
    StateBufferConnector_ms: 0.008535385131835938
    ViewRequirementAgentConnector_ms: 0.18818378448486328
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 779.2523236910474
  episode_reward_mean: 743.8441747203964
  episode_reward_min: 708.4360257497453
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 708.4360257497453
    - 779.2523236910474
    policy_a_reward: [107.32615437530646, 71.68035188068859, 90.27855289509773, 99.43149068675703,
      86.68614257855761, 128.65001054793495, 98.63789518971338, 117.20971540498984,
      82.77705629319425, 79.77977391478088]
    policy_p_reward:
    - 253.03333333333353
    - 272.1978723404258
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 128.65001054793495
    p: 272.1978723404258
  policy_reward_mean:
    a: 96.24571437670207
    p: 262.6156028368797
  policy_reward_min:
    a: 71.68035188068859
    p: 253.03333333333353
  sampler_perf:
    mean_action_processing_ms: 0.5211309454906946
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3.93813946793998
    mean_inference_ms: 7.2601128673029205
    mean_raw_obs_processing_ms: 1.9767137839161475
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.0953972339630127
      StateBufferConnector_ms: 0.008535385131835938
      ViewRequirementAgentConnector_ms: 0.18818378448486328
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 779.2523236910474
    episode_reward_mean: 743.8441747203964
    episode_reward_min: 708.4360257497453
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 708.4360257497453
      - 779.2523236910474
      policy_a_reward: [107.32615437530646, 71.68035188068859, 90.27855289509773,
        99.43149068675703, 86.68614257855761, 128.65001054793495, 98.63789518971338,
        117.20971540498984, 82.77705629319425, 79.77977391478088]
      policy_p_reward:
      - 253.03333333333353
      - 272.1978723404258
    num_faulty_episodes: 0
    policy_reward_max:
      a: 128.65001054793495
      p: 272.1978723404258
    policy_reward_mean:
      a: 96.24571437670207
      p: 262.6156028368797
    policy_reward_min:
      a: 71.68035188068859
      p: 253.03333333333353
    sampler_perf:
      mean_action_processing_ms: 0.5211309454906946
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 3.93813946793998
      mean_inference_ms: 7.2601128673029205
      mean_raw_obs_processing_ms: 1.9767137839161475
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/ge1/dense_logs/logs_03
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.08657574653625488
    StateBufferConnector_ms: 0.009185075759887695
    ViewRequirementAgentConnector_ms: 0.19186735153198242
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 706.8614647534025
  episode_reward_mean: 693.9037646536023
  episode_reward_min: 680.946064553802
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 706.8614647534025
    - 680.946064553802
    policy_a_reward: [112.55183181408795, 67.26229468441213, 107.5265293642632, 79.66143186432471,
      93.59783856477723, 107.64011535769322, 104.72182492240982, 75.37103595244825,
      69.25037869847259, 86.96270962278123]
    policy_p_reward:
    - 246.26153846153875
    - 237.00000000000028
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 112.55183181408795
    p: 246.26153846153875
  policy_reward_mean:
    a: 90.45459908456704
    p: 241.63076923076952
  policy_reward_min:
    a: 67.26229468441213
    p: 237.00000000000028
  sampler_perf:
    mean_action_processing_ms: 0.5170240825483772
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3.924324339935657
    mean_inference_ms: 7.433431141856002
    mean_raw_obs_processing_ms: 1.9613121662650859
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.08657574653625488
      StateBufferConnector_ms: 0.009185075759887695
      ViewRequirementAgentConnector_ms: 0.19186735153198242
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 706.8614647534025
    episode_reward_mean: 693.9037646536023
    episode_reward_min: 680.946064553802
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 706.8614647534025
      - 680.946064553802
      policy_a_reward: [112.55183181408795, 67.26229468441213, 107.5265293642632,
        79.66143186432471, 93.59783856477723, 107.64011535769322, 104.72182492240982,
        75.37103595244825, 69.25037869847259, 86.96270962278123]
      policy_p_reward:
      - 246.26153846153875
      - 237.00000000000028
    num_faulty_episodes: 0
    policy_reward_max:
      a: 112.55183181408795
      p: 246.26153846153875
    policy_reward_mean:
      a: 90.45459908456704
      p: 241.63076923076952
    policy_reward_min:
      a: 67.26229468441213
      p: 237.00000000000028
    sampler_perf:
      mean_action_processing_ms: 0.5170240825483772
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 3.924324339935657
      mean_inference_ms: 7.433431141856002
      mean_raw_obs_processing_ms: 1.9613121662650859
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/ge1/dense_logs/logs_04
Completing! Saving final snapshot...


Final snapshot saved! All done.
seed (final): 37391000
Not restoring trainer...
Restoring agents weights...
Starting with fresh planner weights.
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.09695291519165039
    StateBufferConnector_ms: 0.00966787338256836
    ViewRequirementAgentConnector_ms: 0.24465322494506836
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 655.9870726055888
  episode_reward_mean: 642.8121429751129
  episode_reward_min: 629.6372133446371
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 629.6372133446371
    - 655.9870726055888
    policy_a_reward: [88.60090422294674, 78.10247595271503, 116.19663307716344, 105.6697916387136,
      89.93665869994692, 119.90172347319518, 122.75919933218408, 99.7301808529495,
      66.40551899352816, 69.16488658925313]
    policy_p_reward:
    - 151.13074975315357
    - 178.02556336447785
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 122.75919933218408
    p: 178.02556336447785
  policy_reward_mean:
    a: 95.64679728325959
    p: 164.5781565588157
  policy_reward_min:
    a: 66.40551899352816
    p: 151.13074975315357
  sampler_perf:
    mean_action_processing_ms: 0.59421381312692
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.383714851029143
    mean_inference_ms: 8.087103476305446
    mean_raw_obs_processing_ms: 2.2569159547725834
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.09695291519165039
      StateBufferConnector_ms: 0.00966787338256836
      ViewRequirementAgentConnector_ms: 0.24465322494506836
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 655.9870726055888
    episode_reward_mean: 642.8121429751129
    episode_reward_min: 629.6372133446371
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 629.6372133446371
      - 655.9870726055888
      policy_a_reward: [88.60090422294674, 78.10247595271503, 116.19663307716344,
        105.6697916387136, 89.93665869994692, 119.90172347319518, 122.75919933218408,
        99.7301808529495, 66.40551899352816, 69.16488658925313]
      policy_p_reward:
      - 151.13074975315357
      - 178.02556336447785
    num_faulty_episodes: 0
    policy_reward_max:
      a: 122.75919933218408
      p: 178.02556336447785
    policy_reward_mean:
      a: 95.64679728325959
      p: 164.5781565588157
    policy_reward_min:
      a: 66.40551899352816
      p: 151.13074975315357
    sampler_perf:
      mean_action_processing_ms: 0.59421381312692
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.383714851029143
      mean_inference_ms: 8.087103476305446
      mean_raw_obs_processing_ms: 2.2569159547725834
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/ge1/dense_logs/logs_00
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.11748075485229492
    StateBufferConnector_ms: 0.011563301086425781
    ViewRequirementAgentConnector_ms: 0.21294951438903809
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 788.6179082759505
  episode_reward_mean: 740.0253754639864
  episode_reward_min: 691.4328426520223
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 691.4328426520223
    - 788.6179082759505
    policy_a_reward: [116.83457226280878, 88.8712179805789, 79.27288755146999, 114.57970665906267,
      90.57729740601302, 79.82316772569932, 149.46016534897618, 134.84143666653816,
      98.57435512900042, 107.90024390280142]
    policy_p_reward:
    - 201.29716079208993
    - 218.01853950293702
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 149.46016534897618
    p: 218.01853950293702
  policy_reward_mean:
    a: 106.07350506329487
    p: 209.65785014751347
  policy_reward_min:
    a: 79.27288755146999
    p: 201.29716079208993
  sampler_perf:
    mean_action_processing_ms: 0.5963725167197305
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.458996918532518
    mean_inference_ms: 7.937928179760913
    mean_raw_obs_processing_ms: 2.273853008563702
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.11748075485229492
      StateBufferConnector_ms: 0.011563301086425781
      ViewRequirementAgentConnector_ms: 0.21294951438903809
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 788.6179082759505
    episode_reward_mean: 740.0253754639864
    episode_reward_min: 691.4328426520223
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 691.4328426520223
      - 788.6179082759505
      policy_a_reward: [116.83457226280878, 88.8712179805789, 79.27288755146999, 114.57970665906267,
        90.57729740601302, 79.82316772569932, 149.46016534897618, 134.84143666653816,
        98.57435512900042, 107.90024390280142]
      policy_p_reward:
      - 201.29716079208993
      - 218.01853950293702
    num_faulty_episodes: 0
    policy_reward_max:
      a: 149.46016534897618
      p: 218.01853950293702
    policy_reward_mean:
      a: 106.07350506329487
      p: 209.65785014751347
    policy_reward_min:
      a: 79.27288755146999
      p: 201.29716079208993
    sampler_perf:
      mean_action_processing_ms: 0.5963725167197305
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.458996918532518
      mean_inference_ms: 7.937928179760913
      mean_raw_obs_processing_ms: 2.273853008563702
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/ge1/dense_logs/logs_01
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.09884238243103027
    StateBufferConnector_ms: 0.009101629257202148
    ViewRequirementAgentConnector_ms: 0.19966959953308105
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 897.5499273433998
  episode_reward_mean: 814.3916386742053
  episode_reward_min: 731.233350005011
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 897.5499273433998
    - 731.233350005011
    policy_a_reward: [117.66697372316294, 112.69371647572635, 86.47616246272534, 156.86435760008615,
      120.5358892061467, 153.9224896693957, 120.03928380936618, 96.02067549829732,
      78.77707937675119, 79.41828905895025]
    policy_p_reward:
    - 303.31282787555494
    - 203.05553259224823
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 156.86435760008615
    p: 303.31282787555494
  policy_reward_mean:
    a: 112.24149168806079
    p: 253.1841802339016
  policy_reward_min:
    a: 78.77707937675119
    p: 203.05553259224823
  sampler_perf:
    mean_action_processing_ms: 0.572700328941269
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.34073299507075
    mean_inference_ms: 7.762804418941246
    mean_raw_obs_processing_ms: 2.1616500826536376
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.09884238243103027
      StateBufferConnector_ms: 0.009101629257202148
      ViewRequirementAgentConnector_ms: 0.19966959953308105
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 897.5499273433998
    episode_reward_mean: 814.3916386742053
    episode_reward_min: 731.233350005011
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 897.5499273433998
      - 731.233350005011
      policy_a_reward: [117.66697372316294, 112.69371647572635, 86.47616246272534,
        156.86435760008615, 120.5358892061467, 153.9224896693957, 120.03928380936618,
        96.02067549829732, 78.77707937675119, 79.41828905895025]
      policy_p_reward:
      - 303.31282787555494
      - 203.05553259224823
    num_faulty_episodes: 0
    policy_reward_max:
      a: 156.86435760008615
      p: 303.31282787555494
    policy_reward_mean:
      a: 112.24149168806079
      p: 253.1841802339016
    policy_reward_min:
      a: 78.77707937675119
      p: 203.05553259224823
    sampler_perf:
      mean_action_processing_ms: 0.572700328941269
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.34073299507075
      mean_inference_ms: 7.762804418941246
      mean_raw_obs_processing_ms: 2.1616500826536376
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/ge1/dense_logs/logs_02
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.09863972663879395
    StateBufferConnector_ms: 0.008386373519897461
    ViewRequirementAgentConnector_ms: 0.22875070571899414
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 760.3211550425057
  episode_reward_mean: 700.2962506519897
  episode_reward_min: 640.2713462614737
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 760.3211550425057
    - 640.2713462614737
    policy_a_reward: [134.1746778590301, 119.57709687488757, 102.39671361385977, 85.80278965711766,
      92.96691909808936, 126.90313856429363, 106.60966339867517, 86.56815794881167,
      76.63099150776291, 80.61584278644197]
    policy_p_reward:
    - 225.40295793952558
    - 162.9435520554904
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 134.1746778590301
    p: 225.40295793952558
  policy_reward_mean:
    a: 101.22459913089698
    p: 194.17325499750797
  policy_reward_min:
    a: 76.63099150776291
    p: 162.9435520554904
  sampler_perf:
    mean_action_processing_ms: 0.558760450936031
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.154189594503285
    mean_inference_ms: 7.8684824934487105
    mean_raw_obs_processing_ms: 2.102412800977136
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.09863972663879395
      StateBufferConnector_ms: 0.008386373519897461
      ViewRequirementAgentConnector_ms: 0.22875070571899414
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 760.3211550425057
    episode_reward_mean: 700.2962506519897
    episode_reward_min: 640.2713462614737
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 760.3211550425057
      - 640.2713462614737
      policy_a_reward: [134.1746778590301, 119.57709687488757, 102.39671361385977,
        85.80278965711766, 92.96691909808936, 126.90313856429363, 106.60966339867517,
        86.56815794881167, 76.63099150776291, 80.61584278644197]
      policy_p_reward:
      - 225.40295793952558
      - 162.9435520554904
    num_faulty_episodes: 0
    policy_reward_max:
      a: 134.1746778590301
      p: 225.40295793952558
    policy_reward_mean:
      a: 101.22459913089698
      p: 194.17325499750797
    policy_reward_min:
      a: 76.63099150776291
      p: 162.9435520554904
    sampler_perf:
      mean_action_processing_ms: 0.558760450936031
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.154189594503285
      mean_inference_ms: 7.8684824934487105
      mean_raw_obs_processing_ms: 2.102412800977136
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/ge1/dense_logs/logs_03
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.08990764617919922
    StateBufferConnector_ms: 0.00845789909362793
    ViewRequirementAgentConnector_ms: 0.21703839302062988
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 713.5623739689356
  episode_reward_mean: 709.2944604782209
  episode_reward_min: 705.0265469875062
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 705.0265469875062
    - 713.5623739689356
    policy_a_reward: [107.59429190121358, 135.56911058546766, 81.0262475297326, 117.33656304728389,
      73.7252987566886, 99.82141941270726, 76.19177721624601, 124.33728675071156,
      97.63962190317179, 108.8537575132875]
    policy_p_reward:
    - 189.77503516711747
    - 206.7185111728135
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 135.56911058546766
    p: 206.7185111728135
  policy_reward_mean:
    a: 102.20953746165104
    p: 198.24677316996548
  policy_reward_min:
    a: 73.7252987566886
    p: 189.77503516711747
  sampler_perf:
    mean_action_processing_ms: 0.5511613141913645
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.10911952052675
    mean_inference_ms: 7.920238124232729
    mean_raw_obs_processing_ms: 2.065520913826852
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.08990764617919922
      StateBufferConnector_ms: 0.00845789909362793
      ViewRequirementAgentConnector_ms: 0.21703839302062988
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 713.5623739689356
    episode_reward_mean: 709.2944604782209
    episode_reward_min: 705.0265469875062
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 705.0265469875062
      - 713.5623739689356
      policy_a_reward: [107.59429190121358, 135.56911058546766, 81.0262475297326,
        117.33656304728389, 73.7252987566886, 99.82141941270726, 76.19177721624601,
        124.33728675071156, 97.63962190317179, 108.8537575132875]
      policy_p_reward:
      - 189.77503516711747
      - 206.7185111728135
    num_faulty_episodes: 0
    policy_reward_max:
      a: 135.56911058546766
      p: 206.7185111728135
    policy_reward_mean:
      a: 102.20953746165104
      p: 198.24677316996548
    policy_reward_min:
      a: 73.7252987566886
      p: 189.77503516711747
    sampler_perf:
      mean_action_processing_ms: 0.5511613141913645
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.10911952052675
      mean_inference_ms: 7.920238124232729
      mean_raw_obs_processing_ms: 2.065520913826852
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/ge1/dense_logs/logs_04
Completing! Saving final snapshot...


Final snapshot saved! All done.
seed (final): 63619000
Not restoring trainer...
Restoring agents weights...
Starting with fresh planner weights.
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.10190010070800781
    StateBufferConnector_ms: 0.01068115234375
    ViewRequirementAgentConnector_ms: 0.2509891986846924
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 633.7840889602622
  episode_reward_mean: 617.4867409552492
  episode_reward_min: 601.1893929502362
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 601.1893929502362
    - 633.7840889602622
    policy_a_reward: [107.7118760242355, 108.53371780208587, 118.45846091386521, 116.19294890986853,
      61.247238433915975, 141.80983432884543, 114.04276194357371, 98.82585176288363,
      90.0393746014407, 93.37007262102095]
    policy_p_reward:
    - 89.04515086626525
    - 95.6961937025011
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 141.80983432884543
    p: 95.6961937025011
  policy_reward_mean:
    a: 105.02321373417355
    p: 92.37067228438318
  policy_reward_min:
    a: 61.247238433915975
    p: 89.04515086626525
  sampler_perf:
    mean_action_processing_ms: 0.6281901262477486
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.851086648876319
    mean_inference_ms: 8.214043999860387
    mean_raw_obs_processing_ms: 2.510497194088386
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.10190010070800781
      StateBufferConnector_ms: 0.01068115234375
      ViewRequirementAgentConnector_ms: 0.2509891986846924
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 633.7840889602622
    episode_reward_mean: 617.4867409552492
    episode_reward_min: 601.1893929502362
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 601.1893929502362
      - 633.7840889602622
      policy_a_reward: [107.7118760242355, 108.53371780208587, 118.45846091386521,
        116.19294890986853, 61.247238433915975, 141.80983432884543, 114.04276194357371,
        98.82585176288363, 90.0393746014407, 93.37007262102095]
      policy_p_reward:
      - 89.04515086626525
      - 95.6961937025011
    num_faulty_episodes: 0
    policy_reward_max:
      a: 141.80983432884543
      p: 95.6961937025011
    policy_reward_mean:
      a: 105.02321373417355
      p: 92.37067228438318
    policy_reward_min:
      a: 61.247238433915975
      p: 89.04515086626525
    sampler_perf:
      mean_action_processing_ms: 0.6281901262477486
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.851086648876319
      mean_inference_ms: 8.214043999860387
      mean_raw_obs_processing_ms: 2.510497194088386
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/ge1/dense_logs/logs_00
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.12625455856323242
    StateBufferConnector_ms: 0.010305643081665039
    ViewRequirementAgentConnector_ms: 0.21782517433166504
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 658.2018923946029
  episode_reward_mean: 633.2584063632396
  episode_reward_min: 608.3149203318762
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 608.3149203318762
    - 658.2018923946029
    policy_a_reward: [131.41629024385614, 98.30794981238832, 99.17727854906607, 106.65383405224875,
      105.14778287135309, 116.6512161844516, 107.95802989858578, 92.62181965546651,
      90.55625195406144, 81.25768816454672]
    policy_p_reward:
    - 67.61178480296617
    - 169.1568865374929
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 131.41629024385614
    p: 169.1568865374929
  policy_reward_mean:
    a: 102.97481413860244
    p: 118.38433567022955
  policy_reward_min:
    a: 81.25768816454672
    p: 67.61178480296617
  sampler_perf:
    mean_action_processing_ms: 0.6040631236134472
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.6216453110183275
    mean_inference_ms: 8.662586802845592
    mean_raw_obs_processing_ms: 2.345992135001229
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.12625455856323242
      StateBufferConnector_ms: 0.010305643081665039
      ViewRequirementAgentConnector_ms: 0.21782517433166504
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 658.2018923946029
    episode_reward_mean: 633.2584063632396
    episode_reward_min: 608.3149203318762
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 608.3149203318762
      - 658.2018923946029
      policy_a_reward: [131.41629024385614, 98.30794981238832, 99.17727854906607,
        106.65383405224875, 105.14778287135309, 116.6512161844516, 107.95802989858578,
        92.62181965546651, 90.55625195406144, 81.25768816454672]
      policy_p_reward:
      - 67.61178480296617
      - 169.1568865374929
    num_faulty_episodes: 0
    policy_reward_max:
      a: 131.41629024385614
      p: 169.1568865374929
    policy_reward_mean:
      a: 102.97481413860244
      p: 118.38433567022955
    policy_reward_min:
      a: 81.25768816454672
      p: 67.61178480296617
    sampler_perf:
      mean_action_processing_ms: 0.6040631236134472
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.6216453110183275
      mean_inference_ms: 8.662586802845592
      mean_raw_obs_processing_ms: 2.345992135001229
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/ge1/dense_logs/logs_01
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.10928511619567872
    StateBufferConnector_ms: 0.010001659393310547
    ViewRequirementAgentConnector_ms: 0.20741820335388184
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 574.8339775072258
  episode_reward_mean: 542.5851153648616
  episode_reward_min: 510.3362532224976
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 574.8339775072258
    - 510.3362532224976
    policy_a_reward: [99.23960949802317, 110.5905960685444, 93.15107525950891, 85.63743006164286,
      78.96530398150745, 83.82893771313589, 96.94307043049652, 97.58591801578719,
      79.92977545829929, 88.22620242961197]
    policy_p_reward:
    - 107.2499626379973
    - 63.8223491751659
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 110.5905960685444
    p: 107.2499626379973
  policy_reward_mean:
    a: 91.40979189165577
    p: 85.5361559065816
  policy_reward_min:
    a: 78.96530398150745
    p: 63.8223491751659
  sampler_perf:
    mean_action_processing_ms: 0.5908334834983872
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.549743651073031
    mean_inference_ms: 8.746439579881722
    mean_raw_obs_processing_ms: 2.2755085349797723
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.10928511619567872
      StateBufferConnector_ms: 0.010001659393310547
      ViewRequirementAgentConnector_ms: 0.20741820335388184
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 574.8339775072258
    episode_reward_mean: 542.5851153648616
    episode_reward_min: 510.3362532224976
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 574.8339775072258
      - 510.3362532224976
      policy_a_reward: [99.23960949802317, 110.5905960685444, 93.15107525950891, 85.63743006164286,
        78.96530398150745, 83.82893771313589, 96.94307043049652, 97.58591801578719,
        79.92977545829929, 88.22620242961197]
      policy_p_reward:
      - 107.2499626379973
      - 63.8223491751659
    num_faulty_episodes: 0
    policy_reward_max:
      a: 110.5905960685444
      p: 107.2499626379973
    policy_reward_mean:
      a: 91.40979189165577
      p: 85.5361559065816
    policy_reward_min:
      a: 78.96530398150745
      p: 63.8223491751659
    sampler_perf:
      mean_action_processing_ms: 0.5908334834983872
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.549743651073031
      mean_inference_ms: 8.746439579881722
      mean_raw_obs_processing_ms: 2.2755085349797723
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/ge1/dense_logs/logs_02
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.12363195419311523
    StateBufferConnector_ms: 0.014543533325195312
    ViewRequirementAgentConnector_ms: 0.2438187599182129
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 584.67074769174
  episode_reward_mean: 543.7166999871283
  episode_reward_min: 502.7626522825168
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 584.67074769174
    - 502.7626522825168
    policy_a_reward: [108.70583201733399, 103.97101515334461, 96.29535023009554, 64.98046900625891,
      91.08211507271953, 117.89500074727535, 86.0090846347586, 75.40083224764614,
      87.62647881910017, 91.19850234101456]
    policy_p_reward:
    - 119.63596621198718
    - 44.632753492719985
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 117.89500074727535
    p: 119.63596621198718
  policy_reward_mean:
    a: 92.31646802695472
    p: 82.13435985235358
  policy_reward_min:
    a: 64.98046900625891
    p: 44.632753492719985
  sampler_perf:
    mean_action_processing_ms: 0.5884790110742968
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.4190800231674325
    mean_inference_ms: 8.857442282963133
    mean_raw_obs_processing_ms: 2.2575649126120534
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.12363195419311523
      StateBufferConnector_ms: 0.014543533325195312
      ViewRequirementAgentConnector_ms: 0.2438187599182129
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 584.67074769174
    episode_reward_mean: 543.7166999871283
    episode_reward_min: 502.7626522825168
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 584.67074769174
      - 502.7626522825168
      policy_a_reward: [108.70583201733399, 103.97101515334461, 96.29535023009554,
        64.98046900625891, 91.08211507271953, 117.89500074727535, 86.0090846347586,
        75.40083224764614, 87.62647881910017, 91.19850234101456]
      policy_p_reward:
      - 119.63596621198718
      - 44.632753492719985
    num_faulty_episodes: 0
    policy_reward_max:
      a: 117.89500074727535
      p: 119.63596621198718
    policy_reward_mean:
      a: 92.31646802695472
      p: 82.13435985235358
    policy_reward_min:
      a: 64.98046900625891
      p: 44.632753492719985
    sampler_perf:
      mean_action_processing_ms: 0.5884790110742968
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.4190800231674325
      mean_inference_ms: 8.857442282963133
      mean_raw_obs_processing_ms: 2.2575649126120534
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/ge1/dense_logs/logs_03
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.10176301002502441
    StateBufferConnector_ms: 0.013446807861328125
    ViewRequirementAgentConnector_ms: 0.22554993629455566
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 617.9157429835527
  episode_reward_mean: 604.752651638587
  episode_reward_min: 591.5895602936214
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 617.9157429835527
    - 591.5895602936214
    policy_a_reward: [99.11526192666527, 131.86398900602546, 98.79010839829242, 63.72663450765904,
      106.71531956653105, 98.2220026826747, 68.64810411040158, 89.72504789513034,
      97.92244671651349, 115.62449462042805]
    policy_p_reward:
    - 117.7044295783877
    - 121.44746426847274
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 131.86398900602546
    p: 121.44746426847274
  policy_reward_mean:
    a: 97.03534094303214
    p: 119.57594692343022
  policy_reward_min:
    a: 63.72663450765904
    p: 117.7044295783877
  sampler_perf:
    mean_action_processing_ms: 0.5936588301080935
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.370113603118323
    mean_inference_ms: 9.234290082947535
    mean_raw_obs_processing_ms: 2.2729716745198894
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.10176301002502441
      StateBufferConnector_ms: 0.013446807861328125
      ViewRequirementAgentConnector_ms: 0.22554993629455566
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 617.9157429835527
    episode_reward_mean: 604.752651638587
    episode_reward_min: 591.5895602936214
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 617.9157429835527
      - 591.5895602936214
      policy_a_reward: [99.11526192666527, 131.86398900602546, 98.79010839829242,
        63.72663450765904, 106.71531956653105, 98.2220026826747, 68.64810411040158,
        89.72504789513034, 97.92244671651349, 115.62449462042805]
      policy_p_reward:
      - 117.7044295783877
      - 121.44746426847274
    num_faulty_episodes: 0
    policy_reward_max:
      a: 131.86398900602546
      p: 121.44746426847274
    policy_reward_mean:
      a: 97.03534094303214
      p: 119.57594692343022
    policy_reward_min:
      a: 63.72663450765904
      p: 117.7044295783877
    sampler_perf:
      mean_action_processing_ms: 0.5936588301080935
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.370113603118323
      mean_inference_ms: 9.234290082947535
      mean_raw_obs_processing_ms: 2.2729716745198894
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/ge1/dense_logs/logs_04
Completing! Saving final snapshot...


Final snapshot saved! All done.
seed (final): 29324000
Not restoring trainer...
Restoring agents weights...
Starting with fresh planner weights.
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.07907748222351074
    StateBufferConnector_ms: 0.008541345596313477
    ViewRequirementAgentConnector_ms: 0.18470287322998047
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 662.8631745080027
  episode_reward_mean: 657.0203907272897
  episode_reward_min: 651.1776069465768
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 662.8631745080027
    - 651.1776069465768
    policy_a_reward: [85.71258715770402, 94.53572641632525, 104.71175802711369, 86.94059752375928,
      113.68731583481504, 124.76328848999046, 95.78899934824267, 101.09648650018383,
      72.72808829046129, 124.82999685298435]
    policy_p_reward:
    - 177.2751895482887
    - 131.9707474647115
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 124.82999685298435
    p: 177.2751895482887
  policy_reward_mean:
    a: 100.47948444415799
    p: 154.62296850650011
  policy_reward_min:
    a: 72.72808829046129
    p: 131.9707474647115
  sampler_perf:
    mean_action_processing_ms: 0.509769378783936
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.190296945933572
    mean_inference_ms: 7.41887473298642
    mean_raw_obs_processing_ms: 2.111232209348393
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.07907748222351074
      StateBufferConnector_ms: 0.008541345596313477
      ViewRequirementAgentConnector_ms: 0.18470287322998047
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 662.8631745080027
    episode_reward_mean: 657.0203907272897
    episode_reward_min: 651.1776069465768
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 662.8631745080027
      - 651.1776069465768
      policy_a_reward: [85.71258715770402, 94.53572641632525, 104.71175802711369,
        86.94059752375928, 113.68731583481504, 124.76328848999046, 95.78899934824267,
        101.09648650018383, 72.72808829046129, 124.82999685298435]
      policy_p_reward:
      - 177.2751895482887
      - 131.9707474647115
    num_faulty_episodes: 0
    policy_reward_max:
      a: 124.82999685298435
      p: 177.2751895482887
    policy_reward_mean:
      a: 100.47948444415799
      p: 154.62296850650011
    policy_reward_min:
      a: 72.72808829046129
      p: 131.9707474647115
    sampler_perf:
      mean_action_processing_ms: 0.509769378783936
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.190296945933572
      mean_inference_ms: 7.41887473298642
      mean_raw_obs_processing_ms: 2.111232209348393
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/ge1/dense_logs/logs_00
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.09924173355102539
    StateBufferConnector_ms: 0.009453296661376953
    ViewRequirementAgentConnector_ms: 0.21046996116638184
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 618.7098663631124
  episode_reward_mean: 584.5879427269924
  episode_reward_min: 550.4660190908726
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 618.7098663631124
    - 550.4660190908726
    policy_a_reward: [113.93433865681148, 112.81062367011678, 91.4261281718389, 67.11603185556505,
      89.07108139133214, 117.65988885991221, 90.38658328874313, 114.6520820667355,
      71.3735567511587, 72.69925363413907]
    policy_p_reward:
    - 144.3516626174488
    - 83.69465449019073
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 117.65988885991221
    p: 144.3516626174488
  policy_reward_mean:
    a: 94.1129568346353
    p: 114.02315855381977
  policy_reward_min:
    a: 67.11603185556505
    p: 83.69465449019073
  sampler_perf:
    mean_action_processing_ms: 0.518353192599027
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.118904129013077
    mean_inference_ms: 8.231786581186148
    mean_raw_obs_processing_ms: 2.0570383443460836
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.09924173355102539
      StateBufferConnector_ms: 0.009453296661376953
      ViewRequirementAgentConnector_ms: 0.21046996116638184
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 618.7098663631124
    episode_reward_mean: 584.5879427269924
    episode_reward_min: 550.4660190908726
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 618.7098663631124
      - 550.4660190908726
      policy_a_reward: [113.93433865681148, 112.81062367011678, 91.4261281718389,
        67.11603185556505, 89.07108139133214, 117.65988885991221, 90.38658328874313,
        114.6520820667355, 71.3735567511587, 72.69925363413907]
      policy_p_reward:
      - 144.3516626174488
      - 83.69465449019073
    num_faulty_episodes: 0
    policy_reward_max:
      a: 117.65988885991221
      p: 144.3516626174488
    policy_reward_mean:
      a: 94.1129568346353
      p: 114.02315855381977
    policy_reward_min:
      a: 67.11603185556505
      p: 83.69465449019073
    sampler_perf:
      mean_action_processing_ms: 0.518353192599027
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.118904129013077
      mean_inference_ms: 8.231786581186148
      mean_raw_obs_processing_ms: 2.0570383443460836
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/ge1/dense_logs/logs_01
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.10148286819458008
    StateBufferConnector_ms: 0.009888410568237305
    ViewRequirementAgentConnector_ms: 0.20431280136108398
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 641.4516792356193
  episode_reward_mean: 577.0412444067083
  episode_reward_min: 512.6308095777973
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 641.4516792356193
    - 512.6308095777973
    policy_a_reward: [107.8553577940039, 123.72430016622495, 101.77350266003754, 72.0442221510386,
      82.60931174105639, 78.66712228196936, 102.04606566061437, 94.48630832560892,
      96.68437597068095, 85.96599170639989]
    policy_p_reward:
    - 153.44498472326143
    - 54.78094563252526
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 123.72430016622495
    p: 153.44498472326143
  policy_reward_mean:
    a: 94.58565584576348
    p: 104.11296517789334
  policy_reward_min:
    a: 72.0442221510386
    p: 54.78094563252526
  sampler_perf:
    mean_action_processing_ms: 0.5190329580287946
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.209805932702579
    mean_inference_ms: 8.482778969802196
    mean_raw_obs_processing_ms: 2.0410002747191975
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.10148286819458008
      StateBufferConnector_ms: 0.009888410568237305
      ViewRequirementAgentConnector_ms: 0.20431280136108398
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 641.4516792356193
    episode_reward_mean: 577.0412444067083
    episode_reward_min: 512.6308095777973
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 641.4516792356193
      - 512.6308095777973
      policy_a_reward: [107.8553577940039, 123.72430016622495, 101.77350266003754,
        72.0442221510386, 82.60931174105639, 78.66712228196936, 102.04606566061437,
        94.48630832560892, 96.68437597068095, 85.96599170639989]
      policy_p_reward:
      - 153.44498472326143
      - 54.78094563252526
    num_faulty_episodes: 0
    policy_reward_max:
      a: 123.72430016622495
      p: 153.44498472326143
    policy_reward_mean:
      a: 94.58565584576348
      p: 104.11296517789334
    policy_reward_min:
      a: 72.0442221510386
      p: 54.78094563252526
    sampler_perf:
      mean_action_processing_ms: 0.5190329580287946
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.209805932702579
      mean_inference_ms: 8.482778969802196
      mean_raw_obs_processing_ms: 2.0410002747191975
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/ge1/dense_logs/logs_02
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.11295080184936523
    StateBufferConnector_ms: 0.009143352508544922
    ViewRequirementAgentConnector_ms: 0.2005457878112793
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 536.457853020232
  episode_reward_mean: 532.3290567234554
  episode_reward_min: 528.2002604266788
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 536.457853020232
    - 528.2002604266788
    policy_a_reward: [90.4705560832651, 84.03514833265486, 85.76691222552846, 102.64374566165293,
      102.06622313558267, 94.7429381066372, 112.43460839998943, 75.0332856409153,
      115.20894613067293, 74.57965711448294]
    policy_p_reward:
    - 71.47526758154429
    - 56.20082503397779
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 115.20894613067293
    p: 71.47526758154429
  policy_reward_mean:
    a: 93.69820208313818
    p: 63.83804630776104
  policy_reward_min:
    a: 74.57965711448294
    p: 56.20082503397779
  sampler_perf:
    mean_action_processing_ms: 0.5217966349466868
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.084017680681449
    mean_inference_ms: 8.58934410568001
    mean_raw_obs_processing_ms: 2.0428103008966096
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.11295080184936523
      StateBufferConnector_ms: 0.009143352508544922
      ViewRequirementAgentConnector_ms: 0.2005457878112793
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 536.457853020232
    episode_reward_mean: 532.3290567234554
    episode_reward_min: 528.2002604266788
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 536.457853020232
      - 528.2002604266788
      policy_a_reward: [90.4705560832651, 84.03514833265486, 85.76691222552846, 102.64374566165293,
        102.06622313558267, 94.7429381066372, 112.43460839998943, 75.0332856409153,
        115.20894613067293, 74.57965711448294]
      policy_p_reward:
      - 71.47526758154429
      - 56.20082503397779
    num_faulty_episodes: 0
    policy_reward_max:
      a: 115.20894613067293
      p: 71.47526758154429
    policy_reward_mean:
      a: 93.69820208313818
      p: 63.83804630776104
    policy_reward_min:
      a: 74.57965711448294
      p: 56.20082503397779
    sampler_perf:
      mean_action_processing_ms: 0.5217966349466868
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.084017680681449
      mean_inference_ms: 8.58934410568001
      mean_raw_obs_processing_ms: 2.0428103008966096
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/ge1/dense_logs/logs_03
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.08942484855651855
    StateBufferConnector_ms: 0.00972747802734375
    ViewRequirementAgentConnector_ms: 0.19199252128601074
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 550.3067524219771
  episode_reward_mean: 517.3364840735844
  episode_reward_min: 484.36621572519175
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 484.36621572519175
    - 550.3067524219771
    policy_a_reward: [107.48337281942956, 120.31229584597652, 115.3437577132467, 66.50688069491068,
      49.877619390280806, 98.22839818568616, 121.64390831071809, 73.04828800329703,
      83.2053738211343, 117.05574194287031]
    policy_p_reward:
    - 24.84228926134871
    - 57.12504215826996
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 121.64390831071809
    p: 57.12504215826996
  policy_reward_mean:
    a: 95.27056367275502
    p: 40.98366570980934
  policy_reward_min:
    a: 49.877619390280806
    p: 24.84228926134871
  sampler_perf:
    mean_action_processing_ms: 0.5236084772938969
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.098853412888995
    mean_inference_ms: 8.690813358952074
    mean_raw_obs_processing_ms: 2.0300420176167813
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.08942484855651855
      StateBufferConnector_ms: 0.00972747802734375
      ViewRequirementAgentConnector_ms: 0.19199252128601074
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 550.3067524219771
    episode_reward_mean: 517.3364840735844
    episode_reward_min: 484.36621572519175
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 484.36621572519175
      - 550.3067524219771
      policy_a_reward: [107.48337281942956, 120.31229584597652, 115.3437577132467,
        66.50688069491068, 49.877619390280806, 98.22839818568616, 121.64390831071809,
        73.04828800329703, 83.2053738211343, 117.05574194287031]
      policy_p_reward:
      - 24.84228926134871
      - 57.12504215826996
    num_faulty_episodes: 0
    policy_reward_max:
      a: 121.64390831071809
      p: 57.12504215826996
    policy_reward_mean:
      a: 95.27056367275502
      p: 40.98366570980934
    policy_reward_min:
      a: 49.877619390280806
      p: 24.84228926134871
    sampler_perf:
      mean_action_processing_ms: 0.5236084772938969
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.098853412888995
      mean_inference_ms: 8.690813358952074
      mean_raw_obs_processing_ms: 2.0300420176167813
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/ge1/dense_logs/logs_04
Completing! Saving final snapshot...


Final snapshot saved! All done.
seed (final): 32729000
Not restoring trainer...
Restoring agents weights...
Starting with fresh planner weights.
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0840604305267334
    StateBufferConnector_ms: 0.008475780487060547
    ViewRequirementAgentConnector_ms: 0.19100308418273926
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 687.3105959637178
  episode_reward_mean: 631.7232366860201
  episode_reward_min: 576.1358774083222
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 687.3105959637178
    - 576.1358774083222
    policy_a_reward: [79.46533457792282, 114.88355445284724, 90.50340745644702, 114.33634472911751,
      98.37062818915169, 93.72279661649753, 99.92613457405402, 115.5488153290664,
      78.04355991427738, 58.22081506963562]
    policy_p_reward:
    - 189.7513265582316
    - 130.6737559047955
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 115.5488153290664
    p: 189.7513265582316
  policy_reward_mean:
    a: 94.30213909090172
    p: 160.21254123151357
  policy_reward_min:
    a: 58.22081506963562
    p: 130.6737559047955
  sampler_perf:
    mean_action_processing_ms: 0.5140561543538898
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3.9832525386543804
    mean_inference_ms: 7.496985132822733
    mean_raw_obs_processing_ms: 2.0395633941163083
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.0840604305267334
      StateBufferConnector_ms: 0.008475780487060547
      ViewRequirementAgentConnector_ms: 0.19100308418273926
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 687.3105959637178
    episode_reward_mean: 631.7232366860201
    episode_reward_min: 576.1358774083222
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 687.3105959637178
      - 576.1358774083222
      policy_a_reward: [79.46533457792282, 114.88355445284724, 90.50340745644702,
        114.33634472911751, 98.37062818915169, 93.72279661649753, 99.92613457405402,
        115.5488153290664, 78.04355991427738, 58.22081506963562]
      policy_p_reward:
      - 189.7513265582316
      - 130.6737559047955
    num_faulty_episodes: 0
    policy_reward_max:
      a: 115.5488153290664
      p: 189.7513265582316
    policy_reward_mean:
      a: 94.30213909090172
      p: 160.21254123151357
    policy_reward_min:
      a: 58.22081506963562
      p: 130.6737559047955
    sampler_perf:
      mean_action_processing_ms: 0.5140561543538898
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 3.9832525386543804
      mean_inference_ms: 7.496985132822733
      mean_raw_obs_processing_ms: 2.0395633941163083
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/1/ge/dense_logs/logs_00
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.10501742362976074
    StateBufferConnector_ms: 0.010287761688232422
    ViewRequirementAgentConnector_ms: 0.2550780773162842
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 722.1636311319842
  episode_reward_mean: 680.3566349936702
  episode_reward_min: 638.5496388553562
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 722.1636311319842
    - 638.5496388553562
    policy_a_reward: [120.08356231193379, 108.6295496447395, 80.6917567939758, 82.24953997112772,
      92.04948321550678, 95.60928227909385, 121.48210307261213, 70.61057481304603,
      100.77886107999763, 89.33235172800799]
    policy_p_reward:
    - 238.45973919470356
    - 160.7364658825989
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 121.48210307261213
    p: 238.45973919470356
  policy_reward_mean:
    a: 96.15170649100412
    p: 199.59810253865123
  policy_reward_min:
    a: 70.61057481304603
    p: 160.7364658825989
  sampler_perf:
    mean_action_processing_ms: 0.5489117377526038
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.212837238292713
    mean_inference_ms: 8.305225934420195
    mean_raw_obs_processing_ms: 2.105277973216969
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.10501742362976074
      StateBufferConnector_ms: 0.010287761688232422
      ViewRequirementAgentConnector_ms: 0.2550780773162842
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 722.1636311319842
    episode_reward_mean: 680.3566349936702
    episode_reward_min: 638.5496388553562
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 722.1636311319842
      - 638.5496388553562
      policy_a_reward: [120.08356231193379, 108.6295496447395, 80.6917567939758, 82.24953997112772,
        92.04948321550678, 95.60928227909385, 121.48210307261213, 70.61057481304603,
        100.77886107999763, 89.33235172800799]
      policy_p_reward:
      - 238.45973919470356
      - 160.7364658825989
    num_faulty_episodes: 0
    policy_reward_max:
      a: 121.48210307261213
      p: 238.45973919470356
    policy_reward_mean:
      a: 96.15170649100412
      p: 199.59810253865123
    policy_reward_min:
      a: 70.61057481304603
      p: 160.7364658825989
    sampler_perf:
      mean_action_processing_ms: 0.5489117377526038
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.212837238292713
      mean_inference_ms: 8.305225934420195
      mean_raw_obs_processing_ms: 2.105277973216969
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/1/ge/dense_logs/logs_01
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.10330677032470703
    StateBufferConnector_ms: 0.009763240814208984
    ViewRequirementAgentConnector_ms: 0.20829439163208008
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 769.0394649145197
  episode_reward_mean: 686.8184488622085
  episode_reward_min: 604.5974328098971
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 604.5974328098971
    - 769.0394649145197
    policy_a_reward: [111.34331563285329, 79.35321447066998, 78.35844750097606, 100.6103027108757,
      71.53470809603522, 83.7835203842529, 105.27374811828692, 85.51671529044728,
      106.48389958892493, 121.06323180420323]
    policy_p_reward:
    - 163.39744439848454
    - 266.9183497284043
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 121.06323180420323
    p: 266.9183497284043
  policy_reward_mean:
    a: 94.33211035975255
    p: 215.1578970634444
  policy_reward_min:
    a: 71.53470809603522
    p: 163.39744439848454
  sampler_perf:
    mean_action_processing_ms: 0.5442506865133531
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.207132181908431
    mean_inference_ms: 8.58513694219316
    mean_raw_obs_processing_ms: 2.060616040213913
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.10330677032470703
      StateBufferConnector_ms: 0.009763240814208984
      ViewRequirementAgentConnector_ms: 0.20829439163208008
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 769.0394649145197
    episode_reward_mean: 686.8184488622085
    episode_reward_min: 604.5974328098971
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 604.5974328098971
      - 769.0394649145197
      policy_a_reward: [111.34331563285329, 79.35321447066998, 78.35844750097606,
        100.6103027108757, 71.53470809603522, 83.7835203842529, 105.27374811828692,
        85.51671529044728, 106.48389958892493, 121.06323180420323]
      policy_p_reward:
      - 163.39744439848454
      - 266.9183497284043
    num_faulty_episodes: 0
    policy_reward_max:
      a: 121.06323180420323
      p: 266.9183497284043
    policy_reward_mean:
      a: 94.33211035975255
      p: 215.1578970634444
    policy_reward_min:
      a: 71.53470809603522
      p: 163.39744439848454
    sampler_perf:
      mean_action_processing_ms: 0.5442506865133531
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.207132181908431
      mean_inference_ms: 8.58513694219316
      mean_raw_obs_processing_ms: 2.060616040213913
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/1/ge/dense_logs/logs_02
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.09877681732177734
    StateBufferConnector_ms: 0.00890493392944336
    ViewRequirementAgentConnector_ms: 0.21430253982543945
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 734.2117316666119
  episode_reward_mean: 732.1771987589664
  episode_reward_min: 730.1426658513209
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 734.2117316666119
    - 730.1426658513209
    policy_a_reward: [113.96221774327188, 61.544809347079266, 86.7127769716458, 119.86837995286729,
      120.49294325246639, 110.00766830520439, 93.65341107738931, 86.53930036083389,
      132.12391191584595, 116.50991243913404]
    policy_p_reward:
    - 231.63060439928125
    - 191.30846175291435
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 132.12391191584595
    p: 231.63060439928125
  policy_reward_mean:
    a: 104.1415331365738
    p: 211.4695330760978
  policy_reward_min:
    a: 61.544809347079266
    p: 191.30846175291435
  sampler_perf:
    mean_action_processing_ms: 0.5403806542468512
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.122597405101465
    mean_inference_ms: 8.603353252534804
    mean_raw_obs_processing_ms: 2.045003013096113
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.09877681732177734
      StateBufferConnector_ms: 0.00890493392944336
      ViewRequirementAgentConnector_ms: 0.21430253982543945
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 734.2117316666119
    episode_reward_mean: 732.1771987589664
    episode_reward_min: 730.1426658513209
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 734.2117316666119
      - 730.1426658513209
      policy_a_reward: [113.96221774327188, 61.544809347079266, 86.7127769716458,
        119.86837995286729, 120.49294325246639, 110.00766830520439, 93.65341107738931,
        86.53930036083389, 132.12391191584595, 116.50991243913404]
      policy_p_reward:
      - 231.63060439928125
      - 191.30846175291435
    num_faulty_episodes: 0
    policy_reward_max:
      a: 132.12391191584595
      p: 231.63060439928125
    policy_reward_mean:
      a: 104.1415331365738
      p: 211.4695330760978
    policy_reward_min:
      a: 61.544809347079266
      p: 191.30846175291435
    sampler_perf:
      mean_action_processing_ms: 0.5403806542468512
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.122597405101465
      mean_inference_ms: 8.603353252534804
      mean_raw_obs_processing_ms: 2.045003013096113
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/1/ge/dense_logs/logs_03
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.08867383003234863
    StateBufferConnector_ms: 0.008791685104370117
    ViewRequirementAgentConnector_ms: 0.1988828182220459
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 681.254819997801
  episode_reward_mean: 653.7713807554829
  episode_reward_min: 626.2879415131649
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 626.2879415131649
    - 681.254819997801
    policy_a_reward: [81.58545232144773, 79.8887555665615, 95.44234188867503, 86.29216648399873,
      103.72167397181381, 117.40512929691103, 71.92306386391358, 109.92276507030596,
      71.15824814875877, 108.54790658230758]
    policy_p_reward:
    - 179.35755128067322
    - 202.2977070356068
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 117.40512929691103
    p: 202.2977070356068
  policy_reward_mean:
    a: 92.58875031946937
    p: 190.82762915814
  policy_reward_min:
    a: 71.15824814875877
    p: 179.35755128067322
  sampler_perf:
    mean_action_processing_ms: 0.5379297980209772
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.1206706671274365
    mean_inference_ms: 8.521081447029342
    mean_raw_obs_processing_ms: 2.040829099878603
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.08867383003234863
      StateBufferConnector_ms: 0.008791685104370117
      ViewRequirementAgentConnector_ms: 0.1988828182220459
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 681.254819997801
    episode_reward_mean: 653.7713807554829
    episode_reward_min: 626.2879415131649
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 626.2879415131649
      - 681.254819997801
      policy_a_reward: [81.58545232144773, 79.8887555665615, 95.44234188867503, 86.29216648399873,
        103.72167397181381, 117.40512929691103, 71.92306386391358, 109.92276507030596,
        71.15824814875877, 108.54790658230758]
      policy_p_reward:
      - 179.35755128067322
      - 202.2977070356068
    num_faulty_episodes: 0
    policy_reward_max:
      a: 117.40512929691103
      p: 202.2977070356068
    policy_reward_mean:
      a: 92.58875031946937
      p: 190.82762915814
    policy_reward_min:
      a: 71.15824814875877
      p: 179.35755128067322
    sampler_perf:
      mean_action_processing_ms: 0.5379297980209772
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.1206706671274365
      mean_inference_ms: 8.521081447029342
      mean_raw_obs_processing_ms: 2.040829099878603
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/1/ge/dense_logs/logs_04
Completing! Saving final snapshot...


Final snapshot saved! All done.
