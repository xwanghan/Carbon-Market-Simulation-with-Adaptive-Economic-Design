seed (final): 35642000
Not restoring trainer...
Restoring agents weights...
Starting with fresh planner weights.
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.08258819580078125
    StateBufferConnector_ms: 0.008493661880493164
    ViewRequirementAgentConnector_ms: 0.18439292907714844
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 2632.134500278552
  episode_reward_mean: 2141.5429841780383
  episode_reward_min: 1650.9514680775246
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 2632.134500278552
    - 1650.9514680775246
    policy_a_reward: [220.0679487706561, 294.0366524796004, 357.1369546342498, 295.67736195393127,
      363.7851966374048, 252.28094429944002, 351.85167181700075, 69.8577379007949,
      116.67097208280349, 325.5343621563608]
    policy_p_reward:
    - 1101.4303858027038
    - 534.7557798211257
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 363.7851966374048
    p: 1101.4303858027038
  policy_reward_mean:
    a: 264.68998027322425
    p: 818.0930828119147
  policy_reward_min:
    a: 69.8577379007949
    p: 534.7557798211257
  sampler_perf:
    mean_action_processing_ms: 0.49426408109074815
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3.7486672163485526
    mean_inference_ms: 6.983660414309321
    mean_raw_obs_processing_ms: 1.8899445524234733
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.08258819580078125
      StateBufferConnector_ms: 0.008493661880493164
      ViewRequirementAgentConnector_ms: 0.18439292907714844
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 2632.134500278552
    episode_reward_mean: 2141.5429841780383
    episode_reward_min: 1650.9514680775246
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 2632.134500278552
      - 1650.9514680775246
      policy_a_reward: [220.0679487706561, 294.0366524796004, 357.1369546342498, 295.67736195393127,
        363.7851966374048, 252.28094429944002, 351.85167181700075, 69.8577379007949,
        116.67097208280349, 325.5343621563608]
      policy_p_reward:
      - 1101.4303858027038
      - 534.7557798211257
    num_faulty_episodes: 0
    policy_reward_max:
      a: 363.7851966374048
      p: 1101.4303858027038
    policy_reward_mean:
      a: 264.68998027322425
      p: 818.0930828119147
    policy_reward_min:
      a: 69.8577379007949
      p: 534.7557798211257
    sampler_perf:
      mean_action_processing_ms: 0.49426408109074815
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 3.7486672163485526
      mean_inference_ms: 6.983660414309321
      mean_raw_obs_processing_ms: 1.8899445524234733
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/flat1/dense_logs/logs_00
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.11528730392456055
    StateBufferConnector_ms: 0.00921487808227539
    ViewRequirementAgentConnector_ms: 0.21575093269348145
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 3150.362110977433
  episode_reward_mean: 2714.8989505293275
  episode_reward_min: 2279.435790081222
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 2279.435790081222
    - 3150.362110977433
    policy_a_reward: [225.85203116264424, 312.47103544330025, 189.64328042360813,
      297.6583343244883, 314.0289925885728, 343.8119908418465, 353.896506411631, 286.20592588021765,
      386.39627444358825, 367.50029601714283]
    policy_p_reward:
    - 939.7821161386125
    - 1412.5511173830025
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 386.39627444358825
    p: 1412.5511173830025
  policy_reward_mean:
    a: 307.746466753704
    p: 1176.1666167608075
  policy_reward_min:
    a: 189.64328042360813
    p: 939.7821161386125
  sampler_perf:
    mean_action_processing_ms: 0.509322582782208
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3.905365159818819
    mean_inference_ms: 7.244379965813605
    mean_raw_obs_processing_ms: 1.9172714664028598
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.11528730392456055
      StateBufferConnector_ms: 0.00921487808227539
      ViewRequirementAgentConnector_ms: 0.21575093269348145
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 3150.362110977433
    episode_reward_mean: 2714.8989505293275
    episode_reward_min: 2279.435790081222
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 2279.435790081222
      - 3150.362110977433
      policy_a_reward: [225.85203116264424, 312.47103544330025, 189.64328042360813,
        297.6583343244883, 314.0289925885728, 343.8119908418465, 353.896506411631,
        286.20592588021765, 386.39627444358825, 367.50029601714283]
      policy_p_reward:
      - 939.7821161386125
      - 1412.5511173830025
    num_faulty_episodes: 0
    policy_reward_max:
      a: 386.39627444358825
      p: 1412.5511173830025
    policy_reward_mean:
      a: 307.746466753704
      p: 1176.1666167608075
    policy_reward_min:
      a: 189.64328042360813
      p: 939.7821161386125
    sampler_perf:
      mean_action_processing_ms: 0.509322582782208
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 3.905365159818819
      mean_inference_ms: 7.244379965813605
      mean_raw_obs_processing_ms: 1.9172714664028598
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/flat1/dense_logs/logs_01
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.09336471557617188
    StateBufferConnector_ms: 0.008314847946166992
    ViewRequirementAgentConnector_ms: 0.18328428268432617
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 2633.5149184237152
  episode_reward_mean: 2208.599636663356
  episode_reward_min: 1783.6843549029961
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 2633.5149184237152
    - 1783.6843549029961
    policy_a_reward: [296.2174167045345, 241.47406519484653, 325.712942159536, 337.0376799177502,
      281.0650664182969, 347.28065498380886, 71.98146652512442, 257.9895525778739,
      195.9782270981596, 266.2359856198751]
    policy_p_reward:
    - 1152.0077480287334
    - 644.2184680981563
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 347.28065498380886
    p: 1152.0077480287334
  policy_reward_mean:
    a: 262.09730571998057
    p: 898.1131080634449
  policy_reward_min:
    a: 71.98146652512442
    p: 644.2184680981563
  sampler_perf:
    mean_action_processing_ms: 0.5088816635772277
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3.951387672246416
    mean_inference_ms: 7.778794506563495
    mean_raw_obs_processing_ms: 1.9104502980983868
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.09336471557617188
      StateBufferConnector_ms: 0.008314847946166992
      ViewRequirementAgentConnector_ms: 0.18328428268432617
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 2633.5149184237152
    episode_reward_mean: 2208.599636663356
    episode_reward_min: 1783.6843549029961
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 2633.5149184237152
      - 1783.6843549029961
      policy_a_reward: [296.2174167045345, 241.47406519484653, 325.712942159536, 337.0376799177502,
        281.0650664182969, 347.28065498380886, 71.98146652512442, 257.9895525778739,
        195.9782270981596, 266.2359856198751]
      policy_p_reward:
      - 1152.0077480287334
      - 644.2184680981563
    num_faulty_episodes: 0
    policy_reward_max:
      a: 347.28065498380886
      p: 1152.0077480287334
    policy_reward_mean:
      a: 262.09730571998057
      p: 898.1131080634449
    policy_reward_min:
      a: 71.98146652512442
      p: 644.2184680981563
    sampler_perf:
      mean_action_processing_ms: 0.5088816635772277
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 3.951387672246416
      mean_inference_ms: 7.778794506563495
      mean_raw_obs_processing_ms: 1.9104502980983868
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/flat1/dense_logs/logs_02
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.09471774101257324
    StateBufferConnector_ms: 0.008666515350341797
    ViewRequirementAgentConnector_ms: 0.2205491065979004
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 2411.0742577122205
  episode_reward_mean: 2192.855020182454
  episode_reward_min: 1974.6357826526878
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1974.6357826526878
    - 2411.0742577122205
    policy_a_reward: [286.26185233794956, 189.13316304201086, 223.6492361705867, 226.35513541344815,
      210.52462268594107, 341.7188539906735, 322.6774275506317, 303.7368724587021,
      231.14535482258714, 196.78003936154076]
    policy_p_reward:
    - 838.711773002756
    - 1015.0157095280833
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 341.7188539906735
    p: 1015.0157095280833
  policy_reward_mean:
    a: 253.19825578340715
    p: 926.8637412654197
  policy_reward_min:
    a: 189.13316304201086
    p: 838.711773002756
  sampler_perf:
    mean_action_processing_ms: 0.5107577713294842
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3.8899920214300807
    mean_inference_ms: 8.131309249054366
    mean_raw_obs_processing_ms: 1.9157878879545214
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.09471774101257324
      StateBufferConnector_ms: 0.008666515350341797
      ViewRequirementAgentConnector_ms: 0.2205491065979004
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 2411.0742577122205
    episode_reward_mean: 2192.855020182454
    episode_reward_min: 1974.6357826526878
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1974.6357826526878
      - 2411.0742577122205
      policy_a_reward: [286.26185233794956, 189.13316304201086, 223.6492361705867,
        226.35513541344815, 210.52462268594107, 341.7188539906735, 322.6774275506317,
        303.7368724587021, 231.14535482258714, 196.78003936154076]
      policy_p_reward:
      - 838.711773002756
      - 1015.0157095280833
    num_faulty_episodes: 0
    policy_reward_max:
      a: 341.7188539906735
      p: 1015.0157095280833
    policy_reward_mean:
      a: 253.19825578340715
      p: 926.8637412654197
    policy_reward_min:
      a: 189.13316304201086
      p: 838.711773002756
    sampler_perf:
      mean_action_processing_ms: 0.5107577713294842
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 3.8899920214300807
      mean_inference_ms: 8.131309249054366
      mean_raw_obs_processing_ms: 1.9157878879545214
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/flat1/dense_logs/logs_03
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.09136795997619629
    StateBufferConnector_ms: 0.0091552734375
    ViewRequirementAgentConnector_ms: 0.1916944980621338
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 2531.4083935389144
  episode_reward_mean: 2318.8714077834416
  episode_reward_min: 2106.3344220279687
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 2531.4083935389144
    - 2106.3344220279687
    policy_a_reward: [415.11287400176633, 63.85222451627018, 363.64629009265747, 377.02203907617525,
      298.10976233568294, 194.91035514973103, 188.36167212509798, 273.1383497746721,
      253.79751995543404, 308.0872598223954]
    policy_p_reward:
    - 1013.6652035163627
    - 888.039265200644
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 415.11287400176633
    p: 1013.6652035163627
  policy_reward_mean:
    a: 273.60383468498827
    p: 950.8522343585033
  policy_reward_min:
    a: 63.85222451627018
    p: 888.039265200644
  sampler_perf:
    mean_action_processing_ms: 0.5116520858392483
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 3.9303993902317003
    mean_inference_ms: 8.10489298009434
    mean_raw_obs_processing_ms: 1.9203371545973895
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.09136795997619629
      StateBufferConnector_ms: 0.0091552734375
      ViewRequirementAgentConnector_ms: 0.1916944980621338
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 2531.4083935389144
    episode_reward_mean: 2318.8714077834416
    episode_reward_min: 2106.3344220279687
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 2531.4083935389144
      - 2106.3344220279687
      policy_a_reward: [415.11287400176633, 63.85222451627018, 363.64629009265747,
        377.02203907617525, 298.10976233568294, 194.91035514973103, 188.36167212509798,
        273.1383497746721, 253.79751995543404, 308.0872598223954]
      policy_p_reward:
      - 1013.6652035163627
      - 888.039265200644
    num_faulty_episodes: 0
    policy_reward_max:
      a: 415.11287400176633
      p: 1013.6652035163627
    policy_reward_mean:
      a: 273.60383468498827
      p: 950.8522343585033
    policy_reward_min:
      a: 63.85222451627018
      p: 888.039265200644
    sampler_perf:
      mean_action_processing_ms: 0.5116520858392483
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 3.9303993902317003
      mean_inference_ms: 8.10489298009434
      mean_raw_obs_processing_ms: 1.9203371545973895
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/flat1/dense_logs/logs_04
Completing! Saving final snapshot...


Final snapshot saved! All done.
seed (final): 36995000
seed (final): 37023000
Not restoring trainer...
Restoring agents weights...
Starting with fresh planner weights.
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.08992552757263184
    StateBufferConnector_ms: 0.009745359420776367
    ViewRequirementAgentConnector_ms: 0.2025008201599121
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 2070.6086452502987
  episode_reward_mean: 1988.431078902352
  episode_reward_min: 1906.2535125544052
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1906.2535125544052
    - 2070.6086452502987
    policy_a_reward: [290.90730677697144, 274.8266584266599, 181.59629058394017, 258.571977777852,
      344.3499348141782, 231.438390505919, 272.39983934496894, 366.6799321232377,
      262.304779229195, 393.62277180769354]
    policy_p_reward:
    - 556.0013441748051
    - 544.162932239283
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 393.62277180769354
    p: 556.0013441748051
  policy_reward_mean:
    a: 287.66978813906155
    p: 550.0821382070441
  policy_reward_min:
    a: 181.59629058394017
    p: 544.162932239283
  sampler_perf:
    mean_action_processing_ms: 0.5061597881202926
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.137513166416191
    mean_inference_ms: 7.135978002034261
    mean_raw_obs_processing_ms: 2.1710510025481264
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.08992552757263184
      StateBufferConnector_ms: 0.009745359420776367
      ViewRequirementAgentConnector_ms: 0.2025008201599121
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 2070.6086452502987
    episode_reward_mean: 1988.431078902352
    episode_reward_min: 1906.2535125544052
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1906.2535125544052
      - 2070.6086452502987
      policy_a_reward: [290.90730677697144, 274.8266584266599, 181.59629058394017,
        258.571977777852, 344.3499348141782, 231.438390505919, 272.39983934496894,
        366.6799321232377, 262.304779229195, 393.62277180769354]
      policy_p_reward:
      - 556.0013441748051
      - 544.162932239283
    num_faulty_episodes: 0
    policy_reward_max:
      a: 393.62277180769354
      p: 556.0013441748051
    policy_reward_mean:
      a: 287.66978813906155
      p: 550.0821382070441
    policy_reward_min:
      a: 181.59629058394017
      p: 544.162932239283
    sampler_perf:
      mean_action_processing_ms: 0.5061597881202926
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.137513166416191
      mean_inference_ms: 7.135978002034261
      mean_raw_obs_processing_ms: 2.1710510025481264
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/flat1/dense_logs/logs_00
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.09647011756896973
    StateBufferConnector_ms: 0.008493661880493164
    ViewRequirementAgentConnector_ms: 0.19502639770507812
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 2241.6321550927205
  episode_reward_mean: 2145.7077793977596
  episode_reward_min: 2049.783403702799
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 2049.783403702799
    - 2241.6321550927205
    policy_a_reward: [278.52045829773425, 242.37985380901924, 255.21267940207412,
      245.42792783759708, 292.0162181016113, 324.7376803186582, 372.2793523942548,
      244.44236236395224, 263.43833745708343, 308.65175347612023]
    policy_p_reward:
    - 736.2262662547638
    - 728.0826690826533
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 372.2793523942548
    p: 736.2262662547638
  policy_reward_mean:
    a: 282.7106623458105
    p: 732.1544676687085
  policy_reward_min:
    a: 242.37985380901924
    p: 728.0826690826533
  sampler_perf:
    mean_action_processing_ms: 0.5295924492530175
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.159852817699269
    mean_inference_ms: 7.7952512137063374
    mean_raw_obs_processing_ms: 2.0881863859864502
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.09647011756896973
      StateBufferConnector_ms: 0.008493661880493164
      ViewRequirementAgentConnector_ms: 0.19502639770507812
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 2241.6321550927205
    episode_reward_mean: 2145.7077793977596
    episode_reward_min: 2049.783403702799
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 2049.783403702799
      - 2241.6321550927205
      policy_a_reward: [278.52045829773425, 242.37985380901924, 255.21267940207412,
        245.42792783759708, 292.0162181016113, 324.7376803186582, 372.2793523942548,
        244.44236236395224, 263.43833745708343, 308.65175347612023]
      policy_p_reward:
      - 736.2262662547638
      - 728.0826690826533
    num_faulty_episodes: 0
    policy_reward_max:
      a: 372.2793523942548
      p: 736.2262662547638
    policy_reward_mean:
      a: 282.7106623458105
      p: 732.1544676687085
    policy_reward_min:
      a: 242.37985380901924
      p: 728.0826690826533
    sampler_perf:
      mean_action_processing_ms: 0.5295924492530175
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.159852817699269
      mean_inference_ms: 7.7952512137063374
      mean_raw_obs_processing_ms: 2.0881863859864502
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/flat1/dense_logs/logs_01
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.09441971778869629
    StateBufferConnector_ms: 0.00871419906616211
    ViewRequirementAgentConnector_ms: 0.2007603645324707
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 2125.5287032465517
  episode_reward_mean: 2065.296238309178
  episode_reward_min: 2005.0637733718045
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 2125.5287032465517
    - 2005.0637733718045
    policy_a_reward: [289.02649151483075, 341.38827141869433, 297.553233322809, 255.03470453436955,
      270.03228613499505, 279.91775968008494, 245.87113370102716, 217.26718000346563,
      201.0594646019633, 310.681016435709]
    policy_p_reward:
    - 672.4937163208523
    - 750.2672189495536
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 341.38827141869433
    p: 750.2672189495536
  policy_reward_mean:
    a: 270.78315413479487
    p: 711.3804676352029
  policy_reward_min:
    a: 201.0594646019633
    p: 672.4937163208523
  sampler_perf:
    mean_action_processing_ms: 0.5271711165233106
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.140503640019203
    mean_inference_ms: 8.257630187459663
    mean_raw_obs_processing_ms: 2.0362770136478026
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.09441971778869629
      StateBufferConnector_ms: 0.00871419906616211
      ViewRequirementAgentConnector_ms: 0.2007603645324707
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 2125.5287032465517
    episode_reward_mean: 2065.296238309178
    episode_reward_min: 2005.0637733718045
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 2125.5287032465517
      - 2005.0637733718045
      policy_a_reward: [289.02649151483075, 341.38827141869433, 297.553233322809,
        255.03470453436955, 270.03228613499505, 279.91775968008494, 245.87113370102716,
        217.26718000346563, 201.0594646019633, 310.681016435709]
      policy_p_reward:
      - 672.4937163208523
      - 750.2672189495536
    num_faulty_episodes: 0
    policy_reward_max:
      a: 341.38827141869433
      p: 750.2672189495536
    policy_reward_mean:
      a: 270.78315413479487
      p: 711.3804676352029
    policy_reward_min:
      a: 201.0594646019633
      p: 672.4937163208523
    sampler_perf:
      mean_action_processing_ms: 0.5271711165233106
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.140503640019203
      mean_inference_ms: 8.257630187459663
      mean_raw_obs_processing_ms: 2.0362770136478026
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/flat1/dense_logs/logs_02
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.10159611701965332
    StateBufferConnector_ms: 0.00903010368347168
    ViewRequirementAgentConnector_ms: 0.208359956741333
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1337.1243503594405
  episode_reward_mean: 1292.3134191448103
  episode_reward_min: 1247.5024879301802
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1247.5024879301802
    - 1337.1243503594405
    policy_a_reward: [183.52732790377866, 198.72323406227738, 187.71642543225502,
      280.948834990887, 108.2367823341915, 216.73953530285976, 228.52186128051247,
      84.90228509075418, 129.35996595229113, 259.40131121484734]
    policy_p_reward:
    - 288.3498832067941
    - 418.199391518176
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 280.948834990887
    p: 418.199391518176
  policy_reward_mean:
    a: 187.8077563564654
    p: 353.27463736248507
  policy_reward_min:
    a: 84.90228509075418
    p: 288.3498832067941
  sampler_perf:
    mean_action_processing_ms: 0.5272203299595319
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.093526661961988
    mean_inference_ms: 8.378700516570634
    mean_raw_obs_processing_ms: 2.0264430620383167
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.10159611701965332
      StateBufferConnector_ms: 0.00903010368347168
      ViewRequirementAgentConnector_ms: 0.208359956741333
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1337.1243503594405
    episode_reward_mean: 1292.3134191448103
    episode_reward_min: 1247.5024879301802
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1247.5024879301802
      - 1337.1243503594405
      policy_a_reward: [183.52732790377866, 198.72323406227738, 187.71642543225502,
        280.948834990887, 108.2367823341915, 216.73953530285976, 228.52186128051247,
        84.90228509075418, 129.35996595229113, 259.40131121484734]
      policy_p_reward:
      - 288.3498832067941
      - 418.199391518176
    num_faulty_episodes: 0
    policy_reward_max:
      a: 280.948834990887
      p: 418.199391518176
    policy_reward_mean:
      a: 187.8077563564654
      p: 353.27463736248507
    policy_reward_min:
      a: 84.90228509075418
      p: 288.3498832067941
    sampler_perf:
      mean_action_processing_ms: 0.5272203299595319
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.093526661961988
      mean_inference_ms: 8.378700516570634
      mean_raw_obs_processing_ms: 2.0264430620383167
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/flat1/dense_logs/logs_03
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.10396242141723633
    StateBufferConnector_ms: 0.009775161743164062
    ViewRequirementAgentConnector_ms: 0.24020075798034668
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1386.8407601330546
  episode_reward_mean: 1278.0774759373494
  episode_reward_min: 1169.3141917416442
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1386.8407601330546
    - 1169.3141917416442
    policy_a_reward: [257.2865512403139, 291.043273756378, 254.16463291692114, 143.0336532340398,
      129.10301208528352, 151.8090958792698, 179.19904902139103, 53.25770125257806,
      193.49909856569147, 347.45483996219906]
    policy_p_reward:
    - 312.20963690011865
    - 244.09440706051504
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 347.45483996219906
    p: 312.20963690011865
  policy_reward_mean:
    a: 199.98509079140658
    p: 278.1520219803168
  policy_reward_min:
    a: 53.25770125257806
    p: 244.09440706051504
  sampler_perf:
    mean_action_processing_ms: 0.5473311735791523
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.177284164459216
    mean_inference_ms: 8.893165813356054
    mean_raw_obs_processing_ms: 2.100370731986746
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.10396242141723633
      StateBufferConnector_ms: 0.009775161743164062
      ViewRequirementAgentConnector_ms: 0.24020075798034668
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1386.8407601330546
    episode_reward_mean: 1278.0774759373494
    episode_reward_min: 1169.3141917416442
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1386.8407601330546
      - 1169.3141917416442
      policy_a_reward: [257.2865512403139, 291.043273756378, 254.16463291692114, 143.0336532340398,
        129.10301208528352, 151.8090958792698, 179.19904902139103, 53.25770125257806,
        193.49909856569147, 347.45483996219906]
      policy_p_reward:
      - 312.20963690011865
      - 244.09440706051504
    num_faulty_episodes: 0
    policy_reward_max:
      a: 347.45483996219906
      p: 312.20963690011865
    policy_reward_mean:
      a: 199.98509079140658
      p: 278.1520219803168
    policy_reward_min:
      a: 53.25770125257806
      p: 244.09440706051504
    sampler_perf:
      mean_action_processing_ms: 0.5473311735791523
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.177284164459216
      mean_inference_ms: 8.893165813356054
      mean_raw_obs_processing_ms: 2.100370731986746
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/flat1/dense_logs/logs_04
Completing! Saving final snapshot...


Final snapshot saved! All done.
seed (final): 63868000
Not restoring trainer...
Restoring agents weights...
Starting with fresh planner weights.
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.08973479270935059
    StateBufferConnector_ms: 0.009310245513916016
    ViewRequirementAgentConnector_ms: 0.19707083702087402
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1724.666546885579
  episode_reward_mean: 1610.137311329841
  episode_reward_min: 1495.6080757741026
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1724.666546885579
    - 1495.6080757741026
    policy_a_reward: [281.1588169222309, 324.7953734748568, 234.46810696867757, 339.2158158974193,
      367.06635601089806, 335.82381423163815, 323.12912077989483, 249.2964264907431,
      307.35462863373743, 217.54614120486045]
    policy_p_reward:
    - 177.96207761149572
    - 62.45794443322849
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 367.06635601089806
    p: 177.96207761149572
  policy_reward_mean:
    a: 297.98546006149564
    p: 120.2100110223621
  policy_reward_min:
    a: 217.54614120486045
    p: 62.45794443322849
  sampler_perf:
    mean_action_processing_ms: 0.6359346850427563
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.730190345627105
    mean_inference_ms: 8.484703813960214
    mean_raw_obs_processing_ms: 2.4367487597132396
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.08973479270935059
      StateBufferConnector_ms: 0.009310245513916016
      ViewRequirementAgentConnector_ms: 0.19707083702087402
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1724.666546885579
    episode_reward_mean: 1610.137311329841
    episode_reward_min: 1495.6080757741026
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1724.666546885579
      - 1495.6080757741026
      policy_a_reward: [281.1588169222309, 324.7953734748568, 234.46810696867757,
        339.2158158974193, 367.06635601089806, 335.82381423163815, 323.12912077989483,
        249.2964264907431, 307.35462863373743, 217.54614120486045]
      policy_p_reward:
      - 177.96207761149572
      - 62.45794443322849
    num_faulty_episodes: 0
    policy_reward_max:
      a: 367.06635601089806
      p: 177.96207761149572
    policy_reward_mean:
      a: 297.98546006149564
      p: 120.2100110223621
    policy_reward_min:
      a: 217.54614120486045
      p: 62.45794443322849
    sampler_perf:
      mean_action_processing_ms: 0.6359346850427563
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.730190345627105
      mean_inference_ms: 8.484703813960214
      mean_raw_obs_processing_ms: 2.4367487597132396
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/flat1/dense_logs/logs_00
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.09623169898986816
    StateBufferConnector_ms: 0.00883936882019043
    ViewRequirementAgentConnector_ms: 0.20636916160583496
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1572.0821409625635
  episode_reward_mean: 1532.6054686634413
  episode_reward_min: 1493.1287963643192
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1572.0821409625635
    - 1493.1287963643192
    policy_a_reward: [261.45375279678126, 359.2114134032804, 324.9038118661035, 374.60792382339554,
      209.40533791995082, 372.2042308611535, 257.1945814086554, 400.82918389019153,
      256.91167088601173, 178.6479113387948]
    policy_p_reward:
    - 42.49990115305297
    - 27.34121797951596
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 400.82918389019153
    p: 42.49990115305297
  policy_reward_mean:
    a: 299.5369818194319
    p: 34.92055956628447
  policy_reward_min:
    a: 178.6479113387948
    p: 27.34121797951596
  sampler_perf:
    mean_action_processing_ms: 0.5939021096243844
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.6040154360867405
    mean_inference_ms: 8.728739026781325
    mean_raw_obs_processing_ms: 2.2834421514154792
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.09623169898986816
      StateBufferConnector_ms: 0.00883936882019043
      ViewRequirementAgentConnector_ms: 0.20636916160583496
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1572.0821409625635
    episode_reward_mean: 1532.6054686634413
    episode_reward_min: 1493.1287963643192
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1572.0821409625635
      - 1493.1287963643192
      policy_a_reward: [261.45375279678126, 359.2114134032804, 324.9038118661035,
        374.60792382339554, 209.40533791995082, 372.2042308611535, 257.1945814086554,
        400.82918389019153, 256.91167088601173, 178.6479113387948]
      policy_p_reward:
      - 42.49990115305297
      - 27.34121797951596
    num_faulty_episodes: 0
    policy_reward_max:
      a: 400.82918389019153
      p: 42.49990115305297
    policy_reward_mean:
      a: 299.5369818194319
      p: 34.92055956628447
    policy_reward_min:
      a: 178.6479113387948
      p: 27.34121797951596
    sampler_perf:
      mean_action_processing_ms: 0.5939021096243844
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.6040154360867405
      mean_inference_ms: 8.728739026781325
      mean_raw_obs_processing_ms: 2.2834421514154792
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/flat1/dense_logs/logs_01
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.10918378829956055
    StateBufferConnector_ms: 0.009542703628540039
    ViewRequirementAgentConnector_ms: 0.22872686386108398
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1340.8904061486955
  episode_reward_mean: 1108.1386119695321
  episode_reward_min: 875.3868177903688
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1340.8904061486955
    - 875.3868177903688
    policy_a_reward: [196.18205035768344, 257.3235201812289, 261.9313405589375, 328.35939158034506,
      176.55025376248682, 276.89821027186105, 168.5312745372995, 51.91761682972236,
      310.75858350398073, 57.363089760659015]
    policy_p_reward:
    - 120.54384970801516
    - 9.918042886849356
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 328.35939158034506
    p: 120.54384970801516
  policy_reward_mean:
    a: 208.58153313442045
    p: 65.23094629743225
  policy_reward_min:
    a: 51.91761682972236
    p: 9.918042886849356
  sampler_perf:
    mean_action_processing_ms: 0.580797030241151
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.579347741357649
    mean_inference_ms: 8.772897053209643
    mean_raw_obs_processing_ms: 2.239016990992008
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.10918378829956055
      StateBufferConnector_ms: 0.009542703628540039
      ViewRequirementAgentConnector_ms: 0.22872686386108398
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1340.8904061486955
    episode_reward_mean: 1108.1386119695321
    episode_reward_min: 875.3868177903688
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1340.8904061486955
      - 875.3868177903688
      policy_a_reward: [196.18205035768344, 257.3235201812289, 261.9313405589375,
        328.35939158034506, 176.55025376248682, 276.89821027186105, 168.5312745372995,
        51.91761682972236, 310.75858350398073, 57.363089760659015]
      policy_p_reward:
      - 120.54384970801516
      - 9.918042886849356
    num_faulty_episodes: 0
    policy_reward_max:
      a: 328.35939158034506
      p: 120.54384970801516
    policy_reward_mean:
      a: 208.58153313442045
      p: 65.23094629743225
    policy_reward_min:
      a: 51.91761682972236
      p: 9.918042886849356
    sampler_perf:
      mean_action_processing_ms: 0.580797030241151
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.579347741357649
      mean_inference_ms: 8.772897053209643
      mean_raw_obs_processing_ms: 2.239016990992008
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/flat1/dense_logs/logs_02
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.10294914245605469
    StateBufferConnector_ms: 0.009429454803466797
    ViewRequirementAgentConnector_ms: 0.22293925285339355
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1565.4134000971244
  episode_reward_mean: 1523.4941701295588
  episode_reward_min: 1481.5749401619933
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1481.5749401619933
    - 1565.4134000971244
    policy_a_reward: [292.60469505953347, 158.75101200037335, 292.5234001588317, 275.5095152542052,
      244.16325425226407, 318.7046723343657, 265.431326534955, 327.4755129950161,
      282.20475494933163, 295.36427970795336]
    policy_p_reward:
    - 218.02306343678777
    - 76.23285357549767
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 327.4755129950161
    p: 218.02306343678777
  policy_reward_mean:
    a: 275.27324232468294
    p: 147.1279585061427
  policy_reward_min:
    a: 158.75101200037335
    p: 76.23285357549767
  sampler_perf:
    mean_action_processing_ms: 0.5796198723376959
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.478595305656803
    mean_inference_ms: 8.917767545212989
    mean_raw_obs_processing_ms: 2.2386080738546132
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.10294914245605469
      StateBufferConnector_ms: 0.009429454803466797
      ViewRequirementAgentConnector_ms: 0.22293925285339355
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1565.4134000971244
    episode_reward_mean: 1523.4941701295588
    episode_reward_min: 1481.5749401619933
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1481.5749401619933
      - 1565.4134000971244
      policy_a_reward: [292.60469505953347, 158.75101200037335, 292.5234001588317,
        275.5095152542052, 244.16325425226407, 318.7046723343657, 265.431326534955,
        327.4755129950161, 282.20475494933163, 295.36427970795336]
      policy_p_reward:
      - 218.02306343678777
      - 76.23285357549767
    num_faulty_episodes: 0
    policy_reward_max:
      a: 327.4755129950161
      p: 218.02306343678777
    policy_reward_mean:
      a: 275.27324232468294
      p: 147.1279585061427
    policy_reward_min:
      a: 158.75101200037335
      p: 76.23285357549767
    sampler_perf:
      mean_action_processing_ms: 0.5796198723376959
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.478595305656803
      mean_inference_ms: 8.917767545212989
      mean_raw_obs_processing_ms: 2.2386080738546132
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/flat1/dense_logs/logs_03
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.09412169456481934
    StateBufferConnector_ms: 0.010091066360473633
    ViewRequirementAgentConnector_ms: 0.22385120391845703
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1478.2771405821652
  episode_reward_mean: 1307.126751516176
  episode_reward_min: 1135.9763624501868
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1478.2771405821652
    - 1135.9763624501868
    policy_a_reward: [281.6987552457926, 260.47384933444863, 254.9873680940823, 256.25288257590597,
      274.34595552742866, 294.0018642396092, 134.65801772946455, 74.31902810372256,
      183.96590896727065, 300.4902784840999]
    policy_p_reward:
    - 150.5183298045065
    - 148.54126492602174
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 300.4902784840999
    p: 150.5183298045065
  policy_reward_mean:
    a: 231.51939083018252
    p: 149.52979736526413
  policy_reward_min:
    a: 74.31902810372256
    p: 148.54126492602174
  sampler_perf:
    mean_action_processing_ms: 0.5803513364856694
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.487322979286069
    mean_inference_ms: 9.007122935127708
    mean_raw_obs_processing_ms: 2.236635005269133
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.09412169456481934
      StateBufferConnector_ms: 0.010091066360473633
      ViewRequirementAgentConnector_ms: 0.22385120391845703
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1478.2771405821652
    episode_reward_mean: 1307.126751516176
    episode_reward_min: 1135.9763624501868
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1478.2771405821652
      - 1135.9763624501868
      policy_a_reward: [281.6987552457926, 260.47384933444863, 254.9873680940823,
        256.25288257590597, 274.34595552742866, 294.0018642396092, 134.65801772946455,
        74.31902810372256, 183.96590896727065, 300.4902784840999]
      policy_p_reward:
      - 150.5183298045065
      - 148.54126492602174
    num_faulty_episodes: 0
    policy_reward_max:
      a: 300.4902784840999
      p: 150.5183298045065
    policy_reward_mean:
      a: 231.51939083018252
      p: 149.52979736526413
    policy_reward_min:
      a: 74.31902810372256
      p: 148.54126492602174
    sampler_perf:
      mean_action_processing_ms: 0.5803513364856694
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.487322979286069
      mean_inference_ms: 9.007122935127708
      mean_raw_obs_processing_ms: 2.236635005269133
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/flat1/dense_logs/logs_04
Completing! Saving final snapshot...


Final snapshot saved! All done.
seed (final): 29110000
Not restoring trainer...
Restoring agents weights...
Starting with fresh planner weights.
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.08531808853149414
    StateBufferConnector_ms: 0.008594989776611328
    ViewRequirementAgentConnector_ms: 0.19998550415039062
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1564.8019370824143
  episode_reward_mean: 1526.3582173992627
  episode_reward_min: 1487.9144977161109
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1487.9144977161109
    - 1564.8019370824143
    policy_a_reward: [363.07852831707976, 334.12679030167493, 161.66194092454862,
      258.3930388897973, 309.2036675534025, 253.19073664140993, 264.10962514632513,
      215.45459801257144, 269.78237796588957, 358.60174852717216]
    policy_p_reward:
    - 61.45053172961008
    - 203.66285078904605
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 363.07852831707976
    p: 203.66285078904605
  policy_reward_mean:
    a: 278.76030522798715
    p: 132.55669125932806
  policy_reward_min:
    a: 161.66194092454862
    p: 61.45053172961008
  sampler_perf:
    mean_action_processing_ms: 0.5663597655153559
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.424192710313016
    mean_inference_ms: 7.790770597324637
    mean_raw_obs_processing_ms: 2.2253975896778218
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.08531808853149414
      StateBufferConnector_ms: 0.008594989776611328
      ViewRequirementAgentConnector_ms: 0.19998550415039062
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1564.8019370824143
    episode_reward_mean: 1526.3582173992627
    episode_reward_min: 1487.9144977161109
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1487.9144977161109
      - 1564.8019370824143
      policy_a_reward: [363.07852831707976, 334.12679030167493, 161.66194092454862,
        258.3930388897973, 309.2036675534025, 253.19073664140993, 264.10962514632513,
        215.45459801257144, 269.78237796588957, 358.60174852717216]
      policy_p_reward:
      - 61.45053172961008
      - 203.66285078904605
    num_faulty_episodes: 0
    policy_reward_max:
      a: 363.07852831707976
      p: 203.66285078904605
    policy_reward_mean:
      a: 278.76030522798715
      p: 132.55669125932806
    policy_reward_min:
      a: 161.66194092454862
      p: 61.45053172961008
    sampler_perf:
      mean_action_processing_ms: 0.5663597655153559
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.424192710313016
      mean_inference_ms: 7.790770597324637
      mean_raw_obs_processing_ms: 2.2253975896778218
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/flat1/dense_logs/logs_00
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.10528564453125
    StateBufferConnector_ms: 0.00883340835571289
    ViewRequirementAgentConnector_ms: 0.20862221717834473
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1476.178908812681
  episode_reward_mean: 1260.5866554416746
  episode_reward_min: 1044.9944020706682
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1476.178908812681
    - 1044.9944020706682
    policy_a_reward: [369.5030597709356, 270.48955710701824, 280.97941369679774, 307.2692949662436,
      176.96293286896193, 109.98577028969112, 157.2853067145483, 310.6892915148575,
      109.95397937644115, 311.6250634768197]
    policy_p_reward:
    - 70.97465040272132
    - 45.45499069831736
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 369.5030597709356
    p: 70.97465040272132
  policy_reward_mean:
    a: 240.4743669782315
    p: 58.21482055051934
  policy_reward_min:
    a: 109.95397937644115
    p: 45.45499069831736
  sampler_perf:
    mean_action_processing_ms: 0.5522660323075362
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.299630413760434
    mean_inference_ms: 8.392651002485675
    mean_raw_obs_processing_ms: 2.1013060768882
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.10528564453125
      StateBufferConnector_ms: 0.00883340835571289
      ViewRequirementAgentConnector_ms: 0.20862221717834473
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1476.178908812681
    episode_reward_mean: 1260.5866554416746
    episode_reward_min: 1044.9944020706682
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1476.178908812681
      - 1044.9944020706682
      policy_a_reward: [369.5030597709356, 270.48955710701824, 280.97941369679774,
        307.2692949662436, 176.96293286896193, 109.98577028969112, 157.2853067145483,
        310.6892915148575, 109.95397937644115, 311.6250634768197]
      policy_p_reward:
      - 70.97465040272132
      - 45.45499069831736
    num_faulty_episodes: 0
    policy_reward_max:
      a: 369.5030597709356
      p: 70.97465040272132
    policy_reward_mean:
      a: 240.4743669782315
      p: 58.21482055051934
    policy_reward_min:
      a: 109.95397937644115
      p: 45.45499069831736
    sampler_perf:
      mean_action_processing_ms: 0.5522660323075362
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.299630413760434
      mean_inference_ms: 8.392651002485675
      mean_raw_obs_processing_ms: 2.1013060768882
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/flat1/dense_logs/logs_01
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.11380910873413086
    StateBufferConnector_ms: 0.008577108383178711
    ViewRequirementAgentConnector_ms: 0.1968860626220703
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1641.504768498718
  episode_reward_mean: 1382.3213099187099
  episode_reward_min: 1123.1378513387017
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1123.1378513387017
    - 1641.504768498718
    policy_a_reward: [124.11408251922853, 230.86012983777283, 98.2723547590324, 156.76853238582456,
      291.24604843964386, 261.2940126056909, 308.204050390987, 264.5153299064273,
      268.39524982902674, 255.14220410774988]
    policy_p_reward:
    - 221.87670339720017
    - 283.95392165883374
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 308.204050390987
    p: 283.95392165883374
  policy_reward_mean:
    a: 225.88119947813843
    p: 252.91531252801695
  policy_reward_min:
    a: 98.2723547590324
    p: 221.87670339720017
  sampler_perf:
    mean_action_processing_ms: 0.542091100236561
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.239599042380356
    mean_inference_ms: 8.68524590148519
    mean_raw_obs_processing_ms: 2.052385754620211
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.11380910873413086
      StateBufferConnector_ms: 0.008577108383178711
      ViewRequirementAgentConnector_ms: 0.1968860626220703
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1641.504768498718
    episode_reward_mean: 1382.3213099187099
    episode_reward_min: 1123.1378513387017
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1123.1378513387017
      - 1641.504768498718
      policy_a_reward: [124.11408251922853, 230.86012983777283, 98.2723547590324,
        156.76853238582456, 291.24604843964386, 261.2940126056909, 308.204050390987,
        264.5153299064273, 268.39524982902674, 255.14220410774988]
      policy_p_reward:
      - 221.87670339720017
      - 283.95392165883374
    num_faulty_episodes: 0
    policy_reward_max:
      a: 308.204050390987
      p: 283.95392165883374
    policy_reward_mean:
      a: 225.88119947813843
      p: 252.91531252801695
    policy_reward_min:
      a: 98.2723547590324
      p: 221.87670339720017
    sampler_perf:
      mean_action_processing_ms: 0.542091100236561
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.239599042380356
      mean_inference_ms: 8.68524590148519
      mean_raw_obs_processing_ms: 2.052385754620211
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/flat1/dense_logs/logs_02
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.09019970893859863
    StateBufferConnector_ms: 0.008356571197509766
    ViewRequirementAgentConnector_ms: 0.21371841430664062
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1451.1115226080103
  episode_reward_mean: 1375.3828600130955
  episode_reward_min: 1299.6541974181805
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1451.1115226080103
    - 1299.6541974181805
    policy_a_reward: [284.8536406255977, 295.5196320897466, 251.3665596160416, 368.5887381900974,
      199.92056817771694, 284.21202051536187, 224.08972170747847, 287.4258976257671,
      141.45454072606043, 328.4936133402304]
    policy_p_reward:
    - 50.86238390881226
    - 33.97840350328
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 368.5887381900974
    p: 50.86238390881226
  policy_reward_mean:
    a: 266.5924932614098
    p: 42.42039370604613
  policy_reward_min:
    a: 141.45454072606043
    p: 33.97840350328
  sampler_perf:
    mean_action_processing_ms: 0.5391405440163219
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.11942599714547
    mean_inference_ms: 8.817499247507595
    mean_raw_obs_processing_ms: 2.0291130879948818
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.09019970893859863
      StateBufferConnector_ms: 0.008356571197509766
      ViewRequirementAgentConnector_ms: 0.21371841430664062
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1451.1115226080103
    episode_reward_mean: 1375.3828600130955
    episode_reward_min: 1299.6541974181805
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1451.1115226080103
      - 1299.6541974181805
      policy_a_reward: [284.8536406255977, 295.5196320897466, 251.3665596160416, 368.5887381900974,
        199.92056817771694, 284.21202051536187, 224.08972170747847, 287.4258976257671,
        141.45454072606043, 328.4936133402304]
      policy_p_reward:
      - 50.86238390881226
      - 33.97840350328
    num_faulty_episodes: 0
    policy_reward_max:
      a: 368.5887381900974
      p: 50.86238390881226
    policy_reward_mean:
      a: 266.5924932614098
      p: 42.42039370604613
    policy_reward_min:
      a: 141.45454072606043
      p: 33.97840350328
    sampler_perf:
      mean_action_processing_ms: 0.5391405440163219
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.11942599714547
      mean_inference_ms: 8.817499247507595
      mean_raw_obs_processing_ms: 2.0291130879948818
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/flat1/dense_logs/logs_03
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.11764764785766602
    StateBufferConnector_ms: 0.009369850158691406
    ViewRequirementAgentConnector_ms: 0.20548701286315918
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1319.0222391702464
  episode_reward_mean: 1264.7554757070836
  episode_reward_min: 1210.4887122439209
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1210.4887122439209
    - 1319.0222391702464
    policy_a_reward: [76.64241365965687, 209.59007736060715, 279.03482169806085, 243.85556734308207,
      248.5272163619464, 201.44153743455897, 400.64509389627807, 264.5996797816534,
      291.6873992368773, 116.46544310514545]
    policy_p_reward:
    - 152.8386158205711
    - 44.18308571573209
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 400.64509389627807
    p: 152.8386158205711
  policy_reward_mean:
    a: 233.24892498778664
    p: 98.5108507681516
  policy_reward_min:
    a: 76.64241365965687
    p: 44.18308571573209
  sampler_perf:
    mean_action_processing_ms: 0.5358927062109726
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.19919541338738
    mean_inference_ms: 8.823643584863419
    mean_raw_obs_processing_ms: 2.0574291721909868
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.11764764785766602
      StateBufferConnector_ms: 0.009369850158691406
      ViewRequirementAgentConnector_ms: 0.20548701286315918
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1319.0222391702464
    episode_reward_mean: 1264.7554757070836
    episode_reward_min: 1210.4887122439209
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1210.4887122439209
      - 1319.0222391702464
      policy_a_reward: [76.64241365965687, 209.59007736060715, 279.03482169806085,
        243.85556734308207, 248.5272163619464, 201.44153743455897, 400.64509389627807,
        264.5996797816534, 291.6873992368773, 116.46544310514545]
      policy_p_reward:
      - 152.8386158205711
      - 44.18308571573209
    num_faulty_episodes: 0
    policy_reward_max:
      a: 400.64509389627807
      p: 152.8386158205711
    policy_reward_mean:
      a: 233.24892498778664
      p: 98.5108507681516
    policy_reward_min:
      a: 76.64241365965687
      p: 44.18308571573209
    sampler_perf:
      mean_action_processing_ms: 0.5358927062109726
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.19919541338738
      mean_inference_ms: 8.823643584863419
      mean_raw_obs_processing_ms: 2.0574291721909868
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/flat1/dense_logs/logs_04
Completing! Saving final snapshot...


Final snapshot saved! All done.
seed (final): 32671000
Not restoring trainer...
Restoring agents weights...
Starting with fresh planner weights.
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.08858442306518555
    StateBufferConnector_ms: 0.008791685104370117
    ViewRequirementAgentConnector_ms: 0.19559860229492188
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 2221.9277807985195
  episode_reward_mean: 2193.6445350994727
  episode_reward_min: 2165.3612894004254
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 2221.9277807985195
    - 2165.3612894004254
    policy_a_reward: [339.6452444564054, 351.84188933953146, 268.41450964709827, 314.17755834180605,
      259.5539642785657, 317.57829164210875, 275.7594769079421, 265.26610926829045,
      398.9698930739953, 267.82813722555915]
    policy_p_reward:
    - 688.2946147351187
    - 639.959381282531
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 398.9698930739953
    p: 688.2946147351187
  policy_reward_mean:
    a: 305.90350741813023
    p: 664.1269980088248
  policy_reward_min:
    a: 259.5539642785657
    p: 639.959381282531
  sampler_perf:
    mean_action_processing_ms: 0.5185361393911395
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.128805415597029
    mean_inference_ms: 7.377414170377507
    mean_raw_obs_processing_ms: 2.2122622012140267
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.08858442306518555
      StateBufferConnector_ms: 0.008791685104370117
      ViewRequirementAgentConnector_ms: 0.19559860229492188
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 2221.9277807985195
    episode_reward_mean: 2193.6445350994727
    episode_reward_min: 2165.3612894004254
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 2221.9277807985195
      - 2165.3612894004254
      policy_a_reward: [339.6452444564054, 351.84188933953146, 268.41450964709827,
        314.17755834180605, 259.5539642785657, 317.57829164210875, 275.7594769079421,
        265.26610926829045, 398.9698930739953, 267.82813722555915]
      policy_p_reward:
      - 688.2946147351187
      - 639.959381282531
    num_faulty_episodes: 0
    policy_reward_max:
      a: 398.9698930739953
      p: 688.2946147351187
    policy_reward_mean:
      a: 305.90350741813023
      p: 664.1269980088248
    policy_reward_min:
      a: 259.5539642785657
      p: 639.959381282531
    sampler_perf:
      mean_action_processing_ms: 0.5185361393911395
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.128805415597029
      mean_inference_ms: 7.377414170377507
      mean_raw_obs_processing_ms: 2.2122622012140267
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/1/flat/dense_logs/logs_00
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.10092854499816895
    StateBufferConnector_ms: 0.008803606033325195
    ViewRequirementAgentConnector_ms: 0.20441412925720215
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 2159.6470876020194
  episode_reward_mean: 1623.504919322335
  episode_reward_min: 1087.3627510426506
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 2159.6470876020194
    - 1087.3627510426506
    policy_a_reward: [283.9628176485862, 369.73410673649374, 376.0258408356175, 328.7840226849392,
      191.19230967855503, 226.02398798878238, 80.65415961046605, 252.81241956845435,
      322.01291292087245, 38.83019005444516]
    policy_p_reward:
    - 609.9479900178343
    - 167.02908089963057
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 376.0258408356175
    p: 609.9479900178343
  policy_reward_mean:
    a: 247.00327677272122
    p: 388.48853545873243
  policy_reward_min:
    a: 38.83019005444516
    p: 167.02908089963057
  sampler_perf:
    mean_action_processing_ms: 0.5371380042839241
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.256194883531386
    mean_inference_ms: 8.315632988761116
    mean_raw_obs_processing_ms: 2.1522888294109457
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.10092854499816895
      StateBufferConnector_ms: 0.008803606033325195
      ViewRequirementAgentConnector_ms: 0.20441412925720215
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 2159.6470876020194
    episode_reward_mean: 1623.504919322335
    episode_reward_min: 1087.3627510426506
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 2159.6470876020194
      - 1087.3627510426506
      policy_a_reward: [283.9628176485862, 369.73410673649374, 376.0258408356175,
        328.7840226849392, 191.19230967855503, 226.02398798878238, 80.65415961046605,
        252.81241956845435, 322.01291292087245, 38.83019005444516]
      policy_p_reward:
      - 609.9479900178343
      - 167.02908089963057
    num_faulty_episodes: 0
    policy_reward_max:
      a: 376.0258408356175
      p: 609.9479900178343
    policy_reward_mean:
      a: 247.00327677272122
      p: 388.48853545873243
    policy_reward_min:
      a: 38.83019005444516
      p: 167.02908089963057
    sampler_perf:
      mean_action_processing_ms: 0.5371380042839241
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.256194883531386
      mean_inference_ms: 8.315632988761116
      mean_raw_obs_processing_ms: 2.1522888294109457
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/1/flat/dense_logs/logs_01
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.09692907333374023
    StateBufferConnector_ms: 0.008606910705566406
    ViewRequirementAgentConnector_ms: 0.202256441116333
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 2120.2961640657886
  episode_reward_mean: 1574.4168043813888
  episode_reward_min: 1028.537444696989
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1028.537444696989
    - 2120.2961640657886
    policy_a_reward: [317.60229382002615, 30.396660891974825, 60.11082799066692, 177.0028051912224,
      271.56579838982583, 328.67327344364696, 244.2922867360318, 335.69536606074524,
      321.64670858405026, 308.31878691115173]
    policy_p_reward:
    - 171.85905841327343
    - 581.6697423301638
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 335.69536606074524
    p: 581.6697423301638
  policy_reward_mean:
    a: 239.5304808019342
    p: 376.7644003717186
  policy_reward_min:
    a: 30.396660891974825
    p: 171.85905841327343
  sampler_perf:
    mean_action_processing_ms: 0.5377114097092011
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.2656746647661326
    mean_inference_ms: 8.626133342491316
    mean_raw_obs_processing_ms: 2.107388968470889
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.09692907333374023
      StateBufferConnector_ms: 0.008606910705566406
      ViewRequirementAgentConnector_ms: 0.202256441116333
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 2120.2961640657886
    episode_reward_mean: 1574.4168043813888
    episode_reward_min: 1028.537444696989
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1028.537444696989
      - 2120.2961640657886
      policy_a_reward: [317.60229382002615, 30.396660891974825, 60.11082799066692,
        177.0028051912224, 271.56579838982583, 328.67327344364696, 244.2922867360318,
        335.69536606074524, 321.64670858405026, 308.31878691115173]
      policy_p_reward:
      - 171.85905841327343
      - 581.6697423301638
    num_faulty_episodes: 0
    policy_reward_max:
      a: 335.69536606074524
      p: 581.6697423301638
    policy_reward_mean:
      a: 239.5304808019342
      p: 376.7644003717186
    policy_reward_min:
      a: 30.396660891974825
      p: 171.85905841327343
    sampler_perf:
      mean_action_processing_ms: 0.5377114097092011
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.2656746647661326
      mean_inference_ms: 8.626133342491316
      mean_raw_obs_processing_ms: 2.107388968470889
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/1/flat/dense_logs/logs_02
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.10676980018615723
    StateBufferConnector_ms: 0.012183189392089844
    ViewRequirementAgentConnector_ms: 0.21211504936218262
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 2186.948856225941
  episode_reward_mean: 1820.9881377069937
  episode_reward_min: 1455.027419188046
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1455.027419188046
    - 2186.948856225941
    policy_a_reward: [277.37611366263417, 92.17681716623875, 236.47610275958795, 264.45937446470396,
      183.04503970958328, 269.9993947044558, 329.4727009526007, 262.14755710433457,
      268.6688662262051, 304.2332842778712]
    policy_p_reward:
    - 401.49397142530097
    - 752.4270529604643
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 329.4727009526007
    p: 752.4270529604643
  policy_reward_mean:
    a: 248.80552510282155
    p: 576.9605121928826
  policy_reward_min:
    a: 92.17681716623875
    p: 401.49397142530097
  sampler_perf:
    mean_action_processing_ms: 0.5374775714483456
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.161873559603865
    mean_inference_ms: 8.617975424671698
    mean_raw_obs_processing_ms: 2.091492253026624
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.10676980018615723
      StateBufferConnector_ms: 0.012183189392089844
      ViewRequirementAgentConnector_ms: 0.21211504936218262
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 2186.948856225941
    episode_reward_mean: 1820.9881377069937
    episode_reward_min: 1455.027419188046
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1455.027419188046
      - 2186.948856225941
      policy_a_reward: [277.37611366263417, 92.17681716623875, 236.47610275958795,
        264.45937446470396, 183.04503970958328, 269.9993947044558, 329.4727009526007,
        262.14755710433457, 268.6688662262051, 304.2332842778712]
      policy_p_reward:
      - 401.49397142530097
      - 752.4270529604643
    num_faulty_episodes: 0
    policy_reward_max:
      a: 329.4727009526007
      p: 752.4270529604643
    policy_reward_mean:
      a: 248.80552510282155
      p: 576.9605121928826
    policy_reward_min:
      a: 92.17681716623875
      p: 401.49397142530097
    sampler_perf:
      mean_action_processing_ms: 0.5374775714483456
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.161873559603865
      mean_inference_ms: 8.617975424671698
      mean_raw_obs_processing_ms: 2.091492253026624
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/1/flat/dense_logs/logs_03
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.09128451347351074
    StateBufferConnector_ms: 0.009268522262573242
    ViewRequirementAgentConnector_ms: 0.22527575492858887
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1833.0003093275163
  episode_reward_mean: 1721.0099999391664
  episode_reward_min: 1609.0196905508167
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1833.0003093275163
    - 1609.0196905508167
    policy_a_reward: [251.50533426131022, 275.06036214666335, 222.91390839974287,
      267.9379466235464, 240.52898986573916, 220.1656210176339, 137.15148812551215,
      184.07257044351795, 258.0210784096713, 297.7938600543243]
    policy_p_reward:
    - 575.0537680305142
    - 511.8150725001593
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 297.7938600543243
    p: 575.0537680305142
  policy_reward_mean:
    a: 235.51511593476616
    p: 543.4344202653367
  policy_reward_min:
    a: 137.15148812551215
    p: 511.8150725001593
  sampler_perf:
    mean_action_processing_ms: 0.5412683254334985
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.2273159362658745
    mean_inference_ms: 8.670729954020588
    mean_raw_obs_processing_ms: 2.0836720891782448
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.09128451347351074
      StateBufferConnector_ms: 0.009268522262573242
      ViewRequirementAgentConnector_ms: 0.22527575492858887
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1833.0003093275163
    episode_reward_mean: 1721.0099999391664
    episode_reward_min: 1609.0196905508167
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1833.0003093275163
      - 1609.0196905508167
      policy_a_reward: [251.50533426131022, 275.06036214666335, 222.91390839974287,
        267.9379466235464, 240.52898986573916, 220.1656210176339, 137.15148812551215,
        184.07257044351795, 258.0210784096713, 297.7938600543243]
      policy_p_reward:
      - 575.0537680305142
      - 511.8150725001593
    num_faulty_episodes: 0
    policy_reward_max:
      a: 297.7938600543243
      p: 575.0537680305142
    policy_reward_mean:
      a: 235.51511593476616
      p: 543.4344202653367
    policy_reward_min:
      a: 137.15148812551215
      p: 511.8150725001593
    sampler_perf:
      mean_action_processing_ms: 0.5412683254334985
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.2273159362658745
      mean_inference_ms: 8.670729954020588
      mean_raw_obs_processing_ms: 2.0836720891782448
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/1/flat/dense_logs/logs_04
Completing! Saving final snapshot...


Final snapshot saved! All done.
