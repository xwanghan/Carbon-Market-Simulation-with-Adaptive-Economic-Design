seed (final): 35247000
seed (final): 35291000
Not restoring trainer...
Restoring agents weights...
Restoring planner weights...
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0953376293182373
    StateBufferConnector_ms: 0.009483098983764648
    ViewRequirementAgentConnector_ms: 0.227433443069458
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1868.044998874857
  episode_reward_mean: 1547.419347391944
  episode_reward_min: 1226.7936959090312
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1868.044998874857
    - 1226.7936959090312
    policy_a_reward: [224.5869746553585, 190.44474314357166, 200.334554883269, 263.20737257660284,
      227.17581649205962, 102.55796660943318, 165.23113352256493, 256.5261078677387,
      189.80766894764687, 101.06829063559024]
    policy_p_reward:
    - 762.2955371239934
    - 411.6025283260545
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 263.20737257660284
    p: 762.2955371239934
  policy_reward_mean:
    a: 192.09406293338355
    p: 586.9490327250239
  policy_reward_min:
    a: 101.06829063559024
    p: 411.6025283260545
  sampler_perf:
    mean_action_processing_ms: 0.5249063411872543
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.483185842365562
    mean_inference_ms: 10.600520227245703
    mean_raw_obs_processing_ms: 2.2483723844120838
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.0953376293182373
      StateBufferConnector_ms: 0.009483098983764648
      ViewRequirementAgentConnector_ms: 0.227433443069458
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1868.044998874857
    episode_reward_mean: 1547.419347391944
    episode_reward_min: 1226.7936959090312
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1868.044998874857
      - 1226.7936959090312
      policy_a_reward: [224.5869746553585, 190.44474314357166, 200.334554883269, 263.20737257660284,
        227.17581649205962, 102.55796660943318, 165.23113352256493, 256.5261078677387,
        189.80766894764687, 101.06829063559024]
      policy_p_reward:
      - 762.2955371239934
      - 411.6025283260545
    num_faulty_episodes: 0
    policy_reward_max:
      a: 263.20737257660284
      p: 762.2955371239934
    policy_reward_mean:
      a: 192.09406293338355
      p: 586.9490327250239
    policy_reward_min:
      a: 101.06829063559024
      p: 411.6025283260545
    sampler_perf:
      mean_action_processing_ms: 0.5249063411872543
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.483185842365562
      mean_inference_ms: 10.600520227245703
      mean_raw_obs_processing_ms: 2.2483723844120838
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/marl1/dense_logs/logs_00
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.09645223617553711
    StateBufferConnector_ms: 0.012087821960449219
    ViewRequirementAgentConnector_ms: 0.19112825393676758
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1746.9215447106883
  episode_reward_mean: 1637.1501063944997
  episode_reward_min: 1527.3786680783112
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1527.3786680783112
    - 1746.9215447106883
    policy_a_reward: [243.41824535127074, 196.82637968249335, 138.39421664148765,
      141.29695694070517, 228.20091821026625, 226.9558437449328, 160.99684652937003,
      235.2730944366188, 195.73155066979157, 218.7133170462972]
    policy_p_reward:
    - 579.2419512520897
    - 709.2508922836834
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 243.41824535127074
    p: 709.2508922836834
  policy_reward_mean:
    a: 198.58073692532338
    p: 644.2464217678865
  policy_reward_min:
    a: 138.39421664148765
    p: 579.2419512520897
  sampler_perf:
    mean_action_processing_ms: 0.5276660461883088
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.323638045228088
    mean_inference_ms: 11.559398500593035
    mean_raw_obs_processing_ms: 2.0986980015224033
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.09645223617553711
      StateBufferConnector_ms: 0.012087821960449219
      ViewRequirementAgentConnector_ms: 0.19112825393676758
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1746.9215447106883
    episode_reward_mean: 1637.1501063944997
    episode_reward_min: 1527.3786680783112
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1527.3786680783112
      - 1746.9215447106883
      policy_a_reward: [243.41824535127074, 196.82637968249335, 138.39421664148765,
        141.29695694070517, 228.20091821026625, 226.9558437449328, 160.99684652937003,
        235.2730944366188, 195.73155066979157, 218.7133170462972]
      policy_p_reward:
      - 579.2419512520897
      - 709.2508922836834
    num_faulty_episodes: 0
    policy_reward_max:
      a: 243.41824535127074
      p: 709.2508922836834
    policy_reward_mean:
      a: 198.58073692532338
      p: 644.2464217678865
    policy_reward_min:
      a: 138.39421664148765
      p: 579.2419512520897
    sampler_perf:
      mean_action_processing_ms: 0.5276660461883088
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.323638045228088
      mean_inference_ms: 11.559398500593035
      mean_raw_obs_processing_ms: 2.0986980015224033
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/marl1/dense_logs/logs_01
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.09604096412658691
    StateBufferConnector_ms: 0.008213520050048828
    ViewRequirementAgentConnector_ms: 0.19865632057189941
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 2098.6884097852394
  episode_reward_mean: 1957.1537564528912
  episode_reward_min: 1815.619103120543
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 2098.6884097852394
    - 1815.619103120543
    policy_a_reward: [323.35940012069875, 326.9449622615253, 333.5151067798516, 178.58752932093353,
      129.13435857005894, 211.92166023141866, 227.86584472584968, 267.32996818429984,
      230.7556845150316, 147.43092532349667]
    policy_p_reward:
    - 807.1470527321728
    - 730.3150201404422
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 333.5151067798516
    p: 807.1470527321728
  policy_reward_mean:
    a: 237.68454400331643
    p: 768.7310364363075
  policy_reward_min:
    a: 129.13435857005894
    p: 730.3150201404422
  sampler_perf:
    mean_action_processing_ms: 0.5287159926727721
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.31291489979174
    mean_inference_ms: 12.079880127979866
    mean_raw_obs_processing_ms: 2.061158160540678
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.09604096412658691
      StateBufferConnector_ms: 0.008213520050048828
      ViewRequirementAgentConnector_ms: 0.19865632057189941
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 2098.6884097852394
    episode_reward_mean: 1957.1537564528912
    episode_reward_min: 1815.619103120543
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 2098.6884097852394
      - 1815.619103120543
      policy_a_reward: [323.35940012069875, 326.9449622615253, 333.5151067798516,
        178.58752932093353, 129.13435857005894, 211.92166023141866, 227.86584472584968,
        267.32996818429984, 230.7556845150316, 147.43092532349667]
      policy_p_reward:
      - 807.1470527321728
      - 730.3150201404422
    num_faulty_episodes: 0
    policy_reward_max:
      a: 333.5151067798516
      p: 807.1470527321728
    policy_reward_mean:
      a: 237.68454400331643
      p: 768.7310364363075
    policy_reward_min:
      a: 129.13435857005894
      p: 730.3150201404422
    sampler_perf:
      mean_action_processing_ms: 0.5287159926727721
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.31291489979174
      mean_inference_ms: 12.079880127979866
      mean_raw_obs_processing_ms: 2.061158160540678
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/marl1/dense_logs/logs_02
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.10457038879394531
    StateBufferConnector_ms: 0.009173154830932617
    ViewRequirementAgentConnector_ms: 0.21775364875793457
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1732.1402471489823
  episode_reward_mean: 1648.0538759975834
  episode_reward_min: 1563.9675048461845
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1732.1402471489823
    - 1563.9675048461845
    policy_a_reward: [192.0087209474633, 205.28093454024113, 246.05558731086393, 158.48155710319574,
      210.68044357995348, 208.20431754910925, 115.47683647804025, 265.93147916421043,
      139.01620580303043, 253.9751719323849]
    policy_p_reward:
    - 719.633003667268
    - 581.3634939194083
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 265.93147916421043
    p: 719.633003667268
  policy_reward_mean:
    a: 199.5111254408493
    p: 650.4982487933381
  policy_reward_min:
    a: 115.47683647804025
    p: 581.3634939194083
  sampler_perf:
    mean_action_processing_ms: 0.5283557075908457
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.209630314199285
    mean_inference_ms: 12.344271108426195
    mean_raw_obs_processing_ms: 2.0443735451533875
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.10457038879394531
      StateBufferConnector_ms: 0.009173154830932617
      ViewRequirementAgentConnector_ms: 0.21775364875793457
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1732.1402471489823
    episode_reward_mean: 1648.0538759975834
    episode_reward_min: 1563.9675048461845
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1732.1402471489823
      - 1563.9675048461845
      policy_a_reward: [192.0087209474633, 205.28093454024113, 246.05558731086393,
        158.48155710319574, 210.68044357995348, 208.20431754910925, 115.47683647804025,
        265.93147916421043, 139.01620580303043, 253.9751719323849]
      policy_p_reward:
      - 719.633003667268
      - 581.3634939194083
    num_faulty_episodes: 0
    policy_reward_max:
      a: 265.93147916421043
      p: 719.633003667268
    policy_reward_mean:
      a: 199.5111254408493
      p: 650.4982487933381
    policy_reward_min:
      a: 115.47683647804025
      p: 581.3634939194083
    sampler_perf:
      mean_action_processing_ms: 0.5283557075908457
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.209630314199285
      mean_inference_ms: 12.344271108426195
      mean_raw_obs_processing_ms: 2.0443735451533875
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/marl1/dense_logs/logs_03
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.11224746704101562
    StateBufferConnector_ms: 0.008594989776611328
    ViewRequirementAgentConnector_ms: 0.19579529762268066
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1445.6566577361941
  episode_reward_mean: 1420.8608940566403
  episode_reward_min: 1396.0651303770865
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1396.0651303770865
    - 1445.6566577361941
    policy_a_reward: [100.76921355289086, 211.64512542737248, 167.32026472417607,
      228.2440022196227, 155.28283855334942, 205.56887894555794, 183.05303755941588,
      116.79113248392117, 176.18547950520357, 184.32885665911408]
    policy_p_reward:
    - 532.8036858996921
    - 579.7292725829878
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 228.2440022196227
    p: 579.7292725829878
  policy_reward_mean:
    a: 172.91888296306243
    p: 556.26647924134
  policy_reward_min:
    a: 100.76921355289086
    p: 532.8036858996921
  sampler_perf:
    mean_action_processing_ms: 0.5291164135847126
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.124956481793269
    mean_inference_ms: 12.606361016231935
    mean_raw_obs_processing_ms: 2.0295852949408615
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.11224746704101562
      StateBufferConnector_ms: 0.008594989776611328
      ViewRequirementAgentConnector_ms: 0.19579529762268066
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1445.6566577361941
    episode_reward_mean: 1420.8608940566403
    episode_reward_min: 1396.0651303770865
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1396.0651303770865
      - 1445.6566577361941
      policy_a_reward: [100.76921355289086, 211.64512542737248, 167.32026472417607,
        228.2440022196227, 155.28283855334942, 205.56887894555794, 183.05303755941588,
        116.79113248392117, 176.18547950520357, 184.32885665911408]
      policy_p_reward:
      - 532.8036858996921
      - 579.7292725829878
    num_faulty_episodes: 0
    policy_reward_max:
      a: 228.2440022196227
      p: 579.7292725829878
    policy_reward_mean:
      a: 172.91888296306243
      p: 556.26647924134
    policy_reward_min:
      a: 100.76921355289086
      p: 532.8036858996921
    sampler_perf:
      mean_action_processing_ms: 0.5291164135847126
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.124956481793269
      mean_inference_ms: 12.606361016231935
      mean_raw_obs_processing_ms: 2.0295852949408615
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/marl1/dense_logs/logs_04
Completing! Saving final snapshot...


Final snapshot saved! All done.
seed (final): 37201000
seed (final): 37256000
Not restoring trainer...
Restoring agents weights...
Restoring planner weights...
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.07786750793457031
    StateBufferConnector_ms: 0.007891654968261719
    ViewRequirementAgentConnector_ms: 0.18709301948547363
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1433.778370596011
  episode_reward_mean: 1343.0165978230832
  episode_reward_min: 1252.2548250501554
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1252.2548250501554
    - 1433.778370596011
    policy_a_reward: [149.7205172671004, 153.14564070164622, 187.31760970879375, 145.36992253255704,
      245.50136486531827, 228.2163680751606, 207.72385677276495, 278.92565145895037,
      193.21140671200334, 206.24816715607795]
    policy_p_reward:
    - 371.199769974731
    - 319.4529204210576
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 278.92565145895037
    p: 371.199769974731
  policy_reward_mean:
    a: 199.53805052503728
    p: 345.3263451978943
  policy_reward_min:
    a: 145.36992253255704
    p: 319.4529204210576
  sampler_perf:
    mean_action_processing_ms: 0.505291297288236
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.0489894425321715
    mean_inference_ms: 10.526471033305702
    mean_raw_obs_processing_ms: 2.007640526442233
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.07786750793457031
      StateBufferConnector_ms: 0.007891654968261719
      ViewRequirementAgentConnector_ms: 0.18709301948547363
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1433.778370596011
    episode_reward_mean: 1343.0165978230832
    episode_reward_min: 1252.2548250501554
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1252.2548250501554
      - 1433.778370596011
      policy_a_reward: [149.7205172671004, 153.14564070164622, 187.31760970879375,
        145.36992253255704, 245.50136486531827, 228.2163680751606, 207.72385677276495,
        278.92565145895037, 193.21140671200334, 206.24816715607795]
      policy_p_reward:
      - 371.199769974731
      - 319.4529204210576
    num_faulty_episodes: 0
    policy_reward_max:
      a: 278.92565145895037
      p: 371.199769974731
    policy_reward_mean:
      a: 199.53805052503728
      p: 345.3263451978943
    policy_reward_min:
      a: 145.36992253255704
      p: 319.4529204210576
    sampler_perf:
      mean_action_processing_ms: 0.505291297288236
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.0489894425321715
      mean_inference_ms: 10.526471033305702
      mean_raw_obs_processing_ms: 2.007640526442233
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/marl1/dense_logs/logs_00
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.10486245155334473
    StateBufferConnector_ms: 0.00966191291809082
    ViewRequirementAgentConnector_ms: 0.21333098411560059
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1388.6905908846993
  episode_reward_mean: 1333.3335273490925
  episode_reward_min: 1277.9764638134855
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1277.9764638134855
    - 1388.6905908846993
    policy_a_reward: [193.94824480184099, 260.99155133274746, 260.19997026473175,
      188.06384748444194, 271.43661688393826, 183.78559803099, 175.8951445644297,
      156.74179448556976, 248.04533801516482, 203.90351458198933]
    policy_p_reward:
    - 103.33623304578389
    - 420.31920120655417
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 271.43661688393826
    p: 420.31920120655417
  policy_reward_mean:
    a: 214.3011620445844
    p: 261.82771712616903
  policy_reward_min:
    a: 156.74179448556976
    p: 103.33623304578389
  sampler_perf:
    mean_action_processing_ms: 0.5660309539093719
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.503541893058724
    mean_inference_ms: 13.169807392162282
    mean_raw_obs_processing_ms: 2.1690538713148424
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.10486245155334473
      StateBufferConnector_ms: 0.00966191291809082
      ViewRequirementAgentConnector_ms: 0.21333098411560059
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1388.6905908846993
    episode_reward_mean: 1333.3335273490925
    episode_reward_min: 1277.9764638134855
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1277.9764638134855
      - 1388.6905908846993
      policy_a_reward: [193.94824480184099, 260.99155133274746, 260.19997026473175,
        188.06384748444194, 271.43661688393826, 183.78559803099, 175.8951445644297,
        156.74179448556976, 248.04533801516482, 203.90351458198933]
      policy_p_reward:
      - 103.33623304578389
      - 420.31920120655417
    num_faulty_episodes: 0
    policy_reward_max:
      a: 271.43661688393826
      p: 420.31920120655417
    policy_reward_mean:
      a: 214.3011620445844
      p: 261.82771712616903
    policy_reward_min:
      a: 156.74179448556976
      p: 103.33623304578389
    sampler_perf:
      mean_action_processing_ms: 0.5660309539093719
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.503541893058724
      mean_inference_ms: 13.169807392162282
      mean_raw_obs_processing_ms: 2.1690538713148424
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/marl1/dense_logs/logs_01
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.12254118919372559
    StateBufferConnector_ms: 0.008749961853027344
    ViewRequirementAgentConnector_ms: 0.2425551414489746
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1528.52844975644
  episode_reward_mean: 1232.8940519003718
  episode_reward_min: 937.2596540443035
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 937.2596540443035
    - 1528.52844975644
    policy_a_reward: [163.34500344434613, 115.72833755471781, 163.25084184943267,
      122.4046293795355, 78.52289397249, 166.24511827919895, 263.20122711272745, 229.79454981793228,
      193.10902634523563, 245.44367245002422]
    policy_p_reward:
    - 294.00794784377354
    - 430.73485575131235
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 263.20122711272745
    p: 430.73485575131235
  policy_reward_mean:
    a: 174.10453002056406
    p: 362.37140179754294
  policy_reward_min:
    a: 78.52289397249
    p: 294.00794784377354
  sampler_perf:
    mean_action_processing_ms: 0.555470655315483
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.435542740081645
    mean_inference_ms: 13.833725952768548
    mean_raw_obs_processing_ms: 2.1067964959192245
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.12254118919372559
      StateBufferConnector_ms: 0.008749961853027344
      ViewRequirementAgentConnector_ms: 0.2425551414489746
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1528.52844975644
    episode_reward_mean: 1232.8940519003718
    episode_reward_min: 937.2596540443035
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 937.2596540443035
      - 1528.52844975644
      policy_a_reward: [163.34500344434613, 115.72833755471781, 163.25084184943267,
        122.4046293795355, 78.52289397249, 166.24511827919895, 263.20122711272745,
        229.79454981793228, 193.10902634523563, 245.44367245002422]
      policy_p_reward:
      - 294.00794784377354
      - 430.73485575131235
    num_faulty_episodes: 0
    policy_reward_max:
      a: 263.20122711272745
      p: 430.73485575131235
    policy_reward_mean:
      a: 174.10453002056406
      p: 362.37140179754294
    policy_reward_min:
      a: 78.52289397249
      p: 294.00794784377354
    sampler_perf:
      mean_action_processing_ms: 0.555470655315483
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.435542740081645
      mean_inference_ms: 13.833725952768548
      mean_raw_obs_processing_ms: 2.1067964959192245
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/marl1/dense_logs/logs_02
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.10176301002502441
    StateBufferConnector_ms: 0.0091552734375
    ViewRequirementAgentConnector_ms: 0.23740530014038086
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1003.0665597182609
  episode_reward_mean: 745.5987999747881
  episode_reward_min: 488.1310402313153
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1003.0665597182609
    - 488.1310402313153
    policy_a_reward: [221.61857431915988, 172.0040703139277, 187.02963240177593, 154.92228782287563,
      72.8918400493724, -92.05317482762332, 175.97352382803064, 193.50455523832974,
      145.48208306845814, 50.22699291878163]
    policy_p_reward:
    - 194.60015481115389
    - 14.997060005338366
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 221.61857431915988
    p: 194.60015481115389
  policy_reward_mean:
    a: 128.16003851330885
    p: 104.79860740824613
  policy_reward_min:
    a: -92.05317482762332
    p: 14.997060005338366
  sampler_perf:
    mean_action_processing_ms: 0.5473059931139777
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.315152399424372
    mean_inference_ms: 14.06438859446772
    mean_raw_obs_processing_ms: 2.0685547414509906
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.10176301002502441
      StateBufferConnector_ms: 0.0091552734375
      ViewRequirementAgentConnector_ms: 0.23740530014038086
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1003.0665597182609
    episode_reward_mean: 745.5987999747881
    episode_reward_min: 488.1310402313153
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1003.0665597182609
      - 488.1310402313153
      policy_a_reward: [221.61857431915988, 172.0040703139277, 187.02963240177593,
        154.92228782287563, 72.8918400493724, -92.05317482762332, 175.97352382803064,
        193.50455523832974, 145.48208306845814, 50.22699291878163]
      policy_p_reward:
      - 194.60015481115389
      - 14.997060005338366
    num_faulty_episodes: 0
    policy_reward_max:
      a: 221.61857431915988
      p: 194.60015481115389
    policy_reward_mean:
      a: 128.16003851330885
      p: 104.79860740824613
    policy_reward_min:
      a: -92.05317482762332
      p: 14.997060005338366
    sampler_perf:
      mean_action_processing_ms: 0.5473059931139777
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.315152399424372
      mean_inference_ms: 14.06438859446772
      mean_raw_obs_processing_ms: 2.0685547414509906
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/marl1/dense_logs/logs_03
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.09315013885498047
    StateBufferConnector_ms: 0.01163482666015625
    ViewRequirementAgentConnector_ms: 0.1954495906829834
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1095.1199917151544
  episode_reward_mean: 1019.3241292615692
  episode_reward_min: 943.528266807984
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1095.1199917151544
    - 943.528266807984
    policy_a_reward: [255.82081321816946, 139.76715282007186, 165.4386947523492, 69.69688789102845,
      180.61229696207886, 232.3812286142098, 188.36433758982625, 78.68964203031703,
      72.5720406358528, 253.24955997600188]
    policy_p_reward:
    - 283.7841460714537
    - 118.27145796177544
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 255.82081321816946
    p: 283.7841460714537
  policy_reward_mean:
    a: 163.65926544899054
    p: 201.02780201661454
  policy_reward_min:
    a: 69.69688789102845
    p: 118.27145796177544
  sampler_perf:
    mean_action_processing_ms: 0.5430244817966369
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.294218038950191
    mean_inference_ms: 14.155081585377324
    mean_raw_obs_processing_ms: 2.050101304235386
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.09315013885498047
      StateBufferConnector_ms: 0.01163482666015625
      ViewRequirementAgentConnector_ms: 0.1954495906829834
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1095.1199917151544
    episode_reward_mean: 1019.3241292615692
    episode_reward_min: 943.528266807984
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1095.1199917151544
      - 943.528266807984
      policy_a_reward: [255.82081321816946, 139.76715282007186, 165.4386947523492,
        69.69688789102845, 180.61229696207886, 232.3812286142098, 188.36433758982625,
        78.68964203031703, 72.5720406358528, 253.24955997600188]
      policy_p_reward:
      - 283.7841460714537
      - 118.27145796177544
    num_faulty_episodes: 0
    policy_reward_max:
      a: 255.82081321816946
      p: 283.7841460714537
    policy_reward_mean:
      a: 163.65926544899054
      p: 201.02780201661454
    policy_reward_min:
      a: 69.69688789102845
      p: 118.27145796177544
    sampler_perf:
      mean_action_processing_ms: 0.5430244817966369
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.294218038950191
      mean_inference_ms: 14.155081585377324
      mean_raw_obs_processing_ms: 2.050101304235386
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/marl1/dense_logs/logs_04
Completing! Saving final snapshot...


Final snapshot saved! All done.
seed (final): 63763000
Not restoring trainer...
Restoring agents weights...
Restoring planner weights...
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.10138154029846191
    StateBufferConnector_ms: 0.010645389556884766
    ViewRequirementAgentConnector_ms: 0.2351701259613037
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 943.7759692537614
  episode_reward_mean: 874.4218320698553
  episode_reward_min: 805.0676948859493
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 805.0676948859493
    - 943.7759692537614
    policy_a_reward: [229.63385555308125, 191.52315049270328, 171.99971270454694,
      185.49760973409644, 45.02301801185352, 152.8732348373093, 181.68895723396417,
      166.24294136515638, 232.08037734573216, 225.90127289492287]
    policy_p_reward:
    - -18.609651610326672
    - -15.010814423319328
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 232.08037734573216
    p: -15.010814423319328
  policy_reward_mean:
    a: 178.2464130173366
    p: -16.810233016823
  policy_reward_min:
    a: 45.02301801185352
    p: -18.609651610326672
  sampler_perf:
    mean_action_processing_ms: 0.5421100738281737
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 7.813631655451304
    mean_inference_ms: 12.329737345377604
    mean_raw_obs_processing_ms: 2.5682154291880104
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.10138154029846191
      StateBufferConnector_ms: 0.010645389556884766
      ViewRequirementAgentConnector_ms: 0.2351701259613037
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 943.7759692537614
    episode_reward_mean: 874.4218320698553
    episode_reward_min: 805.0676948859493
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 805.0676948859493
      - 943.7759692537614
      policy_a_reward: [229.63385555308125, 191.52315049270328, 171.99971270454694,
        185.49760973409644, 45.02301801185352, 152.8732348373093, 181.68895723396417,
        166.24294136515638, 232.08037734573216, 225.90127289492287]
      policy_p_reward:
      - -18.609651610326672
      - -15.010814423319328
    num_faulty_episodes: 0
    policy_reward_max:
      a: 232.08037734573216
      p: -15.010814423319328
    policy_reward_mean:
      a: 178.2464130173366
      p: -16.810233016823
    policy_reward_min:
      a: 45.02301801185352
      p: -18.609651610326672
    sampler_perf:
      mean_action_processing_ms: 0.5421100738281737
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 7.813631655451304
      mean_inference_ms: 12.329737345377604
      mean_raw_obs_processing_ms: 2.5682154291880104
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/marl1/dense_logs/logs_00
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.11661648750305176
    StateBufferConnector_ms: 0.009578466415405273
    ViewRequirementAgentConnector_ms: 0.23618340492248535
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1136.79191122676
  episode_reward_mean: 982.5666385679484
  episode_reward_min: 828.3413659091368
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 828.3413659091368
    - 1136.79191122676
    policy_a_reward: [143.1388921749487, 212.5884897340136, 54.88366079673814, 128.03482102555586,
      173.99265457269226, 81.4487533680055, 242.72530844051693, 260.39363721853806,
      233.90152582258233, 252.21533602661924]
    policy_p_reward:
    - 115.70284760518595
    - 66.10735035050315
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 260.39363721853806
    p: 115.70284760518595
  policy_reward_mean:
    a: 178.33230791802106
    p: 90.90509897784455
  policy_reward_min:
    a: 54.88366079673814
    p: 66.10735035050315
  sampler_perf:
    mean_action_processing_ms: 0.5739423540326861
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 6.740375951334433
    mean_inference_ms: 13.728814167933507
    mean_raw_obs_processing_ms: 2.4994813002549208
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.11661648750305176
      StateBufferConnector_ms: 0.009578466415405273
      ViewRequirementAgentConnector_ms: 0.23618340492248535
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1136.79191122676
    episode_reward_mean: 982.5666385679484
    episode_reward_min: 828.3413659091368
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 828.3413659091368
      - 1136.79191122676
      policy_a_reward: [143.1388921749487, 212.5884897340136, 54.88366079673814, 128.03482102555586,
        173.99265457269226, 81.4487533680055, 242.72530844051693, 260.39363721853806,
        233.90152582258233, 252.21533602661924]
      policy_p_reward:
      - 115.70284760518595
      - 66.10735035050315
    num_faulty_episodes: 0
    policy_reward_max:
      a: 260.39363721853806
      p: 115.70284760518595
    policy_reward_mean:
      a: 178.33230791802106
      p: 90.90509897784455
    policy_reward_min:
      a: 54.88366079673814
      p: 66.10735035050315
    sampler_perf:
      mean_action_processing_ms: 0.5739423540326861
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 6.740375951334433
      mean_inference_ms: 13.728814167933507
      mean_raw_obs_processing_ms: 2.4994813002549208
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/marl1/dense_logs/logs_01
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.10364055633544922
    StateBufferConnector_ms: 0.009441375732421875
    ViewRequirementAgentConnector_ms: 0.22034049034118652
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1155.61276333493
  episode_reward_mean: 1107.1571397787047
  episode_reward_min: 1058.7015162224793
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1058.7015162224793
    - 1155.61276333493
    policy_a_reward: [200.53751255127574, 262.4705656807168, 162.02327683598088, 241.96762983748798,
      201.84243548752636, 263.3019585725711, 282.21146470840625, 124.77035895412662,
      218.8343326538952, 229.2582665001899]
    policy_p_reward:
    - -10.139904170514644
    - 37.23638194573842
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 282.21146470840625
    p: 37.23638194573842
  policy_reward_mean:
    a: 218.7217801782177
    p: 13.54823888761189
  policy_reward_min:
    a: 124.77035895412662
    p: -10.139904170514644
  sampler_perf:
    mean_action_processing_ms: 0.5723573937565386
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 6.116987624857761
    mean_inference_ms: 14.021329447716415
    mean_raw_obs_processing_ms: 2.4120288241473458
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.10364055633544922
      StateBufferConnector_ms: 0.009441375732421875
      ViewRequirementAgentConnector_ms: 0.22034049034118652
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1155.61276333493
    episode_reward_mean: 1107.1571397787047
    episode_reward_min: 1058.7015162224793
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1058.7015162224793
      - 1155.61276333493
      policy_a_reward: [200.53751255127574, 262.4705656807168, 162.02327683598088,
        241.96762983748798, 201.84243548752636, 263.3019585725711, 282.21146470840625,
        124.77035895412662, 218.8343326538952, 229.2582665001899]
      policy_p_reward:
      - -10.139904170514644
      - 37.23638194573842
    num_faulty_episodes: 0
    policy_reward_max:
      a: 282.21146470840625
      p: 37.23638194573842
    policy_reward_mean:
      a: 218.7217801782177
      p: 13.54823888761189
    policy_reward_min:
      a: 124.77035895412662
      p: -10.139904170514644
    sampler_perf:
      mean_action_processing_ms: 0.5723573937565386
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 6.116987624857761
      mean_inference_ms: 14.021329447716415
      mean_raw_obs_processing_ms: 2.4120288241473458
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/marl1/dense_logs/logs_02
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.10978579521179199
    StateBufferConnector_ms: 0.010442733764648438
    ViewRequirementAgentConnector_ms: 0.2258002758026123
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1093.345370083265
  episode_reward_mean: 1076.6734260978897
  episode_reward_min: 1060.0014821125144
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1093.345370083265
    - 1060.0014821125144
    policy_a_reward: [237.32773128176763, 103.84078757848152, 241.92045754752027,
      299.405071305196, 147.59306360386316, 217.87513873969613, 124.0553609434334,
      219.33772421400553, 297.36103852602554, 194.30476460809177]
    policy_p_reward:
    - 63.258258766438786
    - 7.067455081262146
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 299.405071305196
    p: 63.258258766438786
  policy_reward_mean:
    a: 208.3021138348081
    p: 35.16285692385047
  policy_reward_min:
    a: 103.84078757848152
    p: 7.067455081262146
  sampler_perf:
    mean_action_processing_ms: 0.5762183862826278
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 5.781098641734431
    mean_inference_ms: 14.207990809359114
    mean_raw_obs_processing_ms: 2.3936402017268343
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.10978579521179199
      StateBufferConnector_ms: 0.010442733764648438
      ViewRequirementAgentConnector_ms: 0.2258002758026123
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1093.345370083265
    episode_reward_mean: 1076.6734260978897
    episode_reward_min: 1060.0014821125144
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1093.345370083265
      - 1060.0014821125144
      policy_a_reward: [237.32773128176763, 103.84078757848152, 241.92045754752027,
        299.405071305196, 147.59306360386316, 217.87513873969613, 124.0553609434334,
        219.33772421400553, 297.36103852602554, 194.30476460809177]
      policy_p_reward:
      - 63.258258766438786
      - 7.067455081262146
    num_faulty_episodes: 0
    policy_reward_max:
      a: 299.405071305196
      p: 63.258258766438786
    policy_reward_mean:
      a: 208.3021138348081
      p: 35.16285692385047
    policy_reward_min:
      a: 103.84078757848152
      p: 7.067455081262146
    sampler_perf:
      mean_action_processing_ms: 0.5762183862826278
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 5.781098641734431
      mean_inference_ms: 14.207990809359114
      mean_raw_obs_processing_ms: 2.3936402017268343
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/marl1/dense_logs/logs_03
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.09645819664001465
    StateBufferConnector_ms: 0.009703636169433594
    ViewRequirementAgentConnector_ms: 0.2158820629119873
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 989.1850372841162
  episode_reward_mean: 930.9391514619706
  episode_reward_min: 872.6932656398249
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 989.1850372841162
    - 872.6932656398249
    policy_a_reward: [136.56421944105662, 163.45265172725024, 156.5898384439903, 151.89878069281374,
      231.18713497859176, 170.3641053301542, 124.92566217203175, 212.00516001915094,
      165.06559535833617, 179.21523367278064]
    policy_p_reward:
    - 149.49241200041334
    - 21.117509087369868
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 231.18713497859176
    p: 149.49241200041334
  policy_reward_mean:
    a: 169.12683818361563
    p: 85.30496054389161
  policy_reward_min:
    a: 124.92566217203175
    p: 21.117509087369868
  sampler_perf:
    mean_action_processing_ms: 0.5759691438976168
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 5.554225910953978
    mean_inference_ms: 14.247108678348729
    mean_raw_obs_processing_ms: 2.358639540552187
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.09645819664001465
      StateBufferConnector_ms: 0.009703636169433594
      ViewRequirementAgentConnector_ms: 0.2158820629119873
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 989.1850372841162
    episode_reward_mean: 930.9391514619706
    episode_reward_min: 872.6932656398249
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 989.1850372841162
      - 872.6932656398249
      policy_a_reward: [136.56421944105662, 163.45265172725024, 156.5898384439903,
        151.89878069281374, 231.18713497859176, 170.3641053301542, 124.92566217203175,
        212.00516001915094, 165.06559535833617, 179.21523367278064]
      policy_p_reward:
      - 149.49241200041334
      - 21.117509087369868
    num_faulty_episodes: 0
    policy_reward_max:
      a: 231.18713497859176
      p: 149.49241200041334
    policy_reward_mean:
      a: 169.12683818361563
      p: 85.30496054389161
    policy_reward_min:
      a: 124.92566217203175
      p: 21.117509087369868
    sampler_perf:
      mean_action_processing_ms: 0.5759691438976168
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 5.554225910953978
      mean_inference_ms: 14.247108678348729
      mean_raw_obs_processing_ms: 2.358639540552187
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/marl1/dense_logs/logs_04
Completing! Saving final snapshot...


Final snapshot saved! All done.
seed (final): 29184000
Not restoring trainer...
Restoring agents weights...
Restoring planner weights...
seed (final): 29220000
Not restoring trainer...
Restoring agents weights...
Restoring planner weights...
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.08434653282165527
    StateBufferConnector_ms: 0.009274482727050781
    ViewRequirementAgentConnector_ms: 0.20600557327270508
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1134.890738805829
  episode_reward_mean: 1091.118701110844
  episode_reward_min: 1047.3466634158594
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1134.890738805829
    - 1047.3466634158594
    policy_a_reward: [179.89535589580382, 268.71002994913607, 223.45087930740323,
      321.0204716406485, 161.8133496174671, 179.38606656831445, 270.49176003458126,
      225.4630722733469, 167.6739126736776, 224.31723731429528]
    policy_p_reward:
    - -19.999347604629193
    - -19.985385448355483
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 321.0204716406485
    p: -19.985385448355483
  policy_reward_mean:
    a: 222.22221352746743
    p: -19.99236652649234
  policy_reward_min:
    a: 161.8133496174671
    p: -19.999347604629193
  sampler_perf:
    mean_action_processing_ms: 0.5086691317682018
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.26470019860182
    mean_inference_ms: 11.020935938029945
    mean_raw_obs_processing_ms: 2.1381102160303413
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.08434653282165527
      StateBufferConnector_ms: 0.009274482727050781
      ViewRequirementAgentConnector_ms: 0.20600557327270508
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1134.890738805829
    episode_reward_mean: 1091.118701110844
    episode_reward_min: 1047.3466634158594
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1134.890738805829
      - 1047.3466634158594
      policy_a_reward: [179.89535589580382, 268.71002994913607, 223.45087930740323,
        321.0204716406485, 161.8133496174671, 179.38606656831445, 270.49176003458126,
        225.4630722733469, 167.6739126736776, 224.31723731429528]
      policy_p_reward:
      - -19.999347604629193
      - -19.985385448355483
    num_faulty_episodes: 0
    policy_reward_max:
      a: 321.0204716406485
      p: -19.985385448355483
    policy_reward_mean:
      a: 222.22221352746743
      p: -19.99236652649234
    policy_reward_min:
      a: 161.8133496174671
      p: -19.999347604629193
    sampler_perf:
      mean_action_processing_ms: 0.5086691317682018
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.26470019860182
      mean_inference_ms: 11.020935938029945
      mean_raw_obs_processing_ms: 2.1381102160303413
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/marl1/dense_logs/logs_00
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.10279417037963867
    StateBufferConnector_ms: 0.00947117805480957
    ViewRequirementAgentConnector_ms: 0.20126104354858398
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1006.7979921569981
  episode_reward_mean: 991.1900679284156
  episode_reward_min: 975.5821436998332
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 975.5821436998332
    - 1006.7979921569981
    policy_a_reward: [256.27296272879585, 111.40164889009507, 279.4764930304009, 123.72339514688358,
      224.70038765975684, 162.0086140071067, 270.3517530563751, 194.42636495511096,
      212.65163846250636, 187.35959067185988]
    policy_p_reward:
    - -19.992743756098463
    - -19.999968995959055
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 279.4764930304009
    p: -19.992743756098463
  policy_reward_mean:
    a: 202.23728486088913
    p: -19.996356376028757
  policy_reward_min:
    a: 111.40164889009507
    p: -19.999968995959055
  sampler_perf:
    mean_action_processing_ms: 0.5241738451825274
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.274474038229837
    mean_inference_ms: 12.320745479572308
    mean_raw_obs_processing_ms: 2.068069193151209
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.10279417037963867
      StateBufferConnector_ms: 0.00947117805480957
      ViewRequirementAgentConnector_ms: 0.20126104354858398
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1006.7979921569981
    episode_reward_mean: 991.1900679284156
    episode_reward_min: 975.5821436998332
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 975.5821436998332
      - 1006.7979921569981
      policy_a_reward: [256.27296272879585, 111.40164889009507, 279.4764930304009,
        123.72339514688358, 224.70038765975684, 162.0086140071067, 270.3517530563751,
        194.42636495511096, 212.65163846250636, 187.35959067185988]
      policy_p_reward:
      - -19.992743756098463
      - -19.999968995959055
    num_faulty_episodes: 0
    policy_reward_max:
      a: 279.4764930304009
      p: -19.992743756098463
    policy_reward_mean:
      a: 202.23728486088913
      p: -19.996356376028757
    policy_reward_min:
      a: 111.40164889009507
      p: -19.999968995959055
    sampler_perf:
      mean_action_processing_ms: 0.5241738451825274
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.274474038229837
      mean_inference_ms: 12.320745479572308
      mean_raw_obs_processing_ms: 2.068069193151209
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/marl1/dense_logs/logs_01
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0976264476776123
    StateBufferConnector_ms: 0.00903010368347168
    ViewRequirementAgentConnector_ms: 0.21790862083435059
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1064.1234618153005
  episode_reward_mean: 1042.137319885777
  episode_reward_min: 1020.1511779562535
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1064.1234618153005
    - 1020.1511779562535
    policy_a_reward: [151.06265435524637, 168.5973758172762, 287.35063156536864, 269.97495924810136,
      207.13408115625487, 206.1692312433753, 163.09264883077645, 214.36905919856554,
      234.17708211035193, 222.3428150848273]
    policy_p_reward:
    - -19.996240326946058
    - -19.999658511644157
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 287.35063156536864
    p: -19.996240326946058
  policy_reward_mean:
    a: 212.42705386101437
    p: -19.997949419295107
  policy_reward_min:
    a: 151.06265435524637
    p: -19.999658511644157
  sampler_perf:
    mean_action_processing_ms: 0.5298510620706801
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.31065826238115
    mean_inference_ms: 12.727813511034554
    mean_raw_obs_processing_ms: 2.045855531686151
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.0976264476776123
      StateBufferConnector_ms: 0.00903010368347168
      ViewRequirementAgentConnector_ms: 0.21790862083435059
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1064.1234618153005
    episode_reward_mean: 1042.137319885777
    episode_reward_min: 1020.1511779562535
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1064.1234618153005
      - 1020.1511779562535
      policy_a_reward: [151.06265435524637, 168.5973758172762, 287.35063156536864,
        269.97495924810136, 207.13408115625487, 206.1692312433753, 163.09264883077645,
        214.36905919856554, 234.17708211035193, 222.3428150848273]
      policy_p_reward:
      - -19.996240326946058
      - -19.999658511644157
    num_faulty_episodes: 0
    policy_reward_max:
      a: 287.35063156536864
      p: -19.996240326946058
    policy_reward_mean:
      a: 212.42705386101437
      p: -19.997949419295107
    policy_reward_min:
      a: 151.06265435524637
      p: -19.999658511644157
    sampler_perf:
      mean_action_processing_ms: 0.5298510620706801
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.31065826238115
      mean_inference_ms: 12.727813511034554
      mean_raw_obs_processing_ms: 2.045855531686151
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/marl1/dense_logs/logs_02
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.10557174682617188
    StateBufferConnector_ms: 0.009423494338989258
    ViewRequirementAgentConnector_ms: 0.20971894264221191
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1094.5785324660621
  episode_reward_mean: -3991.490920789313
  episode_reward_min: -9077.560374044688
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - -9077.560374044688
    - 1094.5785324660621
    policy_a_reward: [239.02940907116212, 191.81782173083744, 268.3527499040021, 137.6984972321426,
      105.28678752783958, 287.31047675734425, 254.87088898306115, 164.6162199776681,
      226.9353276543067, 180.84538954808323]
    policy_p_reward:
    - -10019.74563951067
    - -19.999770454401006
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 287.31047675734425
    p: -19.999770454401006
  policy_reward_mean:
    a: 205.67635683864472
    p: -5019.872704982535
  policy_reward_min:
    a: 105.28678752783958
    p: -10019.74563951067
  sampler_perf:
    mean_action_processing_ms: 0.5326688081130333
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.226116941071701
    mean_inference_ms: 12.862900386507185
    mean_raw_obs_processing_ms: 2.0500181437372746
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.10557174682617188
      StateBufferConnector_ms: 0.009423494338989258
      ViewRequirementAgentConnector_ms: 0.20971894264221191
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1094.5785324660621
    episode_reward_mean: -3991.490920789313
    episode_reward_min: -9077.560374044688
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - -9077.560374044688
      - 1094.5785324660621
      policy_a_reward: [239.02940907116212, 191.81782173083744, 268.3527499040021,
        137.6984972321426, 105.28678752783958, 287.31047675734425, 254.87088898306115,
        164.6162199776681, 226.9353276543067, 180.84538954808323]
      policy_p_reward:
      - -10019.74563951067
      - -19.999770454401006
    num_faulty_episodes: 0
    policy_reward_max:
      a: 287.31047675734425
      p: -19.999770454401006
    policy_reward_mean:
      a: 205.67635683864472
      p: -5019.872704982535
    policy_reward_min:
      a: 105.28678752783958
      p: -10019.74563951067
    sampler_perf:
      mean_action_processing_ms: 0.5326688081130333
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.226116941071701
      mean_inference_ms: 12.862900386507185
      mean_raw_obs_processing_ms: 2.0500181437372746
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/marl1/dense_logs/logs_03
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.09064674377441406
    StateBufferConnector_ms: 0.009256601333618164
    ViewRequirementAgentConnector_ms: 0.21187663078308105
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 878.7539483898821
  episode_reward_mean: 846.8922507977584
  episode_reward_min: 815.0305532056348
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 878.7539483898821
    - 815.0305532056348
    policy_a_reward: [169.84416760576954, 191.15621077577552, 232.74292474760614,
      198.3729095713172, 106.56236797989406, 165.42936307763918, 127.79898254754178,
      160.3583995214745, 235.79584731431882, 145.10636284959472]
    policy_p_reward:
    - -19.924632290485203
    - -19.458402104935143
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 235.79584731431882
    p: -19.458402104935143
  policy_reward_mean:
    a: 173.31675359909315
    p: -19.691517197710173
  policy_reward_min:
    a: 106.56236797989406
    p: -19.924632290485203
  sampler_perf:
    mean_action_processing_ms: 0.5352954681469697
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.162954645412343
    mean_inference_ms: 13.03734907099172
    mean_raw_obs_processing_ms: 2.054274916315212
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.09064674377441406
      StateBufferConnector_ms: 0.009256601333618164
      ViewRequirementAgentConnector_ms: 0.21187663078308105
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 878.7539483898821
    episode_reward_mean: 846.8922507977584
    episode_reward_min: 815.0305532056348
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 878.7539483898821
      - 815.0305532056348
      policy_a_reward: [169.84416760576954, 191.15621077577552, 232.74292474760614,
        198.3729095713172, 106.56236797989406, 165.42936307763918, 127.79898254754178,
        160.3583995214745, 235.79584731431882, 145.10636284959472]
      policy_p_reward:
      - -19.924632290485203
      - -19.458402104935143
    num_faulty_episodes: 0
    policy_reward_max:
      a: 235.79584731431882
      p: -19.458402104935143
    policy_reward_mean:
      a: 173.31675359909315
      p: -19.691517197710173
    policy_reward_min:
      a: 106.56236797989406
      p: -19.924632290485203
    sampler_perf:
      mean_action_processing_ms: 0.5352954681469697
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.162954645412343
      mean_inference_ms: 13.03734907099172
      mean_raw_obs_processing_ms: 2.054274916315212
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/marl1/dense_logs/logs_04
Completing! Saving final snapshot...


Final snapshot saved! All done.
seed (final): 32840000
Not restoring trainer...
Restoring agents weights...
Restoring planner weights...
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0840306282043457
    StateBufferConnector_ms: 0.009387731552124023
    ViewRequirementAgentConnector_ms: 0.2075493335723877
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 2466.3800189920016
  episode_reward_mean: 2072.297536996628
  episode_reward_min: 1678.2150550012548
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1678.2150550012548
    - 2466.3800189920016
    policy_a_reward: [55.57902259432492, 362.49317862591295, 346.52955415354995, 202.861046329092,
      266.82850339248864, 355.4998278672694, 305.0062160703565, 365.27942238783385,
      310.48863698013264, 379.2172941248667]
    policy_p_reward:
    - 443.9237499058845
    - 750.8886215615295
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 379.2172941248667
    p: 750.8886215615295
  policy_reward_mean:
    a: 294.9782702525828
    p: 597.4061857337069
  policy_reward_min:
    a: 55.57902259432492
    p: 443.9237499058845
  sampler_perf:
    mean_action_processing_ms: 0.5727537615808422
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.2333997890145
    mean_inference_ms: 11.575430453180552
    mean_raw_obs_processing_ms: 2.1624336699525752
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.0840306282043457
      StateBufferConnector_ms: 0.009387731552124023
      ViewRequirementAgentConnector_ms: 0.2075493335723877
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 2466.3800189920016
    episode_reward_mean: 2072.297536996628
    episode_reward_min: 1678.2150550012548
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1678.2150550012548
      - 2466.3800189920016
      policy_a_reward: [55.57902259432492, 362.49317862591295, 346.52955415354995,
        202.861046329092, 266.82850339248864, 355.4998278672694, 305.0062160703565,
        365.27942238783385, 310.48863698013264, 379.2172941248667]
      policy_p_reward:
      - 443.9237499058845
      - 750.8886215615295
    num_faulty_episodes: 0
    policy_reward_max:
      a: 379.2172941248667
      p: 750.8886215615295
    policy_reward_mean:
      a: 294.9782702525828
      p: 597.4061857337069
    policy_reward_min:
      a: 55.57902259432492
      p: 443.9237499058845
    sampler_perf:
      mean_action_processing_ms: 0.5727537615808422
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.2333997890145
      mean_inference_ms: 11.575430453180552
      mean_raw_obs_processing_ms: 2.1624336699525752
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/1/marl/dense_logs/logs_00
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.09580850601196289
    StateBufferConnector_ms: 0.008922815322875977
    ViewRequirementAgentConnector_ms: 0.20530223846435547
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 2127.6794210091425
  episode_reward_mean: 1947.1751468883685
  episode_reward_min: 1766.6708727675943
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 2127.6794210091425
    - 1766.6708727675943
    policy_a_reward: [338.2196244549981, 250.63682069557888, 137.87508057586138, 389.75352327636637,
      275.77663466644924, 187.14860888543555, 330.74212146470603, 287.3545593780184,
      174.17406145511708, 289.55534402411314]
    policy_p_reward:
    - 735.4177373398929
    - 497.69617756020307
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 389.75352327636637
    p: 735.4177373398929
  policy_reward_mean:
    a: 266.1236378876645
    p: 616.556957450048
  policy_reward_min:
    a: 137.87508057586138
    p: 497.69617756020307
  sampler_perf:
    mean_action_processing_ms: 0.5727304921640859
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.331753089592293
    mean_inference_ms: 12.614244943136697
    mean_raw_obs_processing_ms: 2.1176169087717702
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.09580850601196289
      StateBufferConnector_ms: 0.008922815322875977
      ViewRequirementAgentConnector_ms: 0.20530223846435547
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 2127.6794210091425
    episode_reward_mean: 1947.1751468883685
    episode_reward_min: 1766.6708727675943
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 2127.6794210091425
      - 1766.6708727675943
      policy_a_reward: [338.2196244549981, 250.63682069557888, 137.87508057586138,
        389.75352327636637, 275.77663466644924, 187.14860888543555, 330.74212146470603,
        287.3545593780184, 174.17406145511708, 289.55534402411314]
      policy_p_reward:
      - 735.4177373398929
      - 497.69617756020307
    num_faulty_episodes: 0
    policy_reward_max:
      a: 389.75352327636637
      p: 735.4177373398929
    policy_reward_mean:
      a: 266.1236378876645
      p: 616.556957450048
    policy_reward_min:
      a: 137.87508057586138
      p: 497.69617756020307
    sampler_perf:
      mean_action_processing_ms: 0.5727304921640859
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.331753089592293
      mean_inference_ms: 12.614244943136697
      mean_raw_obs_processing_ms: 2.1176169087717702
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/1/marl/dense_logs/logs_01
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.10240674018859863
    StateBufferConnector_ms: 0.010269880294799805
    ViewRequirementAgentConnector_ms: 0.2086639404296875
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1776.8556004855946
  episode_reward_mean: 1739.983401044498
  episode_reward_min: 1703.1112016034012
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1703.1112016034012
    - 1776.8556004855946
    policy_a_reward: [262.98922295557634, 256.8043571384434, 302.03058410684173, 177.15484832421987,
      251.86576328303332, 156.03888520511248, 240.13750647163917, 302.1168705231823,
      202.7616558830257, 299.56291437983236]
    policy_p_reward:
    - 452.26642579529164
    - 576.2377680228055
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 302.1168705231823
    p: 576.2377680228055
  policy_reward_mean:
    a: 245.14626082709066
    p: 514.2520969090485
  policy_reward_min:
    a: 156.03888520511248
    p: 452.26642579529164
  sampler_perf:
    mean_action_processing_ms: 0.5634142350229242
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.381494947785461
    mean_inference_ms: 12.756467262639116
    mean_raw_obs_processing_ms: 2.0959648904921133
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.10240674018859863
      StateBufferConnector_ms: 0.010269880294799805
      ViewRequirementAgentConnector_ms: 0.2086639404296875
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1776.8556004855946
    episode_reward_mean: 1739.983401044498
    episode_reward_min: 1703.1112016034012
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1703.1112016034012
      - 1776.8556004855946
      policy_a_reward: [262.98922295557634, 256.8043571384434, 302.03058410684173,
        177.15484832421987, 251.86576328303332, 156.03888520511248, 240.13750647163917,
        302.1168705231823, 202.7616558830257, 299.56291437983236]
      policy_p_reward:
      - 452.26642579529164
      - 576.2377680228055
    num_faulty_episodes: 0
    policy_reward_max:
      a: 302.1168705231823
      p: 576.2377680228055
    policy_reward_mean:
      a: 245.14626082709066
      p: 514.2520969090485
    policy_reward_min:
      a: 156.03888520511248
      p: 452.26642579529164
    sampler_perf:
      mean_action_processing_ms: 0.5634142350229242
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.381494947785461
      mean_inference_ms: 12.756467262639116
      mean_raw_obs_processing_ms: 2.0959648904921133
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/1/marl/dense_logs/logs_02
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.10045766830444336
    StateBufferConnector_ms: 0.009262561798095703
    ViewRequirementAgentConnector_ms: 0.21170973777770996
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1895.7571213286574
  episode_reward_mean: 1673.4785705499041
  episode_reward_min: 1451.200019771151
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1895.7571213286574
    - 1451.200019771151
    policy_a_reward: [215.39961173192052, 261.7344388859448, 290.76717526463557, 345.34593519339336,
      218.07127961912482, 242.82102777062013, 251.7058793568267, 114.37621982460492,
      138.92573319882104, 292.27568040091074]
    policy_p_reward:
    - 564.4386806336374
    - 411.09547921937167
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 345.34593519339336
    p: 564.4386806336374
  policy_reward_mean:
    a: 237.14229812468028
    p: 487.76707992650455
  policy_reward_min:
    a: 114.37621982460492
    p: 411.09547921937167
  sampler_perf:
    mean_action_processing_ms: 0.560055489185034
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.3023124687198635
    mean_inference_ms: 12.943345269580176
    mean_raw_obs_processing_ms: 2.092481314808294
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.10045766830444336
      StateBufferConnector_ms: 0.009262561798095703
      ViewRequirementAgentConnector_ms: 0.21170973777770996
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1895.7571213286574
    episode_reward_mean: 1673.4785705499041
    episode_reward_min: 1451.200019771151
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1895.7571213286574
      - 1451.200019771151
      policy_a_reward: [215.39961173192052, 261.7344388859448, 290.76717526463557,
        345.34593519339336, 218.07127961912482, 242.82102777062013, 251.7058793568267,
        114.37621982460492, 138.92573319882104, 292.27568040091074]
      policy_p_reward:
      - 564.4386806336374
      - 411.09547921937167
    num_faulty_episodes: 0
    policy_reward_max:
      a: 345.34593519339336
      p: 564.4386806336374
    policy_reward_mean:
      a: 237.14229812468028
      p: 487.76707992650455
    policy_reward_min:
      a: 114.37621982460492
      p: 411.09547921937167
    sampler_perf:
      mean_action_processing_ms: 0.560055489185034
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.3023124687198635
      mean_inference_ms: 12.943345269580176
      mean_raw_obs_processing_ms: 2.092481314808294
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/1/marl/dense_logs/logs_03
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.10578036308288574
    StateBufferConnector_ms: 0.010311603546142578
    ViewRequirementAgentConnector_ms: 0.20720362663269043
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 1711.9428615602212
  episode_reward_mean: 1665.4997687051223
  episode_reward_min: 1619.0566758500233
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 1619.0566758500233
    - 1711.9428615602212
    policy_a_reward: [325.49671830317027, 222.88333873282306, 243.52532067741353,
      157.96207159341137, 187.20507356749945, 113.6341161481295, 317.38594462707874,
      227.88684605141506, 363.81602415696216, 158.73079565689574]
    policy_p_reward:
    - 481.98415297570614
    - 530.4891349197386
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 363.81602415696216
    p: 530.4891349197386
  policy_reward_mean:
    a: 231.8526249514799
    p: 506.23664394772237
  policy_reward_min:
    a: 113.6341161481295
    p: 481.98415297570614
  sampler_perf:
    mean_action_processing_ms: 0.5636264781196897
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.264451846367166
    mean_inference_ms: 13.500307808395197
    mean_raw_obs_processing_ms: 2.0927385729057986
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.10578036308288574
      StateBufferConnector_ms: 0.010311603546142578
      ViewRequirementAgentConnector_ms: 0.20720362663269043
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 1711.9428615602212
    episode_reward_mean: 1665.4997687051223
    episode_reward_min: 1619.0566758500233
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 1619.0566758500233
      - 1711.9428615602212
      policy_a_reward: [325.49671830317027, 222.88333873282306, 243.52532067741353,
        157.96207159341137, 187.20507356749945, 113.6341161481295, 317.38594462707874,
        227.88684605141506, 363.81602415696216, 158.73079565689574]
      policy_p_reward:
      - 481.98415297570614
      - 530.4891349197386
    num_faulty_episodes: 0
    policy_reward_max:
      a: 363.81602415696216
      p: 530.4891349197386
    policy_reward_mean:
      a: 231.8526249514799
      p: 506.23664394772237
    policy_reward_min:
      a: 113.6341161481295
      p: 481.98415297570614
    sampler_perf:
      mean_action_processing_ms: 0.5636264781196897
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.264451846367166
      mean_inference_ms: 13.500307808395197
      mean_raw_obs_processing_ms: 2.0927385729057986
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/1/marl/dense_logs/logs_04
Completing! Saving final snapshot...


Final snapshot saved! All done.
