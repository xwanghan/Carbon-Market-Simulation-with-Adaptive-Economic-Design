seed (final): 34103000
seed (final): 34347000
seed (final): 34409000
Not restoring trainer...
Restoring agents weights...
Starting with fresh planner weights.
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.13239383697509766
    StateBufferConnector_ms: 0.010782480239868164
    ViewRequirementAgentConnector_ms: 0.24695992469787598
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 737.5643201958048
  episode_reward_mean: 700.2286131807034
  episode_reward_min: 662.892906165602
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 662.892906165602
    - 737.5643201958048
    policy_a_reward: [116.45796810232103, 90.77978301220212, 61.358598389051835, 115.9378598812358,
      62.99790734163317, 68.44833188300959, 69.61617159746599, 93.04400372364168,
      148.781122456166, 121.68445172922317]
    policy_p_reward:
    - 215.36078943915277
    - 235.99023880629443
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 148.781122456166
    p: 235.99023880629443
  policy_reward_mean:
    a: 94.91061981159504
    p: 225.6755141227236
  policy_reward_min:
    a: 61.358598389051835
    p: 215.36078943915277
  sampler_perf:
    mean_action_processing_ms: 0.6859069336912113
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 5.152591450247698
    mean_inference_ms: 9.307878935884334
    mean_raw_obs_processing_ms: 2.8926802728466408
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/decreasing/dense_logs/logs_00
Completing! Saving final snapshot...


Final snapshot saved! All done.
seed (final): 21398000
Not restoring trainer...
Restoring agents weights...
Starting with fresh planner weights.
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.11036396026611328
    StateBufferConnector_ms: 0.009453296661376953
    ViewRequirementAgentConnector_ms: 0.2254188060760498
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 694.8396164810546
  episode_reward_mean: 672.269978778758
  episode_reward_min: 649.7003410764613
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 649.7003410764613
    - 694.8396164810546
    policy_a_reward: [112.17161781783959, 111.37370598646982, 64.17520817987082, 95.52198407932805,
      55.69703557379724, 70.21336511324311, 126.9124956561849, 119.18812498515844,
      87.14585114487974, 65.00959629517227]
    policy_p_reward:
    - 210.76078943915272
    - 226.3701832864127
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 126.9124956561849
    p: 226.3701832864127
  policy_reward_mean:
    a: 90.7408984831944
    p: 218.5654863627827
  policy_reward_min:
    a: 55.69703557379724
    p: 210.76078943915272
  sampler_perf:
    mean_action_processing_ms: 0.5660157003802453
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.202850326568543
    mean_inference_ms: 7.587132577648657
    mean_raw_obs_processing_ms: 2.3850723654924035
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.11036396026611328
      StateBufferConnector_ms: 0.009453296661376953
      ViewRequirementAgentConnector_ms: 0.2254188060760498
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 694.8396164810546
    episode_reward_mean: 672.269978778758
    episode_reward_min: 649.7003410764613
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 649.7003410764613
      - 694.8396164810546
      policy_a_reward: [112.17161781783959, 111.37370598646982, 64.17520817987082,
        95.52198407932805, 55.69703557379724, 70.21336511324311, 126.9124956561849,
        119.18812498515844, 87.14585114487974, 65.00959629517227]
      policy_p_reward:
      - 210.76078943915272
      - 226.3701832864127
    num_faulty_episodes: 0
    policy_reward_max:
      a: 126.9124956561849
      p: 226.3701832864127
    policy_reward_mean:
      a: 90.7408984831944
      p: 218.5654863627827
    policy_reward_min:
      a: 55.69703557379724
      p: 210.76078943915272
    sampler_perf:
      mean_action_processing_ms: 0.5660157003802453
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.202850326568543
      mean_inference_ms: 7.587132577648657
      mean_raw_obs_processing_ms: 2.3850723654924035
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/decreasing/dense_logs/logs_00
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.10863542556762695
    StateBufferConnector_ms: 0.010585784912109375
    ViewRequirementAgentConnector_ms: 0.20812749862670898
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 661.9641891714156
  episode_reward_mean: 658.3812563988811
  episode_reward_min: 654.7983236263465
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 661.9641891714156
    - 654.7983236263465
    policy_a_reward: [110.49606553817509, 62.503197534945784, 61.86339459855269, 93.40871945685703,
      117.63202260373414, 138.13800206924648, 112.25633707246459, 59.660031767339085,
      64.23891248075839, 76.81811979982292]
    policy_p_reward:
    - 216.06078943915273
    - 203.68692043671768
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 138.13800206924648
    p: 216.06078943915273
  policy_reward_mean:
    a: 89.70148029218963
    p: 209.87385493793522
  policy_reward_min:
    a: 59.660031767339085
    p: 203.68692043671768
  sampler_perf:
    mean_action_processing_ms: 0.5731463551402212
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.350734876466917
    mean_inference_ms: 8.531106697334039
    mean_raw_obs_processing_ms: 2.2690146119444523
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.10863542556762695
      StateBufferConnector_ms: 0.010585784912109375
      ViewRequirementAgentConnector_ms: 0.20812749862670898
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 661.9641891714156
    episode_reward_mean: 658.3812563988811
    episode_reward_min: 654.7983236263465
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 661.9641891714156
      - 654.7983236263465
      policy_a_reward: [110.49606553817509, 62.503197534945784, 61.86339459855269,
        93.40871945685703, 117.63202260373414, 138.13800206924648, 112.25633707246459,
        59.660031767339085, 64.23891248075839, 76.81811979982292]
      policy_p_reward:
      - 216.06078943915273
      - 203.68692043671768
    num_faulty_episodes: 0
    policy_reward_max:
      a: 138.13800206924648
      p: 216.06078943915273
    policy_reward_mean:
      a: 89.70148029218963
      p: 209.87385493793522
    policy_reward_min:
      a: 59.660031767339085
      p: 203.68692043671768
    sampler_perf:
      mean_action_processing_ms: 0.5731463551402212
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.350734876466917
      mean_inference_ms: 8.531106697334039
      mean_raw_obs_processing_ms: 2.2690146119444523
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/decreasing/dense_logs/logs_01
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.10260343551635742
    StateBufferConnector_ms: 0.009101629257202148
    ViewRequirementAgentConnector_ms: 0.20537376403808594
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 668.8925247596917
  episode_reward_mean: 663.655711095119
  episode_reward_min: 658.4188974305464
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 668.8925247596917
    - 658.4188974305464
    policy_a_reward: [110.15530227943518, 120.56591987893948, 67.55638164092969, 61.35655667154734,
      90.071736427772, 122.68307079627233, 105.69704633312382, 87.4960565511014, 67.8630129901137,
      60.193082898868646]
    policy_p_reward:
    - 219.186627861067
    - 214.48662786106698
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 122.68307079627233
    p: 219.186627861067
  policy_reward_mean:
    a: 89.36381664681035
    p: 216.83662786106697
  policy_reward_min:
    a: 60.193082898868646
    p: 214.48662786106698
  sampler_perf:
    mean_action_processing_ms: 0.5698005490744614
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.356504995611649
    mean_inference_ms: 8.823028808431097
    mean_raw_obs_processing_ms: 2.2303387770884675
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.10260343551635742
      StateBufferConnector_ms: 0.009101629257202148
      ViewRequirementAgentConnector_ms: 0.20537376403808594
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 668.8925247596917
    episode_reward_mean: 663.655711095119
    episode_reward_min: 658.4188974305464
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 668.8925247596917
      - 658.4188974305464
      policy_a_reward: [110.15530227943518, 120.56591987893948, 67.55638164092969,
        61.35655667154734, 90.071736427772, 122.68307079627233, 105.69704633312382,
        87.4960565511014, 67.8630129901137, 60.193082898868646]
      policy_p_reward:
      - 219.186627861067
      - 214.48662786106698
    num_faulty_episodes: 0
    policy_reward_max:
      a: 122.68307079627233
      p: 219.186627861067
    policy_reward_mean:
      a: 89.36381664681035
      p: 216.83662786106697
    policy_reward_min:
      a: 60.193082898868646
      p: 214.48662786106698
    sampler_perf:
      mean_action_processing_ms: 0.5698005490744614
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.356504995611649
      mean_inference_ms: 8.823028808431097
      mean_raw_obs_processing_ms: 2.2303387770884675
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/decreasing/dense_logs/logs_02
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.09836554527282715
    StateBufferConnector_ms: 0.009882450103759766
    ViewRequirementAgentConnector_ms: 0.2135157585144043
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 736.9524363138353
  episode_reward_mean: 712.9107732230143
  episode_reward_min: 688.8691101321933
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 688.8691101321933
    - 736.9524363138353
    policy_a_reward: [131.8537468342412, 113.14866911026351, 90.88514687957506, 63.408462512928615,
      66.14996844879259, 66.62428581755475, 94.5516824074983, 119.15875496557548,
      147.07360713934798, 71.11012122225077]
    policy_p_reward:
    - 223.4231163463871
    - 238.43398476160894
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 147.07360713934798
    p: 238.43398476160894
  policy_reward_mean:
    a: 96.39644453380282
    p: 230.92855055399804
  policy_reward_min:
    a: 63.408462512928615
    p: 223.4231163463871
  sampler_perf:
    mean_action_processing_ms: 0.569216195849524
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.291015050221777
    mean_inference_ms: 8.993347426285332
    mean_raw_obs_processing_ms: 2.21582414626122
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.09836554527282715
      StateBufferConnector_ms: 0.009882450103759766
      ViewRequirementAgentConnector_ms: 0.2135157585144043
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 736.9524363138353
    episode_reward_mean: 712.9107732230143
    episode_reward_min: 688.8691101321933
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 688.8691101321933
      - 736.9524363138353
      policy_a_reward: [131.8537468342412, 113.14866911026351, 90.88514687957506,
        63.408462512928615, 66.14996844879259, 66.62428581755475, 94.5516824074983,
        119.15875496557548, 147.07360713934798, 71.11012122225077]
      policy_p_reward:
      - 223.4231163463871
      - 238.43398476160894
    num_faulty_episodes: 0
    policy_reward_max:
      a: 147.07360713934798
      p: 238.43398476160894
    policy_reward_mean:
      a: 96.39644453380282
      p: 230.92855055399804
    policy_reward_min:
      a: 63.408462512928615
      p: 223.4231163463871
    sampler_perf:
      mean_action_processing_ms: 0.569216195849524
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.291015050221777
      mean_inference_ms: 8.993347426285332
      mean_raw_obs_processing_ms: 2.21582414626122
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/decreasing/dense_logs/logs_03
evaluation:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.09726285934448242
    StateBufferConnector_ms: 0.009465217590332031
    ViewRequirementAgentConnector_ms: 0.2155601978302002
  custom_metrics: {}
  episode_len_mean: 500.0
  episode_media: {}
  episode_reward_max: 685.9203888394675
  episode_reward_mean: 678.7553081727518
  episode_reward_min: 671.5902275060362
  episodes_this_iter: 2
  hist_stats:
    episode_lengths:
    - 500
    - 500
    episode_reward:
    - 685.9203888394675
    - 671.5902275060362
    policy_a_reward: [128.219838318996, 95.32434107667193, 117.2034467675782, 57.736801975233355,
      66.22202951571332, 67.22620212817166, 115.07728515631284, 59.42518731554298,
      113.99698399817368, 93.54145256145539]
    policy_p_reward:
    - 221.21393118527172
    - 222.32311634638702
  num_agent_steps_sampled_this_iter: 6000
  num_env_steps_sampled_this_iter: 1000
  num_faulty_episodes: 0
  policy_reward_max:
    a: 128.219838318996
    p: 222.32311634638702
  policy_reward_mean:
    a: 91.39735688138492
    p: 221.76852376582937
  policy_reward_min:
    a: 57.736801975233355
    p: 221.21393118527172
  sampler_perf:
    mean_action_processing_ms: 0.5693218318141493
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 4.288278546918635
    mean_inference_ms: 9.157721207934063
    mean_raw_obs_processing_ms: 2.207615622421686
  sampler_results:
    connector_metrics:
      ObsPreprocessorConnector_ms: 0.09726285934448242
      StateBufferConnector_ms: 0.009465217590332031
      ViewRequirementAgentConnector_ms: 0.2155601978302002
    custom_metrics: {}
    episode_len_mean: 500.0
    episode_media: {}
    episode_reward_max: 685.9203888394675
    episode_reward_mean: 678.7553081727518
    episode_reward_min: 671.5902275060362
    episodes_this_iter: 2
    hist_stats:
      episode_lengths:
      - 500
      - 500
      episode_reward:
      - 685.9203888394675
      - 671.5902275060362
      policy_a_reward: [128.219838318996, 95.32434107667193, 117.2034467675782, 57.736801975233355,
        66.22202951571332, 67.22620212817166, 115.07728515631284, 59.42518731554298,
        113.99698399817368, 93.54145256145539]
      policy_p_reward:
      - 221.21393118527172
      - 222.32311634638702
    num_faulty_episodes: 0
    policy_reward_max:
      a: 128.219838318996
      p: 222.32311634638702
    policy_reward_mean:
      a: 91.39735688138492
      p: 221.76852376582937
    policy_reward_min:
      a: 57.736801975233355
      p: 221.21393118527172
    sampler_perf:
      mean_action_processing_ms: 0.5693218318141493
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 4.288278546918635
      mean_inference_ms: 9.157721207934063
      mean_raw_obs_processing_ms: 2.207615622421686
  timesteps_this_iter: 1000

>> Wrote metrix to: rllib_test/test/decreasing/dense_logs/logs_04
Completing! Saving final snapshot...


Final snapshot saved! All done.
